nohup: ignoring input
{'R': 54000,
 'alpha': 10.0,
 'batch_size': 256,
 'cq_lambda': 0.0,
 'dataset': 'cifar10',
 'decay_step': 5000,
 'device': '/gpu:0',
 'finetune_all': True,
 'img_db': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/database.txt',
 'img_model': 'alexnet',
 'img_te': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/test.txt',
 'img_tr': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/train.txt',
 'label_dim': 10,
 'learning_rate': 5e-05,
 'learning_rate_decay_factor': 0.5,
 'log_dir': 'tflog',
 'loss_type': 'normed_cross_entropy',
 'max_iter': 2000,
 'model_weights': '../../DeepHash/architecture/pretrained_model/reference_pretrain.npy',
 'output_dim': 32,
 'save_dir': './models/',
 'val_batch_size': 100}
initializing
launching session
loading img model from ../../DeepHash/architecture/pretrained_model/reference_pretrain.npy
['hash_layer', 'fc6', 'fc7', 'conv3', 'conv2', 'conv1', 'conv5', 'conv4']
img model loading finished
Initializing Dataset
Dataset already
2019-05-04 12:48:17.343926 #train# start training
2019-05-04 12:48:33.043020 #train# step    1, loss = 2.8390, cross_entropy loss = 2.8390, 8.8 sec/batch
2019-05-04 12:48:34.621332 #train# step    2, loss = 2.0502, cross_entropy loss = 2.0502, 1.5 sec/batch
2019-05-04 12:48:36.070090 #train# step    3, loss = 1.7787, cross_entropy loss = 1.7787, 1.3 sec/batch
2019-05-04 12:48:37.551648 #train# step    4, loss = 1.8770, cross_entropy loss = 1.8770, 1.4 sec/batch
2019-05-04 12:48:39.078162 #train# step    5, loss = 2.1959, cross_entropy loss = 2.1959, 1.4 sec/batch
2019-05-04 12:48:40.599202 #train# step    6, loss = 2.3358, cross_entropy loss = 2.3358, 1.4 sec/batch
2019-05-04 12:48:42.096545 #train# step    7, loss = 1.9383, cross_entropy loss = 1.9383, 1.4 sec/batch
2019-05-04 12:48:43.612961 #train# step    8, loss = 1.7922, cross_entropy loss = 1.7922, 1.4 sec/batch
2019-05-04 12:48:45.074909 #train# step    9, loss = 1.8046, cross_entropy loss = 1.8046, 1.4 sec/batch
2019-05-04 12:48:46.510765 #train# step   10, loss = 1.8583, cross_entropy loss = 1.8583, 1.3 sec/batch
2019-05-04 12:48:47.923734 #train# step   11, loss = 2.0262, cross_entropy loss = 2.0262, 1.3 sec/batch
2019-05-04 12:48:49.365264 #train# step   12, loss = 2.0027, cross_entropy loss = 2.0027, 1.3 sec/batch
2019-05-04 12:48:50.763260 #train# step   13, loss = 1.7800, cross_entropy loss = 1.7800, 1.3 sec/batch
2019-05-04 12:48:52.222399 #train# step   14, loss = 1.7225, cross_entropy loss = 1.7225, 1.4 sec/batch
2019-05-04 12:48:53.654396 #train# step   15, loss = 1.6960, cross_entropy loss = 1.6960, 1.3 sec/batch
2019-05-04 12:48:55.135783 #train# step   16, loss = 1.6588, cross_entropy loss = 1.6588, 1.4 sec/batch
2019-05-04 12:48:56.564725 #train# step   17, loss = 1.7174, cross_entropy loss = 1.7174, 1.3 sec/batch
2019-05-04 12:48:58.038468 #train# step   18, loss = 1.7882, cross_entropy loss = 1.7882, 1.4 sec/batch
2019-05-04 12:48:59.532858 #train# step   19, loss = 1.7213, cross_entropy loss = 1.7213, 1.4 sec/batch
2019-05-04 12:49:00.904825 #train# step   20, loss = 1.7004, cross_entropy loss = 1.7004, 1.3 sec/batch
2019-05-04 12:49:02.256633 #train# step   21, loss = 1.6444, cross_entropy loss = 1.6444, 1.3 sec/batch
2019-05-04 12:49:03.648522 #train# step   22, loss = 1.6357, cross_entropy loss = 1.6357, 1.3 sec/batch
2019-05-04 12:49:05.025713 #train# step   23, loss = 1.6722, cross_entropy loss = 1.6722, 1.3 sec/batch
2019-05-04 12:49:06.388011 #train# step   24, loss = 1.6276, cross_entropy loss = 1.6276, 1.3 sec/batch
2019-05-04 12:49:07.758992 #train# step   25, loss = 1.6496, cross_entropy loss = 1.6496, 1.3 sec/batch
2019-05-04 12:49:09.123632 #train# step   26, loss = 1.6768, cross_entropy loss = 1.6768, 1.3 sec/batch
2019-05-04 12:49:10.486582 #train# step   27, loss = 1.6101, cross_entropy loss = 1.6101, 1.3 sec/batch
2019-05-04 12:49:11.844747 #train# step   28, loss = 1.6672, cross_entropy loss = 1.6672, 1.3 sec/batch
2019-05-04 12:49:13.214583 #train# step   29, loss = 1.6293, cross_entropy loss = 1.6293, 1.3 sec/batch
2019-05-04 12:49:14.573672 #train# step   30, loss = 1.5604, cross_entropy loss = 1.5604, 1.3 sec/batch
2019-05-04 12:49:15.920924 #train# step   31, loss = 1.5498, cross_entropy loss = 1.5498, 1.3 sec/batch
2019-05-04 12:49:17.315570 #train# step   32, loss = 1.5505, cross_entropy loss = 1.5505, 1.3 sec/batch
2019-05-04 12:49:18.672577 #train# step   33, loss = 1.5942, cross_entropy loss = 1.5942, 1.3 sec/batch
2019-05-04 12:49:20.025828 #train# step   34, loss = 1.6138, cross_entropy loss = 1.6138, 1.3 sec/batch
2019-05-04 12:49:21.403689 #train# step   35, loss = 1.5889, cross_entropy loss = 1.5889, 1.3 sec/batch
2019-05-04 12:49:22.776580 #train# step   36, loss = 1.6063, cross_entropy loss = 1.6063, 1.3 sec/batch
2019-05-04 12:49:24.133826 #train# step   37, loss = 1.5788, cross_entropy loss = 1.5788, 1.3 sec/batch
2019-05-04 12:49:25.508471 #train# step   38, loss = 1.5853, cross_entropy loss = 1.5853, 1.3 sec/batch
2019-05-04 12:49:26.848504 #train# step   39, loss = 1.5332, cross_entropy loss = 1.5332, 1.3 sec/batch
2019-05-04 12:49:28.213420 #train# step   40, loss = 1.5433, cross_entropy loss = 1.5433, 1.3 sec/batch
2019-05-04 12:49:29.574020 #train# step   41, loss = 1.5718, cross_entropy loss = 1.5718, 1.3 sec/batch
2019-05-04 12:49:30.936938 #train# step   42, loss = 1.5824, cross_entropy loss = 1.5824, 1.3 sec/batch
2019-05-04 12:49:32.281763 #train# step   43, loss = 1.5153, cross_entropy loss = 1.5153, 1.3 sec/batch
2019-05-04 12:49:33.645645 #train# step   44, loss = 1.4905, cross_entropy loss = 1.4905, 1.3 sec/batch
2019-05-04 12:49:35.008717 #train# step   45, loss = 1.5045, cross_entropy loss = 1.5045, 1.3 sec/batch
2019-05-04 12:49:36.380985 #train# step   46, loss = 1.5665, cross_entropy loss = 1.5665, 1.3 sec/batch
2019-05-04 12:49:37.734107 #train# step   47, loss = 1.5110, cross_entropy loss = 1.5110, 1.3 sec/batch
2019-05-04 12:49:39.073822 #train# step   48, loss = 1.4657, cross_entropy loss = 1.4657, 1.3 sec/batch
2019-05-04 12:49:40.408201 #train# step   49, loss = 1.5191, cross_entropy loss = 1.5191, 1.3 sec/batch
2019-05-04 12:49:41.777701 #train# step   50, loss = 1.4757, cross_entropy loss = 1.4757, 1.3 sec/batch
2019-05-04 12:49:43.130914 #train# step   51, loss = 1.4852, cross_entropy loss = 1.4852, 1.3 sec/batch
2019-05-04 12:49:44.539046 #train# step   52, loss = 1.4595, cross_entropy loss = 1.4595, 1.4 sec/batch
2019-05-04 12:49:45.880852 #train# step   53, loss = 1.4450, cross_entropy loss = 1.4450, 1.3 sec/batch
2019-05-04 12:49:47.231677 #train# step   54, loss = 1.4822, cross_entropy loss = 1.4822, 1.3 sec/batch
2019-05-04 12:49:48.568957 #train# step   55, loss = 1.5370, cross_entropy loss = 1.5370, 1.3 sec/batch
2019-05-04 12:49:49.930206 #train# step   56, loss = 1.5288, cross_entropy loss = 1.5288, 1.3 sec/batch
2019-05-04 12:49:52.194902 #train# step   57, loss = 1.5114, cross_entropy loss = 1.5114, 1.3 sec/batch
2019-05-04 12:49:53.538394 #train# step   58, loss = 1.4921, cross_entropy loss = 1.4921, 1.3 sec/batch
2019-05-04 12:49:54.862522 #train# step   59, loss = 1.4604, cross_entropy loss = 1.4604, 1.3 sec/batch
2019-05-04 12:49:56.203547 #train# step   60, loss = 1.5157, cross_entropy loss = 1.5157, 1.3 sec/batch
2019-05-04 12:49:57.542645 #train# step   61, loss = 1.4563, cross_entropy loss = 1.4563, 1.3 sec/batch
2019-05-04 12:49:58.902494 #train# step   62, loss = 1.5321, cross_entropy loss = 1.5321, 1.3 sec/batch
2019-05-04 12:50:00.244239 #train# step   63, loss = 1.5145, cross_entropy loss = 1.5145, 1.3 sec/batch
2019-05-04 12:50:01.661092 #train# step   64, loss = 1.4292, cross_entropy loss = 1.4292, 1.4 sec/batch
2019-05-04 12:50:03.026804 #train# step   65, loss = 1.4375, cross_entropy loss = 1.4375, 1.3 sec/batch
2019-05-04 12:50:04.390647 #train# step   66, loss = 1.4658, cross_entropy loss = 1.4658, 1.3 sec/batch
2019-05-04 12:50:05.737244 #train# step   67, loss = 1.4220, cross_entropy loss = 1.4220, 1.3 sec/batch
2019-05-04 12:50:07.083895 #train# step   68, loss = 1.4627, cross_entropy loss = 1.4627, 1.3 sec/batch
2019-05-04 12:50:08.425316 #train# step   69, loss = 1.4164, cross_entropy loss = 1.4164, 1.3 sec/batch
2019-05-04 12:50:09.801053 #train# step   70, loss = 1.4003, cross_entropy loss = 1.4003, 1.3 sec/batch
2019-05-04 12:50:11.141061 #train# step   71, loss = 1.4370, cross_entropy loss = 1.4370, 1.3 sec/batch
2019-05-04 12:50:12.487975 #train# step   72, loss = 1.4350, cross_entropy loss = 1.4350, 1.3 sec/batch
2019-05-04 12:50:13.854367 #train# step   73, loss = 1.4524, cross_entropy loss = 1.4524, 1.3 sec/batch
2019-05-04 12:50:15.208122 #train# step   74, loss = 1.4649, cross_entropy loss = 1.4649, 1.3 sec/batch
2019-05-04 12:50:16.561316 #train# step   75, loss = 1.4254, cross_entropy loss = 1.4254, 1.3 sec/batch
2019-05-04 12:50:17.917212 #train# step   76, loss = 1.4123, cross_entropy loss = 1.4123, 1.3 sec/batch
2019-05-04 12:50:19.239212 #train# step   77, loss = 1.4030, cross_entropy loss = 1.4030, 1.3 sec/batch
2019-05-04 12:50:20.594136 #train# step   78, loss = 1.4253, cross_entropy loss = 1.4253, 1.3 sec/batch
2019-05-04 12:50:21.911339 #train# step   79, loss = 1.3983, cross_entropy loss = 1.3983, 1.3 sec/batch
2019-05-04 12:50:23.271083 #train# step   80, loss = 1.4207, cross_entropy loss = 1.4207, 1.3 sec/batch
2019-05-04 12:50:24.605689 #train# step   81, loss = 1.4793, cross_entropy loss = 1.4793, 1.3 sec/batch
2019-05-04 12:50:25.975088 #train# step   82, loss = 1.4451, cross_entropy loss = 1.4451, 1.3 sec/batch
2019-05-04 12:50:27.309214 #train# step   83, loss = 1.3914, cross_entropy loss = 1.3914, 1.3 sec/batch
2019-05-04 12:50:28.692956 #train# step   84, loss = 1.5079, cross_entropy loss = 1.5079, 1.3 sec/batch
2019-05-04 12:50:30.085177 #train# step   85, loss = 1.4008, cross_entropy loss = 1.4008, 1.3 sec/batch
2019-05-04 12:50:31.440928 #train# step   86, loss = 1.4199, cross_entropy loss = 1.4199, 1.3 sec/batch
2019-05-04 12:50:32.780647 #train# step   87, loss = 1.4764, cross_entropy loss = 1.4764, 1.3 sec/batch
2019-05-04 12:50:34.131181 #train# step   88, loss = 1.3875, cross_entropy loss = 1.3875, 1.3 sec/batch
2019-05-04 12:50:35.492095 #train# step   89, loss = 1.4303, cross_entropy loss = 1.4303, 1.3 sec/batch
2019-05-04 12:50:36.844139 #train# step   90, loss = 1.4043, cross_entropy loss = 1.4043, 1.3 sec/batch
2019-05-04 12:50:38.204168 #train# step   91, loss = 1.4237, cross_entropy loss = 1.4237, 1.3 sec/batch
2019-05-04 12:50:39.556106 #train# step   92, loss = 1.4606, cross_entropy loss = 1.4606, 1.3 sec/batch
2019-05-04 12:50:40.880424 #train# step   93, loss = 1.4074, cross_entropy loss = 1.4074, 1.3 sec/batch
2019-05-04 12:50:42.234645 #train# step   94, loss = 1.3697, cross_entropy loss = 1.3697, 1.3 sec/batch
2019-05-04 12:50:43.572484 #train# step   95, loss = 1.4705, cross_entropy loss = 1.4705, 1.3 sec/batch
2019-05-04 12:50:44.923604 #train# step   96, loss = 1.4464, cross_entropy loss = 1.4464, 1.3 sec/batch
2019-05-04 12:50:46.244804 #train# step   97, loss = 1.3460, cross_entropy loss = 1.3460, 1.3 sec/batch
2019-05-04 12:50:47.593890 #train# step   98, loss = 1.3350, cross_entropy loss = 1.3350, 1.3 sec/batch
2019-05-04 12:50:48.926306 #train# step   99, loss = 1.4798, cross_entropy loss = 1.4798, 1.3 sec/batch
2019-05-04 12:50:50.260152 #train# step  100, loss = 1.3937, cross_entropy loss = 1.3937, 1.3 sec/batch
2019-05-04 12:50:51.591079 #train# step  101, loss = 1.3611, cross_entropy loss = 1.3611, 1.3 sec/batch
2019-05-04 12:50:52.978617 #train# step  102, loss = 1.4246, cross_entropy loss = 1.4246, 1.3 sec/batch
2019-05-04 12:50:54.316273 #train# step  103, loss = 1.4321, cross_entropy loss = 1.4321, 1.3 sec/batch
2019-05-04 12:50:55.674769 #train# step  104, loss = 1.4565, cross_entropy loss = 1.4565, 1.3 sec/batch
2019-05-04 12:50:57.014364 #train# step  105, loss = 1.3877, cross_entropy loss = 1.3877, 1.3 sec/batch
2019-05-04 12:50:58.431251 #train# step  106, loss = 1.3980, cross_entropy loss = 1.3980, 1.4 sec/batch
2019-05-04 12:50:59.805589 #train# step  107, loss = 1.3375, cross_entropy loss = 1.3375, 1.3 sec/batch
2019-05-04 12:51:01.162643 #train# step  108, loss = 1.4165, cross_entropy loss = 1.4165, 1.3 sec/batch
2019-05-04 12:51:02.500261 #train# step  109, loss = 1.4105, cross_entropy loss = 1.4105, 1.3 sec/batch
2019-05-04 12:51:03.861457 #train# step  110, loss = 1.3680, cross_entropy loss = 1.3680, 1.3 sec/batch
2019-05-04 12:51:05.181147 #train# step  111, loss = 1.3900, cross_entropy loss = 1.3900, 1.3 sec/batch
2019-05-04 12:51:06.515272 #train# step  112, loss = 1.3985, cross_entropy loss = 1.3985, 1.3 sec/batch
2019-05-04 12:51:07.862404 #train# step  113, loss = 1.4441, cross_entropy loss = 1.4441, 1.3 sec/batch
2019-05-04 12:51:09.206087 #train# step  114, loss = 1.3563, cross_entropy loss = 1.3563, 1.3 sec/batch
2019-05-04 12:51:10.525532 #train# step  115, loss = 1.4396, cross_entropy loss = 1.4396, 1.3 sec/batch
2019-05-04 12:51:11.865392 #train# step  116, loss = 1.3342, cross_entropy loss = 1.3342, 1.3 sec/batch
2019-05-04 12:51:13.188572 #train# step  117, loss = 1.3801, cross_entropy loss = 1.3801, 1.3 sec/batch
2019-05-04 12:51:14.532304 #train# step  118, loss = 1.3596, cross_entropy loss = 1.3596, 1.3 sec/batch
2019-05-04 12:51:15.883795 #train# step  119, loss = 1.3896, cross_entropy loss = 1.3896, 1.3 sec/batch
2019-05-04 12:51:17.231694 #train# step  120, loss = 1.3572, cross_entropy loss = 1.3572, 1.3 sec/batch
2019-05-04 12:51:18.565919 #train# step  121, loss = 1.4079, cross_entropy loss = 1.4079, 1.3 sec/batch
2019-05-04 12:51:19.910985 #train# step  122, loss = 1.3978, cross_entropy loss = 1.3978, 1.3 sec/batch
2019-05-04 12:51:21.252292 #train# step  123, loss = 1.4610, cross_entropy loss = 1.4610, 1.3 sec/batch
2019-05-04 12:51:22.573043 #train# step  124, loss = 1.3536, cross_entropy loss = 1.3536, 1.3 sec/batch
2019-05-04 12:51:23.909829 #train# step  125, loss = 1.3657, cross_entropy loss = 1.3657, 1.3 sec/batch
2019-05-04 12:51:25.263648 #train# step  126, loss = 1.3975, cross_entropy loss = 1.3975, 1.3 sec/batch
2019-05-04 12:51:26.591942 #train# step  127, loss = 1.4422, cross_entropy loss = 1.4422, 1.3 sec/batch
2019-05-04 12:51:27.934775 #train# step  128, loss = 1.3418, cross_entropy loss = 1.3418, 1.3 sec/batch
2019-05-04 12:51:29.269074 #train# step  129, loss = 1.3946, cross_entropy loss = 1.3946, 1.3 sec/batch
2019-05-04 12:51:30.617769 #train# step  130, loss = 1.3872, cross_entropy loss = 1.3872, 1.3 sec/batch
2019-05-04 12:51:31.941360 #train# step  131, loss = 1.3373, cross_entropy loss = 1.3373, 1.3 sec/batch
2019-05-04 12:51:33.285421 #train# step  132, loss = 1.3581, cross_entropy loss = 1.3581, 1.3 sec/batch
2019-05-04 12:51:34.620595 #train# step  133, loss = 1.3825, cross_entropy loss = 1.3825, 1.3 sec/batch
2019-05-04 12:51:35.966617 #train# step  134, loss = 1.3407, cross_entropy loss = 1.3407, 1.3 sec/batch
2019-05-04 12:51:37.275794 #train# step  135, loss = 1.3895, cross_entropy loss = 1.3895, 1.3 sec/batch
2019-05-04 12:51:38.631490 #train# step  136, loss = 1.2832, cross_entropy loss = 1.2832, 1.3 sec/batch
2019-05-04 12:51:39.989391 #train# step  137, loss = 1.3486, cross_entropy loss = 1.3486, 1.3 sec/batch
2019-05-04 12:51:41.371155 #train# step  138, loss = 1.3831, cross_entropy loss = 1.3831, 1.3 sec/batch
2019-05-04 12:51:42.703458 #train# step  139, loss = 1.3854, cross_entropy loss = 1.3854, 1.3 sec/batch
2019-05-04 12:51:44.044286 #train# step  140, loss = 1.3574, cross_entropy loss = 1.3574, 1.3 sec/batch
2019-05-04 12:51:45.378790 #train# step  141, loss = 1.4075, cross_entropy loss = 1.4075, 1.3 sec/batch
2019-05-04 12:51:46.734040 #train# step  142, loss = 1.3084, cross_entropy loss = 1.3084, 1.3 sec/batch
2019-05-04 12:51:48.060872 #train# step  143, loss = 1.4427, cross_entropy loss = 1.4427, 1.3 sec/batch
2019-05-04 12:51:49.411945 #train# step  144, loss = 1.3564, cross_entropy loss = 1.3564, 1.3 sec/batch
2019-05-04 12:51:50.746921 #train# step  145, loss = 1.2958, cross_entropy loss = 1.2958, 1.3 sec/batch
2019-05-04 12:51:52.105310 #train# step  146, loss = 1.3311, cross_entropy loss = 1.3311, 1.3 sec/batch
2019-05-04 12:51:53.424928 #train# step  147, loss = 1.3005, cross_entropy loss = 1.3005, 1.3 sec/batch
2019-05-04 12:51:54.784093 #train# step  148, loss = 1.3595, cross_entropy loss = 1.3595, 1.3 sec/batch
2019-05-04 12:51:56.107722 #train# step  149, loss = 1.3875, cross_entropy loss = 1.3875, 1.3 sec/batch
2019-05-04 12:51:57.446673 #train# step  150, loss = 1.3724, cross_entropy loss = 1.3724, 1.3 sec/batch
2019-05-04 12:51:58.780775 #train# step  151, loss = 1.2637, cross_entropy loss = 1.2637, 1.3 sec/batch
2019-05-04 12:52:00.130944 #train# step  152, loss = 1.3994, cross_entropy loss = 1.3994, 1.3 sec/batch
2019-05-04 12:52:01.454384 #train# step  153, loss = 1.3599, cross_entropy loss = 1.3599, 1.3 sec/batch
2019-05-04 12:52:02.793105 #train# step  154, loss = 1.3614, cross_entropy loss = 1.3614, 1.3 sec/batch
2019-05-04 12:52:04.122484 #train# step  155, loss = 1.3368, cross_entropy loss = 1.3368, 1.3 sec/batch
2019-05-04 12:52:05.454725 #train# step  156, loss = 1.3220, cross_entropy loss = 1.3220, 1.3 sec/batch
2019-05-04 12:52:06.783470 #train# step  157, loss = 1.3718, cross_entropy loss = 1.3718, 1.3 sec/batch
2019-05-04 12:52:08.131764 #train# step  158, loss = 1.3532, cross_entropy loss = 1.3532, 1.3 sec/batch
2019-05-04 12:52:09.451853 #train# step  159, loss = 1.4156, cross_entropy loss = 1.4156, 1.3 sec/batch
2019-05-04 12:52:10.795972 #train# step  160, loss = 1.4025, cross_entropy loss = 1.4025, 1.3 sec/batch
2019-05-04 12:52:12.119677 #train# step  161, loss = 1.3258, cross_entropy loss = 1.3258, 1.3 sec/batch
2019-05-04 12:52:13.431529 #train# step  162, loss = 1.3367, cross_entropy loss = 1.3367, 1.3 sec/batch
2019-05-04 12:52:14.769694 #train# step  163, loss = 1.3272, cross_entropy loss = 1.3272, 1.3 sec/batch
2019-05-04 12:52:16.113180 #train# step  164, loss = 1.3558, cross_entropy loss = 1.3558, 1.3 sec/batch
2019-05-04 12:52:17.432432 #train# step  165, loss = 1.3264, cross_entropy loss = 1.3264, 1.3 sec/batch
2019-05-04 12:52:18.786374 #train# step  166, loss = 1.2902, cross_entropy loss = 1.2902, 1.3 sec/batch
2019-05-04 12:52:20.108817 #train# step  167, loss = 1.3209, cross_entropy loss = 1.3209, 1.3 sec/batch
2019-05-04 12:52:21.462163 #train# step  168, loss = 1.3507, cross_entropy loss = 1.3507, 1.3 sec/batch
2019-05-04 12:52:22.813007 #train# step  169, loss = 1.3806, cross_entropy loss = 1.3806, 1.3 sec/batch
2019-05-04 12:52:24.147737 #train# step  170, loss = 1.3473, cross_entropy loss = 1.3473, 1.3 sec/batch
2019-05-04 12:52:25.549403 #train# step  171, loss = 1.3283, cross_entropy loss = 1.3283, 1.4 sec/batch
2019-05-04 12:52:26.898121 #train# step  172, loss = 1.3033, cross_entropy loss = 1.3033, 1.3 sec/batch
2019-05-04 12:52:28.224448 #train# step  173, loss = 1.3293, cross_entropy loss = 1.3293, 1.3 sec/batch
2019-05-04 12:52:29.640333 #train# step  174, loss = 1.3319, cross_entropy loss = 1.3319, 1.4 sec/batch
2019-05-04 12:52:30.971683 #train# step  175, loss = 1.2586, cross_entropy loss = 1.2586, 1.3 sec/batch
2019-05-04 12:52:32.276126 #train# step  176, loss = 1.2841, cross_entropy loss = 1.2841, 1.3 sec/batch
2019-05-04 12:52:33.572024 #train# step  177, loss = 1.3376, cross_entropy loss = 1.3376, 1.3 sec/batch
2019-05-04 12:52:34.940388 #train# step  178, loss = 1.3189, cross_entropy loss = 1.3189, 1.3 sec/batch
2019-05-04 12:52:36.245026 #train# step  179, loss = 1.3128, cross_entropy loss = 1.3128, 1.3 sec/batch
2019-05-04 12:52:37.596316 #train# step  180, loss = 1.3138, cross_entropy loss = 1.3138, 1.3 sec/batch
2019-05-04 12:52:38.916584 #train# step  181, loss = 1.2714, cross_entropy loss = 1.2714, 1.3 sec/batch
2019-05-04 12:52:40.266566 #train# step  182, loss = 1.3756, cross_entropy loss = 1.3756, 1.3 sec/batch
2019-05-04 12:52:41.599751 #train# step  183, loss = 1.3293, cross_entropy loss = 1.3293, 1.3 sec/batch
2019-05-04 12:52:42.941463 #train# step  184, loss = 1.3828, cross_entropy loss = 1.3828, 1.3 sec/batch
2019-05-04 12:52:44.272093 #train# step  185, loss = 1.2905, cross_entropy loss = 1.2905, 1.3 sec/batch
2019-05-04 12:52:45.619790 #train# step  186, loss = 1.3146, cross_entropy loss = 1.3146, 1.3 sec/batch
2019-05-04 12:52:46.942591 #train# step  187, loss = 1.2720, cross_entropy loss = 1.2720, 1.3 sec/batch
2019-05-04 12:52:48.280198 #train# step  188, loss = 1.3555, cross_entropy loss = 1.3555, 1.3 sec/batch
2019-05-04 12:52:49.601296 #train# step  189, loss = 1.2920, cross_entropy loss = 1.2920, 1.3 sec/batch
2019-05-04 12:52:50.947583 #train# step  190, loss = 1.3447, cross_entropy loss = 1.3447, 1.3 sec/batch
2019-05-04 12:52:52.260461 #train# step  191, loss = 1.2974, cross_entropy loss = 1.2974, 1.3 sec/batch
2019-05-04 12:52:53.597468 #train# step  192, loss = 1.3148, cross_entropy loss = 1.3148, 1.3 sec/batch
2019-05-04 12:52:54.886703 #train# step  193, loss = 1.2326, cross_entropy loss = 1.2326, 1.3 sec/batch
2019-05-04 12:52:56.223771 #train# step  194, loss = 1.2985, cross_entropy loss = 1.2985, 1.3 sec/batch
2019-05-04 12:52:57.537335 #train# step  195, loss = 1.3074, cross_entropy loss = 1.3074, 1.3 sec/batch
2019-05-04 12:52:58.869311 #train# step  196, loss = 1.2896, cross_entropy loss = 1.2896, 1.3 sec/batch
2019-05-04 12:53:00.194628 #train# step  197, loss = 1.2349, cross_entropy loss = 1.2349, 1.3 sec/batch
2019-05-04 12:53:01.524025 #train# step  198, loss = 1.3149, cross_entropy loss = 1.3149, 1.3 sec/batch
2019-05-04 12:53:02.839059 #train# step  199, loss = 1.3385, cross_entropy loss = 1.3385, 1.3 sec/batch
2019-05-04 12:53:04.164559 #train# step  200, loss = 1.2615, cross_entropy loss = 1.2615, 1.3 sec/batch
2019-05-04 12:53:05.475288 #train# step  201, loss = 1.3750, cross_entropy loss = 1.3750, 1.3 sec/batch
2019-05-04 12:53:06.826488 #train# step  202, loss = 1.3200, cross_entropy loss = 1.3200, 1.3 sec/batch
2019-05-04 12:53:08.121962 #train# step  203, loss = 1.2899, cross_entropy loss = 1.2899, 1.3 sec/batch
2019-05-04 12:53:09.451267 #train# step  204, loss = 1.2622, cross_entropy loss = 1.2622, 1.3 sec/batch
2019-05-04 12:53:10.759740 #train# step  205, loss = 1.3385, cross_entropy loss = 1.3385, 1.3 sec/batch
2019-05-04 12:53:12.079385 #train# step  206, loss = 1.2950, cross_entropy loss = 1.2950, 1.3 sec/batch
2019-05-04 12:53:13.384209 #train# step  207, loss = 1.3219, cross_entropy loss = 1.3219, 1.3 sec/batch
2019-05-04 12:53:14.704984 #train# step  208, loss = 1.3530, cross_entropy loss = 1.3530, 1.3 sec/batch
2019-05-04 12:53:15.990733 #train# step  209, loss = 1.3086, cross_entropy loss = 1.3086, 1.3 sec/batch
2019-05-04 12:53:17.289492 #train# step  210, loss = 1.2963, cross_entropy loss = 1.2963, 1.3 sec/batch
2019-05-04 12:53:18.610793 #train# step  211, loss = 1.2999, cross_entropy loss = 1.2999, 1.3 sec/batch
2019-05-04 12:53:19.951990 #train# step  212, loss = 1.2661, cross_entropy loss = 1.2661, 1.3 sec/batch
2019-05-04 12:53:21.264660 #train# step  213, loss = 1.3207, cross_entropy loss = 1.3207, 1.3 sec/batch
2019-05-04 12:53:22.578908 #train# step  214, loss = 1.3179, cross_entropy loss = 1.3179, 1.3 sec/batch
2019-05-04 12:53:23.898862 #train# step  215, loss = 1.3612, cross_entropy loss = 1.3612, 1.3 sec/batch
2019-05-04 12:53:25.236763 #train# step  216, loss = 1.3453, cross_entropy loss = 1.3453, 1.3 sec/batch
2019-05-04 12:53:26.560614 #train# step  217, loss = 1.3156, cross_entropy loss = 1.3156, 1.3 sec/batch
2019-05-04 12:53:27.896897 #train# step  218, loss = 1.2196, cross_entropy loss = 1.2196, 1.3 sec/batch
2019-05-04 12:53:29.206253 #train# step  219, loss = 1.2392, cross_entropy loss = 1.2392, 1.3 sec/batch
2019-05-04 12:53:30.543310 #train# step  220, loss = 1.3080, cross_entropy loss = 1.3080, 1.3 sec/batch
2019-05-04 12:53:31.856455 #train# step  221, loss = 1.2236, cross_entropy loss = 1.2236, 1.3 sec/batch
2019-05-04 12:53:33.184625 #train# step  222, loss = 1.3311, cross_entropy loss = 1.3311, 1.3 sec/batch
2019-05-04 12:53:34.503120 #train# step  223, loss = 1.2979, cross_entropy loss = 1.2979, 1.3 sec/batch
2019-05-04 12:53:35.844505 #train# step  224, loss = 1.3156, cross_entropy loss = 1.3156, 1.3 sec/batch
2019-05-04 12:53:37.166286 #train# step  225, loss = 1.2919, cross_entropy loss = 1.2919, 1.3 sec/batch
2019-05-04 12:53:38.494323 #train# step  226, loss = 1.3078, cross_entropy loss = 1.3078, 1.3 sec/batch
2019-05-04 12:53:39.816179 #train# step  227, loss = 1.2956, cross_entropy loss = 1.2956, 1.3 sec/batch
2019-05-04 12:53:41.152041 #train# step  228, loss = 1.3117, cross_entropy loss = 1.3117, 1.3 sec/batch
2019-05-04 12:53:42.466262 #train# step  229, loss = 1.3035, cross_entropy loss = 1.3035, 1.3 sec/batch
2019-05-04 12:53:43.809994 #train# step  230, loss = 1.2861, cross_entropy loss = 1.2861, 1.3 sec/batch
2019-05-04 12:53:45.152035 #train# step  231, loss = 1.3226, cross_entropy loss = 1.3226, 1.3 sec/batch
2019-05-04 12:53:46.486913 #train# step  232, loss = 1.2655, cross_entropy loss = 1.2655, 1.3 sec/batch
2019-05-04 12:53:47.799039 #train# step  233, loss = 1.2939, cross_entropy loss = 1.2939, 1.3 sec/batch
2019-05-04 12:53:49.131005 #train# step  234, loss = 1.1920, cross_entropy loss = 1.1920, 1.3 sec/batch
2019-05-04 12:53:50.515391 #train# step  235, loss = 1.3268, cross_entropy loss = 1.3268, 1.3 sec/batch
2019-05-04 12:53:51.849314 #train# step  236, loss = 1.3666, cross_entropy loss = 1.3666, 1.3 sec/batch
2019-05-04 12:53:53.151672 #train# step  237, loss = 1.3014, cross_entropy loss = 1.3014, 1.3 sec/batch
2019-05-04 12:53:54.484190 #train# step  238, loss = 1.3051, cross_entropy loss = 1.3051, 1.3 sec/batch
2019-05-04 12:53:55.802614 #train# step  239, loss = 1.2653, cross_entropy loss = 1.2653, 1.3 sec/batch
2019-05-04 12:53:57.140243 #train# step  240, loss = 1.2836, cross_entropy loss = 1.2836, 1.3 sec/batch
2019-05-04 12:53:58.464521 #train# step  241, loss = 1.3249, cross_entropy loss = 1.3249, 1.3 sec/batch
2019-05-04 12:53:59.808629 #train# step  242, loss = 1.2419, cross_entropy loss = 1.2419, 1.3 sec/batch
2019-05-04 12:54:01.125711 #train# step  243, loss = 1.2174, cross_entropy loss = 1.2174, 1.3 sec/batch
2019-05-04 12:54:02.467850 #train# step  244, loss = 1.2635, cross_entropy loss = 1.2635, 1.3 sec/batch
2019-05-04 12:54:03.793700 #train# step  245, loss = 1.2523, cross_entropy loss = 1.2523, 1.3 sec/batch
2019-05-04 12:54:05.122740 #train# step  246, loss = 1.3446, cross_entropy loss = 1.3446, 1.3 sec/batch
2019-05-04 12:54:06.443918 #train# step  247, loss = 1.3428, cross_entropy loss = 1.3428, 1.3 sec/batch
2019-05-04 12:54:07.739921 #train# step  248, loss = 1.3496, cross_entropy loss = 1.3496, 1.3 sec/batch
2019-05-04 12:54:09.020141 #train# step  249, loss = 1.2204, cross_entropy loss = 1.2204, 1.2 sec/batch
2019-05-04 12:54:10.341642 #train# step  250, loss = 1.2800, cross_entropy loss = 1.2800, 1.3 sec/batch
2019-05-04 12:54:11.646062 #train# step  251, loss = 1.2371, cross_entropy loss = 1.2371, 1.3 sec/batch
2019-05-04 12:54:12.972135 #train# step  252, loss = 1.3433, cross_entropy loss = 1.3433, 1.3 sec/batch
2019-05-04 12:54:14.257574 #train# step  253, loss = 1.2788, cross_entropy loss = 1.2788, 1.3 sec/batch
2019-05-04 12:54:15.580782 #train# step  254, loss = 1.2768, cross_entropy loss = 1.2768, 1.3 sec/batch
2019-05-04 12:54:16.875649 #train# step  255, loss = 1.3019, cross_entropy loss = 1.3019, 1.3 sec/batch
2019-05-04 12:54:18.198998 #train# step  256, loss = 1.2743, cross_entropy loss = 1.2743, 1.3 sec/batch
2019-05-04 12:54:19.504156 #train# step  257, loss = 1.3100, cross_entropy loss = 1.3100, 1.3 sec/batch
2019-05-04 12:54:20.815059 #train# step  258, loss = 1.2927, cross_entropy loss = 1.2927, 1.3 sec/batch
2019-05-04 12:54:22.114804 #train# step  259, loss = 1.1665, cross_entropy loss = 1.1665, 1.3 sec/batch
2019-05-04 12:54:23.423676 #train# step  260, loss = 1.2671, cross_entropy loss = 1.2671, 1.3 sec/batch
2019-05-04 12:54:24.726230 #train# step  261, loss = 1.2793, cross_entropy loss = 1.2793, 1.3 sec/batch
2019-05-04 12:54:26.050447 #train# step  262, loss = 1.2886, cross_entropy loss = 1.2886, 1.3 sec/batch
2019-05-04 12:54:27.350555 #train# step  263, loss = 1.2654, cross_entropy loss = 1.2654, 1.3 sec/batch
2019-05-04 12:54:28.669865 #train# step  264, loss = 1.2057, cross_entropy loss = 1.2057, 1.3 sec/batch
2019-05-04 12:54:29.977373 #train# step  265, loss = 1.2646, cross_entropy loss = 1.2646, 1.3 sec/batch
2019-05-04 12:54:31.302382 #train# step  266, loss = 1.2771, cross_entropy loss = 1.2771, 1.3 sec/batch
2019-05-04 12:54:32.607904 #train# step  267, loss = 1.2442, cross_entropy loss = 1.2442, 1.3 sec/batch
2019-05-04 12:54:33.893390 #train# step  268, loss = 1.2893, cross_entropy loss = 1.2893, 1.3 sec/batch
2019-05-04 12:54:35.192386 #train# step  269, loss = 1.2303, cross_entropy loss = 1.2303, 1.3 sec/batch
2019-05-04 12:54:36.529633 #train# step  270, loss = 1.2472, cross_entropy loss = 1.2472, 1.3 sec/batch
2019-05-04 12:54:37.828387 #train# step  271, loss = 1.2608, cross_entropy loss = 1.2608, 1.3 sec/batch
2019-05-04 12:54:39.158685 #train# step  272, loss = 1.3158, cross_entropy loss = 1.3158, 1.3 sec/batch
2019-05-04 12:54:40.464546 #train# step  273, loss = 1.2314, cross_entropy loss = 1.2314, 1.3 sec/batch
2019-05-04 12:54:41.786966 #train# step  274, loss = 1.3059, cross_entropy loss = 1.3059, 1.3 sec/batch
2019-05-04 12:54:43.108083 #train# step  275, loss = 1.2323, cross_entropy loss = 1.2323, 1.3 sec/batch
2019-05-04 12:54:44.430507 #train# step  276, loss = 1.3164, cross_entropy loss = 1.3164, 1.3 sec/batch
2019-05-04 12:54:45.732175 #train# step  277, loss = 1.2882, cross_entropy loss = 1.2882, 1.3 sec/batch
2019-05-04 12:54:47.067098 #train# step  278, loss = 1.2534, cross_entropy loss = 1.2534, 1.3 sec/batch
2019-05-04 12:54:48.377116 #train# step  279, loss = 1.2945, cross_entropy loss = 1.2945, 1.3 sec/batch
2019-05-04 12:54:49.687843 #train# step  280, loss = 1.2578, cross_entropy loss = 1.2578, 1.3 sec/batch
2019-05-04 12:54:51.002000 #train# step  281, loss = 1.2715, cross_entropy loss = 1.2715, 1.3 sec/batch
2019-05-04 12:54:52.325745 #train# step  282, loss = 1.2264, cross_entropy loss = 1.2264, 1.3 sec/batch
2019-05-04 12:54:53.633711 #train# step  283, loss = 1.2621, cross_entropy loss = 1.2621, 1.3 sec/batch
2019-05-04 12:54:54.964671 #train# step  284, loss = 1.2618, cross_entropy loss = 1.2618, 1.3 sec/batch
2019-05-04 12:54:56.267259 #train# step  285, loss = 1.2530, cross_entropy loss = 1.2530, 1.3 sec/batch
2019-05-04 12:54:57.620210 #train# step  286, loss = 1.2339, cross_entropy loss = 1.2339, 1.3 sec/batch
2019-05-04 12:54:58.929019 #train# step  287, loss = 1.2554, cross_entropy loss = 1.2554, 1.3 sec/batch
2019-05-04 12:55:00.262234 #train# step  288, loss = 1.2612, cross_entropy loss = 1.2612, 1.3 sec/batch
2019-05-04 12:55:01.567232 #train# step  289, loss = 1.2212, cross_entropy loss = 1.2212, 1.3 sec/batch
2019-05-04 12:55:02.900182 #train# step  290, loss = 1.3367, cross_entropy loss = 1.3367, 1.3 sec/batch
2019-05-04 12:55:04.216497 #train# step  291, loss = 1.2501, cross_entropy loss = 1.2501, 1.3 sec/batch
2019-05-04 12:55:05.558069 #train# step  292, loss = 1.2820, cross_entropy loss = 1.2820, 1.3 sec/batch
2019-05-04 12:55:06.873142 #train# step  293, loss = 1.2408, cross_entropy loss = 1.2408, 1.3 sec/batch
2019-05-04 12:55:08.203704 #train# step  294, loss = 1.2722, cross_entropy loss = 1.2722, 1.3 sec/batch
2019-05-04 12:55:09.524362 #train# step  295, loss = 1.2875, cross_entropy loss = 1.2875, 1.3 sec/batch
2019-05-04 12:55:10.865228 #train# step  296, loss = 1.3378, cross_entropy loss = 1.3378, 1.3 sec/batch
2019-05-04 12:55:12.178091 #train# step  297, loss = 1.1990, cross_entropy loss = 1.1990, 1.3 sec/batch
2019-05-04 12:55:13.505838 #train# step  298, loss = 1.2722, cross_entropy loss = 1.2722, 1.3 sec/batch
2019-05-04 12:55:14.825566 #train# step  299, loss = 1.2368, cross_entropy loss = 1.2368, 1.3 sec/batch
2019-05-04 12:55:16.166102 #train# step  300, loss = 1.2731, cross_entropy loss = 1.2731, 1.3 sec/batch
2019-05-04 12:55:17.536846 #train# step  301, loss = 1.2788, cross_entropy loss = 1.2788, 1.3 sec/batch
2019-05-04 12:55:18.877138 #train# step  302, loss = 1.2249, cross_entropy loss = 1.2249, 1.3 sec/batch
2019-05-04 12:55:20.226729 #train# step  303, loss = 1.1873, cross_entropy loss = 1.1873, 1.3 sec/batch
2019-05-04 12:55:21.565468 #train# step  304, loss = 1.2436, cross_entropy loss = 1.2436, 1.3 sec/batch
2019-05-04 12:55:22.848811 #train# step  305, loss = 1.2480, cross_entropy loss = 1.2480, 1.2 sec/batch
2019-05-04 12:55:24.156517 #train# step  306, loss = 1.1917, cross_entropy loss = 1.1917, 1.3 sec/batch
2019-05-04 12:55:25.462144 #train# step  307, loss = 1.2739, cross_entropy loss = 1.2739, 1.3 sec/batch
2019-05-04 12:55:26.796921 #train# step  308, loss = 1.3066, cross_entropy loss = 1.3066, 1.3 sec/batch
2019-05-04 12:55:28.094563 #train# step  309, loss = 1.2498, cross_entropy loss = 1.2498, 1.3 sec/batch
2019-05-04 12:55:29.419903 #train# step  310, loss = 1.3123, cross_entropy loss = 1.3123, 1.3 sec/batch
2019-05-04 12:55:30.737192 #train# step  311, loss = 1.2219, cross_entropy loss = 1.2219, 1.3 sec/batch
2019-05-04 12:55:32.055014 #train# step  312, loss = 1.2137, cross_entropy loss = 1.2137, 1.3 sec/batch
2019-05-04 12:55:33.371226 #train# step  313, loss = 1.2173, cross_entropy loss = 1.2173, 1.3 sec/batch
2019-05-04 12:55:34.702172 #train# step  314, loss = 1.2695, cross_entropy loss = 1.2695, 1.3 sec/batch
2019-05-04 12:55:36.013103 #train# step  315, loss = 1.2765, cross_entropy loss = 1.2765, 1.3 sec/batch
2019-05-04 12:55:37.333907 #train# step  316, loss = 1.3249, cross_entropy loss = 1.3249, 1.3 sec/batch
2019-05-04 12:55:38.649303 #train# step  317, loss = 1.1646, cross_entropy loss = 1.1646, 1.3 sec/batch
2019-05-04 12:55:39.973159 #train# step  318, loss = 1.2689, cross_entropy loss = 1.2689, 1.3 sec/batch
2019-05-04 12:55:41.270251 #train# step  319, loss = 1.2627, cross_entropy loss = 1.2627, 1.3 sec/batch
2019-05-04 12:55:42.605383 #train# step  320, loss = 1.2187, cross_entropy loss = 1.2187, 1.3 sec/batch
2019-05-04 12:55:43.919996 #train# step  321, loss = 1.2186, cross_entropy loss = 1.2186, 1.3 sec/batch
2019-05-04 12:55:45.239006 #train# step  322, loss = 1.1719, cross_entropy loss = 1.1719, 1.3 sec/batch
2019-05-04 12:55:46.547863 #train# step  323, loss = 1.2433, cross_entropy loss = 1.2433, 1.3 sec/batch
2019-05-04 12:55:47.866957 #train# step  324, loss = 1.2731, cross_entropy loss = 1.2731, 1.3 sec/batch
2019-05-04 12:55:49.166376 #train# step  325, loss = 1.2609, cross_entropy loss = 1.2609, 1.3 sec/batch
2019-05-04 12:55:50.496743 #train# step  326, loss = 1.2316, cross_entropy loss = 1.2316, 1.3 sec/batch
2019-05-04 12:55:51.801167 #train# step  327, loss = 1.2311, cross_entropy loss = 1.2311, 1.3 sec/batch
2019-05-04 12:55:53.109640 #train# step  328, loss = 1.2281, cross_entropy loss = 1.2281, 1.3 sec/batch
2019-05-04 12:55:54.404532 #train# step  329, loss = 1.2867, cross_entropy loss = 1.2867, 1.3 sec/batch
2019-05-04 12:55:55.710375 #train# step  330, loss = 1.2261, cross_entropy loss = 1.2261, 1.3 sec/batch
2019-05-04 12:55:57.019215 #train# step  331, loss = 1.2133, cross_entropy loss = 1.2133, 1.3 sec/batch
2019-05-04 12:55:58.330324 #train# step  332, loss = 1.2746, cross_entropy loss = 1.2746, 1.3 sec/batch
2019-05-04 12:55:59.642053 #train# step  333, loss = 1.2280, cross_entropy loss = 1.2280, 1.3 sec/batch
2019-05-04 12:56:00.957967 #train# step  334, loss = 1.1867, cross_entropy loss = 1.1867, 1.3 sec/batch
2019-05-04 12:56:02.254751 #train# step  335, loss = 1.1891, cross_entropy loss = 1.1891, 1.3 sec/batch
2019-05-04 12:56:03.549831 #train# step  336, loss = 1.2533, cross_entropy loss = 1.2533, 1.3 sec/batch
2019-05-04 12:56:04.840384 #train# step  337, loss = 1.2405, cross_entropy loss = 1.2405, 1.3 sec/batch
2019-05-04 12:56:06.161995 #train# step  338, loss = 1.2694, cross_entropy loss = 1.2694, 1.3 sec/batch
2019-05-04 12:56:07.465563 #train# step  339, loss = 1.2396, cross_entropy loss = 1.2396, 1.3 sec/batch
2019-05-04 12:56:08.781318 #train# step  340, loss = 1.1792, cross_entropy loss = 1.1792, 1.3 sec/batch
2019-05-04 12:56:10.085590 #train# step  341, loss = 1.2752, cross_entropy loss = 1.2752, 1.3 sec/batch
2019-05-04 12:56:11.393160 #train# step  342, loss = 1.2477, cross_entropy loss = 1.2477, 1.3 sec/batch
2019-05-04 12:56:12.704245 #train# step  343, loss = 1.2696, cross_entropy loss = 1.2696, 1.3 sec/batch
2019-05-04 12:56:14.022805 #train# step  344, loss = 1.2638, cross_entropy loss = 1.2638, 1.3 sec/batch
2019-05-04 12:56:15.328626 #train# step  345, loss = 1.2692, cross_entropy loss = 1.2692, 1.3 sec/batch
2019-05-04 12:56:16.647261 #train# step  346, loss = 1.2702, cross_entropy loss = 1.2702, 1.3 sec/batch
2019-05-04 12:56:17.927722 #train# step  347, loss = 1.2533, cross_entropy loss = 1.2533, 1.2 sec/batch
2019-05-04 12:56:19.248666 #train# step  348, loss = 1.2718, cross_entropy loss = 1.2718, 1.3 sec/batch
2019-05-04 12:56:20.571198 #train# step  349, loss = 1.2130, cross_entropy loss = 1.2130, 1.3 sec/batch
2019-05-04 12:56:21.892842 #train# step  350, loss = 1.2633, cross_entropy loss = 1.2633, 1.3 sec/batch
2019-05-04 12:56:23.201534 #train# step  351, loss = 1.2095, cross_entropy loss = 1.2095, 1.3 sec/batch
2019-05-04 12:56:24.528653 #train# step  352, loss = 1.2196, cross_entropy loss = 1.2196, 1.3 sec/batch
2019-05-04 12:56:25.829543 #train# step  353, loss = 1.1582, cross_entropy loss = 1.1582, 1.3 sec/batch
2019-05-04 12:56:27.152521 #train# step  354, loss = 1.2697, cross_entropy loss = 1.2697, 1.3 sec/batch
2019-05-04 12:56:28.455173 #train# step  355, loss = 1.2215, cross_entropy loss = 1.2215, 1.3 sec/batch
2019-05-04 12:56:29.770313 #train# step  356, loss = 1.2713, cross_entropy loss = 1.2713, 1.3 sec/batch
2019-05-04 12:56:31.081702 #train# step  357, loss = 1.2115, cross_entropy loss = 1.2115, 1.3 sec/batch
2019-05-04 12:56:32.398772 #train# step  358, loss = 1.1527, cross_entropy loss = 1.1527, 1.3 sec/batch
2019-05-04 12:56:33.708374 #train# step  359, loss = 1.2459, cross_entropy loss = 1.2459, 1.3 sec/batch
2019-05-04 12:56:35.046230 #train# step  360, loss = 1.2556, cross_entropy loss = 1.2556, 1.3 sec/batch
2019-05-04 12:56:36.359978 #train# step  361, loss = 1.2121, cross_entropy loss = 1.2121, 1.3 sec/batch
2019-05-04 12:56:37.690529 #train# step  362, loss = 1.2653, cross_entropy loss = 1.2653, 1.3 sec/batch
2019-05-04 12:56:38.993660 #train# step  363, loss = 1.2108, cross_entropy loss = 1.2108, 1.3 sec/batch
2019-05-04 12:56:40.299755 #train# step  364, loss = 1.2774, cross_entropy loss = 1.2774, 1.3 sec/batch
2019-05-04 12:56:41.595687 #train# step  365, loss = 1.1694, cross_entropy loss = 1.1694, 1.3 sec/batch
2019-05-04 12:56:42.921823 #train# step  366, loss = 1.1808, cross_entropy loss = 1.1808, 1.3 sec/batch
2019-05-04 12:56:44.245778 #train# step  367, loss = 1.2052, cross_entropy loss = 1.2052, 1.3 sec/batch
2019-05-04 12:56:45.590089 #train# step  368, loss = 1.1898, cross_entropy loss = 1.1898, 1.3 sec/batch
2019-05-04 12:56:46.924529 #train# step  369, loss = 1.2124, cross_entropy loss = 1.2124, 1.3 sec/batch
2019-05-04 12:56:48.268296 #train# step  370, loss = 1.2243, cross_entropy loss = 1.2243, 1.3 sec/batch
2019-05-04 12:56:49.601877 #train# step  371, loss = 1.2356, cross_entropy loss = 1.2356, 1.3 sec/batch
2019-05-04 12:56:50.934718 #train# step  372, loss = 1.2516, cross_entropy loss = 1.2516, 1.3 sec/batch
2019-05-04 12:56:52.256215 #train# step  373, loss = 1.1981, cross_entropy loss = 1.1981, 1.3 sec/batch
2019-05-04 12:56:53.600323 #train# step  374, loss = 1.2319, cross_entropy loss = 1.2319, 1.3 sec/batch
2019-05-04 12:56:54.922074 #train# step  375, loss = 1.1211, cross_entropy loss = 1.1211, 1.3 sec/batch
2019-05-04 12:56:56.254168 #train# step  376, loss = 1.1948, cross_entropy loss = 1.1948, 1.3 sec/batch
2019-05-04 12:56:57.579620 #train# step  377, loss = 1.2120, cross_entropy loss = 1.2120, 1.3 sec/batch
2019-05-04 12:56:58.914951 #train# step  378, loss = 1.3156, cross_entropy loss = 1.3156, 1.3 sec/batch
2019-05-04 12:57:00.236269 #train# step  379, loss = 1.2948, cross_entropy loss = 1.2948, 1.3 sec/batch
2019-05-04 12:57:01.564769 #train# step  380, loss = 1.2220, cross_entropy loss = 1.2220, 1.3 sec/batch
2019-05-04 12:57:02.877402 #train# step  381, loss = 1.1997, cross_entropy loss = 1.1997, 1.3 sec/batch
2019-05-04 12:57:04.201214 #train# step  382, loss = 1.2604, cross_entropy loss = 1.2604, 1.3 sec/batch
2019-05-04 12:57:05.511303 #train# step  383, loss = 1.2063, cross_entropy loss = 1.2063, 1.3 sec/batch
2019-05-04 12:57:06.831086 #train# step  384, loss = 1.1810, cross_entropy loss = 1.1810, 1.3 sec/batch
2019-05-04 12:57:08.136793 #train# step  385, loss = 1.2810, cross_entropy loss = 1.2810, 1.3 sec/batch
2019-05-04 12:57:09.454636 #train# step  386, loss = 1.2008, cross_entropy loss = 1.2008, 1.3 sec/batch
2019-05-04 12:57:10.766860 #train# step  387, loss = 1.2079, cross_entropy loss = 1.2079, 1.3 sec/batch
2019-05-04 12:57:12.099902 #train# step  388, loss = 1.1613, cross_entropy loss = 1.1613, 1.3 sec/batch
2019-05-04 12:57:13.378209 #train# step  389, loss = 1.2670, cross_entropy loss = 1.2670, 1.2 sec/batch
2019-05-04 12:57:14.667805 #train# step  390, loss = 1.2787, cross_entropy loss = 1.2787, 1.3 sec/batch
2019-05-04 12:57:15.951816 #train# step  391, loss = 1.2812, cross_entropy loss = 1.2812, 1.2 sec/batch
2019-05-04 12:57:17.262290 #train# step  392, loss = 1.2396, cross_entropy loss = 1.2396, 1.3 sec/batch
2019-05-04 12:57:18.548420 #train# step  393, loss = 1.2089, cross_entropy loss = 1.2089, 1.2 sec/batch
2019-05-04 12:57:19.847408 #train# step  394, loss = 1.1737, cross_entropy loss = 1.1737, 1.3 sec/batch
2019-05-04 12:57:21.155004 #train# step  395, loss = 1.2499, cross_entropy loss = 1.2499, 1.3 sec/batch
2019-05-04 12:57:22.477750 #train# step  396, loss = 1.2128, cross_entropy loss = 1.2128, 1.3 sec/batch
2019-05-04 12:57:23.784497 #train# step  397, loss = 1.2305, cross_entropy loss = 1.2305, 1.3 sec/batch
2019-05-04 12:57:25.096280 #train# step  398, loss = 1.1229, cross_entropy loss = 1.1229, 1.3 sec/batch
2019-05-04 12:57:26.408305 #train# step  399, loss = 1.2500, cross_entropy loss = 1.2500, 1.3 sec/batch
2019-05-04 12:57:27.730388 #train# step  400, loss = 1.1931, cross_entropy loss = 1.1931, 1.3 sec/batch
2019-05-04 12:57:29.036478 #train# step  401, loss = 1.2190, cross_entropy loss = 1.2190, 1.3 sec/batch
2019-05-04 12:57:30.354988 #train# step  402, loss = 1.2090, cross_entropy loss = 1.2090, 1.3 sec/batch
2019-05-04 12:57:31.661767 #train# step  403, loss = 1.2207, cross_entropy loss = 1.2207, 1.3 sec/batch
2019-05-04 12:57:32.982826 #train# step  404, loss = 1.2101, cross_entropy loss = 1.2101, 1.3 sec/batch
2019-05-04 12:57:34.297708 #train# step  405, loss = 1.2334, cross_entropy loss = 1.2334, 1.3 sec/batch
2019-05-04 12:57:35.622044 #train# step  406, loss = 1.1228, cross_entropy loss = 1.1228, 1.3 sec/batch
2019-05-04 12:57:36.929310 #train# step  407, loss = 1.1853, cross_entropy loss = 1.1853, 1.3 sec/batch
2019-05-04 12:57:38.240958 #train# step  408, loss = 1.2338, cross_entropy loss = 1.2338, 1.3 sec/batch
2019-05-04 12:57:39.523241 #train# step  409, loss = 1.2477, cross_entropy loss = 1.2477, 1.2 sec/batch
2019-05-04 12:57:40.818172 #train# step  410, loss = 1.2328, cross_entropy loss = 1.2328, 1.3 sec/batch
2019-05-04 12:57:42.125376 #train# step  411, loss = 1.2099, cross_entropy loss = 1.2099, 1.3 sec/batch
2019-05-04 12:57:43.423358 #train# step  412, loss = 1.2831, cross_entropy loss = 1.2831, 1.3 sec/batch
2019-05-04 12:57:44.721065 #train# step  413, loss = 1.2190, cross_entropy loss = 1.2190, 1.3 sec/batch
2019-05-04 12:57:46.038896 #train# step  414, loss = 1.2120, cross_entropy loss = 1.2120, 1.3 sec/batch
2019-05-04 12:57:47.355303 #train# step  415, loss = 1.2070, cross_entropy loss = 1.2070, 1.3 sec/batch
2019-05-04 12:57:48.668273 #train# step  416, loss = 1.1617, cross_entropy loss = 1.1617, 1.3 sec/batch
2019-05-04 12:57:49.969011 #train# step  417, loss = 1.2068, cross_entropy loss = 1.2068, 1.3 sec/batch
2019-05-04 12:57:51.298144 #train# step  418, loss = 1.1494, cross_entropy loss = 1.1494, 1.3 sec/batch
2019-05-04 12:57:52.605742 #train# step  419, loss = 1.1983, cross_entropy loss = 1.1983, 1.3 sec/batch
2019-05-04 12:57:53.945472 #train# step  420, loss = 1.1980, cross_entropy loss = 1.1980, 1.3 sec/batch
2019-05-04 12:57:55.258919 #train# step  421, loss = 1.1893, cross_entropy loss = 1.1893, 1.3 sec/batch
2019-05-04 12:57:56.568997 #train# step  422, loss = 1.2056, cross_entropy loss = 1.2056, 1.3 sec/batch
2019-05-04 12:57:57.886900 #train# step  423, loss = 1.2311, cross_entropy loss = 1.2311, 1.3 sec/batch
2019-05-04 12:57:59.210316 #train# step  424, loss = 1.1899, cross_entropy loss = 1.1899, 1.3 sec/batch
2019-05-04 12:58:00.492046 #train# step  425, loss = 1.2627, cross_entropy loss = 1.2627, 1.2 sec/batch
2019-05-04 12:58:01.823803 #train# step  426, loss = 1.1796, cross_entropy loss = 1.1796, 1.3 sec/batch
2019-05-04 12:58:03.128351 #train# step  427, loss = 1.2158, cross_entropy loss = 1.2158, 1.3 sec/batch
2019-05-04 12:58:04.458736 #train# step  428, loss = 1.1718, cross_entropy loss = 1.1718, 1.3 sec/batch
2019-05-04 12:58:05.768217 #train# step  429, loss = 1.2113, cross_entropy loss = 1.2113, 1.3 sec/batch
2019-05-04 12:58:07.096690 #train# step  430, loss = 1.2398, cross_entropy loss = 1.2398, 1.3 sec/batch
2019-05-04 12:58:08.398530 #train# step  431, loss = 1.2133, cross_entropy loss = 1.2133, 1.3 sec/batch
2019-05-04 12:58:09.699978 #train# step  432, loss = 1.2908, cross_entropy loss = 1.2908, 1.3 sec/batch
2019-05-04 12:58:11.013954 #train# step  433, loss = 1.1864, cross_entropy loss = 1.1864, 1.3 sec/batch
2019-05-04 12:58:12.339220 #train# step  434, loss = 1.2001, cross_entropy loss = 1.2001, 1.3 sec/batch
2019-05-04 12:58:13.629799 #train# step  435, loss = 1.1751, cross_entropy loss = 1.1751, 1.3 sec/batch
2019-05-04 12:58:14.941536 #train# step  436, loss = 1.2353, cross_entropy loss = 1.2353, 1.3 sec/batch
2019-05-04 12:58:16.237110 #train# step  437, loss = 1.1547, cross_entropy loss = 1.1547, 1.3 sec/batch
2019-05-04 12:58:17.559159 #train# step  438, loss = 1.2082, cross_entropy loss = 1.2082, 1.3 sec/batch
2019-05-04 12:58:18.873593 #train# step  439, loss = 1.2258, cross_entropy loss = 1.2258, 1.3 sec/batch
2019-05-04 12:58:20.197041 #train# step  440, loss = 1.2539, cross_entropy loss = 1.2539, 1.3 sec/batch
2019-05-04 12:58:21.526177 #train# step  441, loss = 1.1871, cross_entropy loss = 1.1871, 1.3 sec/batch
2019-05-04 12:58:22.842383 #train# step  442, loss = 1.1907, cross_entropy loss = 1.1907, 1.3 sec/batch
2019-05-04 12:58:24.160593 #train# step  443, loss = 1.2051, cross_entropy loss = 1.2051, 1.3 sec/batch
2019-05-04 12:58:25.493695 #train# step  444, loss = 1.2445, cross_entropy loss = 1.2445, 1.3 sec/batch
2019-05-04 12:58:26.819255 #train# step  445, loss = 1.2035, cross_entropy loss = 1.2035, 1.3 sec/batch
2019-05-04 12:58:28.179321 #train# step  446, loss = 1.1858, cross_entropy loss = 1.1858, 1.3 sec/batch
2019-05-04 12:58:29.478753 #train# step  447, loss = 1.1500, cross_entropy loss = 1.1500, 1.2 sec/batch
2019-05-04 12:58:30.806248 #train# step  448, loss = 1.1522, cross_entropy loss = 1.1522, 1.3 sec/batch
2019-05-04 12:58:32.146425 #train# step  449, loss = 1.2858, cross_entropy loss = 1.2858, 1.3 sec/batch
2019-05-04 12:58:33.438989 #train# step  450, loss = 1.2118, cross_entropy loss = 1.2118, 1.2 sec/batch
2019-05-04 12:58:34.769861 #train# step  451, loss = 1.2530, cross_entropy loss = 1.2530, 1.3 sec/batch
2019-05-04 12:58:36.062813 #train# step  452, loss = 1.1906, cross_entropy loss = 1.1906, 1.2 sec/batch
2019-05-04 12:58:37.377010 #train# step  453, loss = 1.2409, cross_entropy loss = 1.2409, 1.3 sec/batch
2019-05-04 12:58:38.712505 #train# step  454, loss = 1.1909, cross_entropy loss = 1.1909, 1.3 sec/batch
2019-05-04 12:58:40.038319 #train# step  455, loss = 1.1502, cross_entropy loss = 1.1502, 1.3 sec/batch
2019-05-04 12:58:41.367099 #train# step  456, loss = 1.1910, cross_entropy loss = 1.1910, 1.3 sec/batch
2019-05-04 12:58:42.678884 #train# step  457, loss = 1.1903, cross_entropy loss = 1.1903, 1.3 sec/batch
2019-05-04 12:58:44.003357 #train# step  458, loss = 1.2271, cross_entropy loss = 1.2271, 1.3 sec/batch
2019-05-04 12:58:45.304254 #train# step  459, loss = 1.2723, cross_entropy loss = 1.2723, 1.3 sec/batch
2019-05-04 12:58:46.619200 #train# step  460, loss = 1.1580, cross_entropy loss = 1.1580, 1.3 sec/batch
2019-05-04 12:58:47.932193 #train# step  461, loss = 1.2188, cross_entropy loss = 1.2188, 1.3 sec/batch
2019-05-04 12:58:49.219767 #train# step  462, loss = 1.1639, cross_entropy loss = 1.1639, 1.3 sec/batch
2019-05-04 12:58:50.521759 #train# step  463, loss = 1.1350, cross_entropy loss = 1.1350, 1.3 sec/batch
2019-05-04 12:58:51.828969 #train# step  464, loss = 1.1894, cross_entropy loss = 1.1894, 1.3 sec/batch
2019-05-04 12:58:53.169507 #train# step  465, loss = 1.1729, cross_entropy loss = 1.1729, 1.3 sec/batch
2019-05-04 12:58:54.474351 #train# step  466, loss = 1.1587, cross_entropy loss = 1.1587, 1.3 sec/batch
2019-05-04 12:58:55.780714 #train# step  467, loss = 1.2335, cross_entropy loss = 1.2335, 1.3 sec/batch
2019-05-04 12:58:57.091795 #train# step  468, loss = 1.1874, cross_entropy loss = 1.1874, 1.3 sec/batch
2019-05-04 12:58:58.397479 #train# step  469, loss = 1.1905, cross_entropy loss = 1.1905, 1.3 sec/batch
2019-05-04 12:58:59.677735 #train# step  470, loss = 1.1787, cross_entropy loss = 1.1787, 1.2 sec/batch
2019-05-04 12:59:00.994997 #train# step  471, loss = 1.2087, cross_entropy loss = 1.2087, 1.3 sec/batch
2019-05-04 12:59:02.276644 #train# step  472, loss = 1.1880, cross_entropy loss = 1.1880, 1.2 sec/batch
2019-05-04 12:59:03.590305 #train# step  473, loss = 1.2178, cross_entropy loss = 1.2178, 1.3 sec/batch
2019-05-04 12:59:04.911005 #train# step  474, loss = 1.1629, cross_entropy loss = 1.1629, 1.3 sec/batch
2019-05-04 12:59:06.219273 #train# step  475, loss = 1.1623, cross_entropy loss = 1.1623, 1.3 sec/batch
2019-05-04 12:59:07.500197 #train# step  476, loss = 1.1154, cross_entropy loss = 1.1154, 1.2 sec/batch
2019-05-04 12:59:08.818359 #train# step  477, loss = 1.2062, cross_entropy loss = 1.2062, 1.3 sec/batch
2019-05-04 12:59:10.103308 #train# step  478, loss = 1.1844, cross_entropy loss = 1.1844, 1.2 sec/batch
2019-05-04 12:59:11.417858 #train# step  479, loss = 1.2128, cross_entropy loss = 1.2128, 1.3 sec/batch
2019-05-04 12:59:12.733958 #train# step  480, loss = 1.1878, cross_entropy loss = 1.1878, 1.3 sec/batch
2019-05-04 12:59:14.020988 #train# step  481, loss = 1.2810, cross_entropy loss = 1.2810, 1.3 sec/batch
2019-05-04 12:59:15.293230 #train# step  482, loss = 1.1830, cross_entropy loss = 1.1830, 1.2 sec/batch
2019-05-04 12:59:16.602552 #train# step  483, loss = 1.1448, cross_entropy loss = 1.1448, 1.3 sec/batch
2019-05-04 12:59:17.926636 #train# step  484, loss = 1.1972, cross_entropy loss = 1.1972, 1.3 sec/batch
2019-05-04 12:59:19.192730 #train# step  485, loss = 1.2037, cross_entropy loss = 1.2037, 1.2 sec/batch
2019-05-04 12:59:20.474323 #train# step  486, loss = 1.1607, cross_entropy loss = 1.1607, 1.2 sec/batch
2019-05-04 12:59:21.769077 #train# step  487, loss = 1.2259, cross_entropy loss = 1.2259, 1.3 sec/batch
2019-05-04 12:59:23.069746 #train# step  488, loss = 1.1997, cross_entropy loss = 1.1997, 1.3 sec/batch
2019-05-04 12:59:24.345379 #train# step  489, loss = 1.1816, cross_entropy loss = 1.1816, 1.2 sec/batch
2019-05-04 12:59:25.664489 #train# step  490, loss = 1.2083, cross_entropy loss = 1.2083, 1.3 sec/batch
2019-05-04 12:59:26.986894 #train# step  491, loss = 1.1597, cross_entropy loss = 1.1597, 1.3 sec/batch
2019-05-04 12:59:28.315832 #train# step  492, loss = 1.1426, cross_entropy loss = 1.1426, 1.3 sec/batch
2019-05-04 12:59:29.621262 #train# step  493, loss = 1.2095, cross_entropy loss = 1.2095, 1.3 sec/batch
2019-05-04 12:59:30.895673 #train# step  494, loss = 1.1546, cross_entropy loss = 1.1546, 1.2 sec/batch
2019-05-04 12:59:32.204717 #train# step  495, loss = 1.1388, cross_entropy loss = 1.1388, 1.3 sec/batch
2019-05-04 12:59:33.527323 #train# step  496, loss = 1.1396, cross_entropy loss = 1.1396, 1.3 sec/batch
2019-05-04 12:59:34.819588 #train# step  497, loss = 1.1989, cross_entropy loss = 1.1989, 1.2 sec/batch
2019-05-04 12:59:36.133792 #train# step  498, loss = 1.2599, cross_entropy loss = 1.2599, 1.3 sec/batch
2019-05-04 12:59:37.458019 #train# step  499, loss = 1.1233, cross_entropy loss = 1.1233, 1.3 sec/batch
2019-05-04 12:59:38.803035 #train# step  500, loss = 1.2074, cross_entropy loss = 1.2074, 1.3 sec/batch
2019-05-04 12:59:40.133977 #train# step  501, loss = 1.1625, cross_entropy loss = 1.1625, 1.3 sec/batch
2019-05-04 12:59:41.468260 #train# step  502, loss = 1.1667, cross_entropy loss = 1.1667, 1.3 sec/batch
2019-05-04 12:59:42.780081 #train# step  503, loss = 1.1907, cross_entropy loss = 1.1907, 1.3 sec/batch
2019-05-04 12:59:44.079813 #train# step  504, loss = 1.1642, cross_entropy loss = 1.1642, 1.3 sec/batch
2019-05-04 12:59:45.367027 #train# step  505, loss = 1.1884, cross_entropy loss = 1.1884, 1.2 sec/batch
2019-05-04 12:59:46.701350 #train# step  506, loss = 1.2999, cross_entropy loss = 1.2999, 1.3 sec/batch
2019-05-04 12:59:48.018498 #train# step  507, loss = 1.1848, cross_entropy loss = 1.1848, 1.3 sec/batch
2019-05-04 12:59:49.349245 #train# step  508, loss = 1.1604, cross_entropy loss = 1.1604, 1.3 sec/batch
2019-05-04 12:59:50.661135 #train# step  509, loss = 1.1285, cross_entropy loss = 1.1285, 1.3 sec/batch
2019-05-04 12:59:51.993366 #train# step  510, loss = 1.1569, cross_entropy loss = 1.1569, 1.3 sec/batch
2019-05-04 12:59:53.313035 #train# step  511, loss = 1.1270, cross_entropy loss = 1.1270, 1.3 sec/batch
2019-05-04 12:59:54.626150 #train# step  512, loss = 1.1728, cross_entropy loss = 1.1728, 1.3 sec/batch
2019-05-04 12:59:55.935902 #train# step  513, loss = 1.1866, cross_entropy loss = 1.1866, 1.3 sec/batch
2019-05-04 12:59:57.247139 #train# step  514, loss = 1.1300, cross_entropy loss = 1.1300, 1.3 sec/batch
2019-05-04 12:59:58.560332 #train# step  515, loss = 1.2220, cross_entropy loss = 1.2220, 1.3 sec/batch
2019-05-04 12:59:59.883277 #train# step  516, loss = 1.1119, cross_entropy loss = 1.1119, 1.3 sec/batch
2019-05-04 13:00:01.190144 #train# step  517, loss = 1.2130, cross_entropy loss = 1.2130, 1.3 sec/batch
2019-05-04 13:00:02.494371 #train# step  518, loss = 1.1243, cross_entropy loss = 1.1243, 1.3 sec/batch
2019-05-04 13:00:03.807820 #train# step  519, loss = 1.2032, cross_entropy loss = 1.2032, 1.3 sec/batch
2019-05-04 13:00:05.121990 #train# step  520, loss = 1.1637, cross_entropy loss = 1.1637, 1.3 sec/batch
2019-05-04 13:00:06.429284 #train# step  521, loss = 1.1871, cross_entropy loss = 1.1871, 1.3 sec/batch
2019-05-04 13:00:07.728718 #train# step  522, loss = 1.2229, cross_entropy loss = 1.2229, 1.3 sec/batch
2019-05-04 13:00:09.034174 #train# step  523, loss = 1.0776, cross_entropy loss = 1.0776, 1.3 sec/batch
2019-05-04 13:00:10.348943 #train# step  524, loss = 1.1645, cross_entropy loss = 1.1645, 1.3 sec/batch
2019-05-04 13:00:11.659854 #train# step  525, loss = 1.1831, cross_entropy loss = 1.1831, 1.3 sec/batch
2019-05-04 13:00:12.972355 #train# step  526, loss = 1.1992, cross_entropy loss = 1.1992, 1.3 sec/batch
2019-05-04 13:00:14.254091 #train# step  527, loss = 1.1606, cross_entropy loss = 1.1606, 1.2 sec/batch
2019-05-04 13:00:15.566012 #train# step  528, loss = 1.1647, cross_entropy loss = 1.1647, 1.3 sec/batch
2019-05-04 13:00:16.874293 #train# step  529, loss = 1.2680, cross_entropy loss = 1.2680, 1.3 sec/batch
2019-05-04 13:00:18.195444 #train# step  530, loss = 1.1571, cross_entropy loss = 1.1571, 1.3 sec/batch
2019-05-04 13:00:19.503996 #train# step  531, loss = 1.1851, cross_entropy loss = 1.1851, 1.3 sec/batch
2019-05-04 13:00:20.806315 #train# step  532, loss = 1.1739, cross_entropy loss = 1.1739, 1.3 sec/batch
2019-05-04 13:00:22.110518 #train# step  533, loss = 1.1892, cross_entropy loss = 1.1892, 1.3 sec/batch
2019-05-04 13:00:23.421930 #train# step  534, loss = 1.1881, cross_entropy loss = 1.1881, 1.3 sec/batch
2019-05-04 13:00:24.709635 #train# step  535, loss = 1.2065, cross_entropy loss = 1.2065, 1.3 sec/batch
2019-05-04 13:00:26.025451 #train# step  536, loss = 1.1287, cross_entropy loss = 1.1287, 1.3 sec/batch
2019-05-04 13:00:27.329866 #train# step  537, loss = 1.0928, cross_entropy loss = 1.0928, 1.3 sec/batch
2019-05-04 13:00:28.645991 #train# step  538, loss = 1.2221, cross_entropy loss = 1.2221, 1.3 sec/batch
2019-05-04 13:00:29.959321 #train# step  539, loss = 1.1461, cross_entropy loss = 1.1461, 1.3 sec/batch
2019-05-04 13:00:31.274661 #train# step  540, loss = 1.1368, cross_entropy loss = 1.1368, 1.3 sec/batch
2019-05-04 13:00:32.580688 #train# step  541, loss = 1.1202, cross_entropy loss = 1.1202, 1.3 sec/batch
2019-05-04 13:00:33.894932 #train# step  542, loss = 1.1581, cross_entropy loss = 1.1581, 1.3 sec/batch
2019-05-04 13:00:35.203507 #train# step  543, loss = 1.1595, cross_entropy loss = 1.1595, 1.3 sec/batch
2019-05-04 13:00:36.526251 #train# step  544, loss = 1.1716, cross_entropy loss = 1.1716, 1.3 sec/batch
2019-05-04 13:00:37.813027 #train# step  545, loss = 1.1327, cross_entropy loss = 1.1327, 1.3 sec/batch
2019-05-04 13:00:39.122055 #train# step  546, loss = 1.1155, cross_entropy loss = 1.1155, 1.3 sec/batch
2019-05-04 13:00:40.426536 #train# step  547, loss = 1.1286, cross_entropy loss = 1.1286, 1.3 sec/batch
2019-05-04 13:00:41.715253 #train# step  548, loss = 1.2118, cross_entropy loss = 1.2118, 1.3 sec/batch
2019-05-04 13:00:43.013054 #train# step  549, loss = 1.1635, cross_entropy loss = 1.1635, 1.3 sec/batch
2019-05-04 13:00:44.319361 #train# step  550, loss = 1.1798, cross_entropy loss = 1.1798, 1.3 sec/batch
2019-05-04 13:00:45.623833 #train# step  551, loss = 1.1741, cross_entropy loss = 1.1741, 1.3 sec/batch
2019-05-04 13:00:46.946780 #train# step  552, loss = 1.2052, cross_entropy loss = 1.2052, 1.3 sec/batch
2019-05-04 13:00:48.248701 #train# step  553, loss = 1.1880, cross_entropy loss = 1.1880, 1.3 sec/batch
2019-05-04 13:00:49.575078 #train# step  554, loss = 1.1165, cross_entropy loss = 1.1165, 1.3 sec/batch
2019-05-04 13:00:50.875703 #train# step  555, loss = 1.2103, cross_entropy loss = 1.2103, 1.3 sec/batch
2019-05-04 13:00:52.203967 #train# step  556, loss = 1.1854, cross_entropy loss = 1.1854, 1.3 sec/batch
2019-05-04 13:00:53.525534 #train# step  557, loss = 1.1513, cross_entropy loss = 1.1513, 1.3 sec/batch
2019-05-04 13:00:54.841857 #train# step  558, loss = 1.2209, cross_entropy loss = 1.2209, 1.3 sec/batch
2019-05-04 13:00:56.144479 #train# step  559, loss = 1.1644, cross_entropy loss = 1.1644, 1.3 sec/batch
2019-05-04 13:00:57.466512 #train# step  560, loss = 1.1478, cross_entropy loss = 1.1478, 1.3 sec/batch
2019-05-04 13:00:58.791433 #train# step  561, loss = 1.1464, cross_entropy loss = 1.1464, 1.3 sec/batch
2019-05-04 13:01:00.071061 #train# step  562, loss = 1.2328, cross_entropy loss = 1.2328, 1.2 sec/batch
2019-05-04 13:01:01.377215 #train# step  563, loss = 1.1965, cross_entropy loss = 1.1965, 1.3 sec/batch
2019-05-04 13:01:02.657643 #train# step  564, loss = 1.1487, cross_entropy loss = 1.1487, 1.2 sec/batch
2019-05-04 13:01:03.978972 #train# step  565, loss = 1.1625, cross_entropy loss = 1.1625, 1.3 sec/batch
2019-05-04 13:01:05.293302 #train# step  566, loss = 1.2250, cross_entropy loss = 1.2250, 1.3 sec/batch
2019-05-04 13:01:06.618949 #train# step  567, loss = 1.1393, cross_entropy loss = 1.1393, 1.3 sec/batch
2019-05-04 13:01:07.947309 #train# step  568, loss = 1.1376, cross_entropy loss = 1.1376, 1.3 sec/batch
2019-05-04 13:01:09.289610 #train# step  569, loss = 1.1196, cross_entropy loss = 1.1196, 1.3 sec/batch
2019-05-04 13:01:10.601724 #train# step  570, loss = 1.1407, cross_entropy loss = 1.1407, 1.3 sec/batch
2019-05-04 13:01:11.914734 #train# step  571, loss = 1.1795, cross_entropy loss = 1.1795, 1.3 sec/batch
2019-05-04 13:01:13.214115 #train# step  572, loss = 1.1504, cross_entropy loss = 1.1504, 1.3 sec/batch
2019-05-04 13:01:14.523320 #train# step  573, loss = 1.1262, cross_entropy loss = 1.1262, 1.3 sec/batch
2019-05-04 13:01:15.840869 #train# step  574, loss = 1.1174, cross_entropy loss = 1.1174, 1.3 sec/batch
2019-05-04 13:01:17.149389 #train# step  575, loss = 1.1627, cross_entropy loss = 1.1627, 1.3 sec/batch
2019-05-04 13:01:18.455720 #train# step  576, loss = 1.1541, cross_entropy loss = 1.1541, 1.3 sec/batch
2019-05-04 13:01:19.768335 #train# step  577, loss = 1.1902, cross_entropy loss = 1.1902, 1.3 sec/batch
2019-05-04 13:01:21.045306 #train# step  578, loss = 1.1709, cross_entropy loss = 1.1709, 1.2 sec/batch
2019-05-04 13:01:22.362090 #train# step  579, loss = 1.1698, cross_entropy loss = 1.1698, 1.3 sec/batch
2019-05-04 13:01:23.672603 #train# step  580, loss = 1.1840, cross_entropy loss = 1.1840, 1.3 sec/batch
2019-05-04 13:01:24.980937 #train# step  581, loss = 1.1427, cross_entropy loss = 1.1427, 1.3 sec/batch
2019-05-04 13:01:26.289286 #train# step  582, loss = 1.2044, cross_entropy loss = 1.2044, 1.3 sec/batch
2019-05-04 13:01:27.592776 #train# step  583, loss = 1.1869, cross_entropy loss = 1.1869, 1.3 sec/batch
2019-05-04 13:01:28.900946 #train# step  584, loss = 1.2146, cross_entropy loss = 1.2146, 1.3 sec/batch
2019-05-04 13:01:30.202117 #train# step  585, loss = 1.1210, cross_entropy loss = 1.1210, 1.3 sec/batch
2019-05-04 13:01:31.522759 #train# step  586, loss = 1.1568, cross_entropy loss = 1.1568, 1.3 sec/batch
2019-05-04 13:01:32.834046 #train# step  587, loss = 1.1683, cross_entropy loss = 1.1683, 1.3 sec/batch
2019-05-04 13:01:34.159852 #train# step  588, loss = 1.1626, cross_entropy loss = 1.1626, 1.3 sec/batch
2019-05-04 13:01:35.470629 #train# step  589, loss = 1.2040, cross_entropy loss = 1.2040, 1.3 sec/batch
2019-05-04 13:01:36.778444 #train# step  590, loss = 1.1739, cross_entropy loss = 1.1739, 1.3 sec/batch
2019-05-04 13:01:38.076969 #train# step  591, loss = 1.1910, cross_entropy loss = 1.1910, 1.3 sec/batch
2019-05-04 13:01:39.376787 #train# step  592, loss = 1.1556, cross_entropy loss = 1.1556, 1.3 sec/batch
2019-05-04 13:01:40.665783 #train# step  593, loss = 1.1468, cross_entropy loss = 1.1468, 1.3 sec/batch
2019-05-04 13:01:41.971265 #train# step  594, loss = 1.1132, cross_entropy loss = 1.1132, 1.3 sec/batch
2019-05-04 13:01:43.273350 #train# step  595, loss = 1.1858, cross_entropy loss = 1.1858, 1.3 sec/batch
2019-05-04 13:01:44.572287 #train# step  596, loss = 1.1413, cross_entropy loss = 1.1413, 1.3 sec/batch
2019-05-04 13:01:45.882471 #train# step  597, loss = 1.1731, cross_entropy loss = 1.1731, 1.3 sec/batch
2019-05-04 13:01:47.196033 #train# step  598, loss = 1.1583, cross_entropy loss = 1.1583, 1.3 sec/batch
2019-05-04 13:01:48.504141 #train# step  599, loss = 1.1874, cross_entropy loss = 1.1874, 1.3 sec/batch
2019-05-04 13:01:49.801181 #train# step  600, loss = 1.1459, cross_entropy loss = 1.1459, 1.3 sec/batch
2019-05-04 13:01:51.089518 #train# step  601, loss = 1.1451, cross_entropy loss = 1.1451, 1.3 sec/batch
2019-05-04 13:01:52.400431 #train# step  602, loss = 1.1170, cross_entropy loss = 1.1170, 1.3 sec/batch
2019-05-04 13:01:53.699272 #train# step  603, loss = 1.1962, cross_entropy loss = 1.1962, 1.3 sec/batch
2019-05-04 13:01:55.005093 #train# step  604, loss = 1.1909, cross_entropy loss = 1.1909, 1.3 sec/batch
2019-05-04 13:01:56.309955 #train# step  605, loss = 1.1438, cross_entropy loss = 1.1438, 1.3 sec/batch
2019-05-04 13:01:57.609170 #train# step  606, loss = 1.1961, cross_entropy loss = 1.1961, 1.3 sec/batch
2019-05-04 13:01:58.917421 #train# step  607, loss = 1.1937, cross_entropy loss = 1.1937, 1.3 sec/batch
2019-05-04 13:02:00.228098 #train# step  608, loss = 1.1537, cross_entropy loss = 1.1537, 1.3 sec/batch
2019-05-04 13:02:01.536777 #train# step  609, loss = 1.1430, cross_entropy loss = 1.1430, 1.3 sec/batch
2019-05-04 13:02:02.860331 #train# step  610, loss = 1.1123, cross_entropy loss = 1.1123, 1.3 sec/batch
2019-05-04 13:02:04.171005 #train# step  611, loss = 1.1212, cross_entropy loss = 1.1212, 1.3 sec/batch
2019-05-04 13:02:05.483125 #train# step  612, loss = 1.1657, cross_entropy loss = 1.1657, 1.3 sec/batch
2019-05-04 13:02:06.803332 #train# step  613, loss = 1.1503, cross_entropy loss = 1.1503, 1.3 sec/batch
2019-05-04 13:02:08.104994 #train# step  614, loss = 1.1740, cross_entropy loss = 1.1740, 1.3 sec/batch
2019-05-04 13:02:09.400459 #train# step  615, loss = 1.1831, cross_entropy loss = 1.1831, 1.3 sec/batch
2019-05-04 13:02:10.715858 #train# step  616, loss = 1.1034, cross_entropy loss = 1.1034, 1.3 sec/batch
2019-05-04 13:02:11.996542 #train# step  617, loss = 1.1876, cross_entropy loss = 1.1876, 1.2 sec/batch
2019-05-04 13:02:13.321874 #train# step  618, loss = 1.1201, cross_entropy loss = 1.1201, 1.3 sec/batch
2019-05-04 13:02:14.626107 #train# step  619, loss = 1.1559, cross_entropy loss = 1.1559, 1.3 sec/batch
2019-05-04 13:02:15.943010 #train# step  620, loss = 1.1651, cross_entropy loss = 1.1651, 1.3 sec/batch
2019-05-04 13:02:17.234686 #train# step  621, loss = 1.1620, cross_entropy loss = 1.1620, 1.3 sec/batch
2019-05-04 13:02:18.542248 #train# step  622, loss = 1.1628, cross_entropy loss = 1.1628, 1.3 sec/batch
2019-05-04 13:02:19.863450 #train# step  623, loss = 1.1568, cross_entropy loss = 1.1568, 1.3 sec/batch
2019-05-04 13:02:21.166157 #train# step  624, loss = 1.1817, cross_entropy loss = 1.1817, 1.3 sec/batch
2019-05-04 13:02:22.479965 #train# step  625, loss = 1.1536, cross_entropy loss = 1.1536, 1.3 sec/batch
2019-05-04 13:02:23.782269 #train# step  626, loss = 1.1170, cross_entropy loss = 1.1170, 1.3 sec/batch
2019-05-04 13:02:25.093410 #train# step  627, loss = 1.1524, cross_entropy loss = 1.1524, 1.3 sec/batch
2019-05-04 13:02:26.410870 #train# step  628, loss = 1.0132, cross_entropy loss = 1.0132, 1.3 sec/batch
2019-05-04 13:02:27.724862 #train# step  629, loss = 1.1334, cross_entropy loss = 1.1334, 1.3 sec/batch
2019-05-04 13:02:29.028948 #train# step  630, loss = 1.1907, cross_entropy loss = 1.1907, 1.3 sec/batch
2019-05-04 13:02:30.350023 #train# step  631, loss = 1.1340, cross_entropy loss = 1.1340, 1.3 sec/batch
2019-05-04 13:02:31.673082 #train# step  632, loss = 1.1466, cross_entropy loss = 1.1466, 1.3 sec/batch
2019-05-04 13:02:32.973108 #train# step  633, loss = 1.1695, cross_entropy loss = 1.1695, 1.3 sec/batch
2019-05-04 13:02:34.297793 #train# step  634, loss = 1.1480, cross_entropy loss = 1.1480, 1.3 sec/batch
2019-05-04 13:02:35.603886 #train# step  635, loss = 1.1518, cross_entropy loss = 1.1518, 1.3 sec/batch
2019-05-04 13:02:36.921044 #train# step  636, loss = 1.1406, cross_entropy loss = 1.1406, 1.3 sec/batch
2019-05-04 13:02:38.211140 #train# step  637, loss = 1.1730, cross_entropy loss = 1.1730, 1.2 sec/batch
2019-05-04 13:02:39.497717 #train# step  638, loss = 1.1490, cross_entropy loss = 1.1490, 1.2 sec/batch
2019-05-04 13:02:40.801103 #train# step  639, loss = 1.1372, cross_entropy loss = 1.1372, 1.3 sec/batch
2019-05-04 13:02:42.131602 #train# step  640, loss = 1.2051, cross_entropy loss = 1.2051, 1.3 sec/batch
2019-05-04 13:02:43.436490 #train# step  641, loss = 1.1567, cross_entropy loss = 1.1567, 1.3 sec/batch
2019-05-04 13:02:44.726360 #train# step  642, loss = 1.1215, cross_entropy loss = 1.1215, 1.2 sec/batch
2019-05-04 13:02:46.077692 #train# step  643, loss = 1.2075, cross_entropy loss = 1.2075, 1.3 sec/batch
2019-05-04 13:02:47.398529 #train# step  644, loss = 1.1652, cross_entropy loss = 1.1652, 1.3 sec/batch
2019-05-04 13:02:48.752755 #train# step  645, loss = 1.1764, cross_entropy loss = 1.1764, 1.3 sec/batch
2019-05-04 13:02:50.073484 #train# step  646, loss = 1.2017, cross_entropy loss = 1.2017, 1.3 sec/batch
2019-05-04 13:02:51.369419 #train# step  647, loss = 1.1144, cross_entropy loss = 1.1144, 1.3 sec/batch
2019-05-04 13:02:52.665295 #train# step  648, loss = 1.0797, cross_entropy loss = 1.0797, 1.3 sec/batch
2019-05-04 13:02:53.935817 #train# step  649, loss = 1.1388, cross_entropy loss = 1.1388, 1.2 sec/batch
2019-05-04 13:02:55.209559 #train# step  650, loss = 1.1364, cross_entropy loss = 1.1364, 1.2 sec/batch
2019-05-04 13:02:56.507507 #train# step  651, loss = 1.1864, cross_entropy loss = 1.1864, 1.3 sec/batch
2019-05-04 13:02:57.811212 #train# step  652, loss = 1.2455, cross_entropy loss = 1.2455, 1.3 sec/batch
2019-05-04 13:02:59.114911 #train# step  653, loss = 1.1663, cross_entropy loss = 1.1663, 1.3 sec/batch
2019-05-04 13:03:00.416537 #train# step  654, loss = 1.0957, cross_entropy loss = 1.0957, 1.3 sec/batch
2019-05-04 13:03:01.725208 #train# step  655, loss = 1.1944, cross_entropy loss = 1.1944, 1.3 sec/batch
2019-05-04 13:03:03.037309 #train# step  656, loss = 1.1951, cross_entropy loss = 1.1951, 1.3 sec/batch
2019-05-04 13:03:04.346314 #train# step  657, loss = 1.1477, cross_entropy loss = 1.1477, 1.3 sec/batch
2019-05-04 13:03:05.645399 #train# step  658, loss = 1.1314, cross_entropy loss = 1.1314, 1.3 sec/batch
2019-05-04 13:03:06.958372 #train# step  659, loss = 1.1515, cross_entropy loss = 1.1515, 1.3 sec/batch
2019-05-04 13:03:08.231438 #train# step  660, loss = 1.1175, cross_entropy loss = 1.1175, 1.2 sec/batch
2019-05-04 13:03:09.535318 #train# step  661, loss = 1.1548, cross_entropy loss = 1.1548, 1.3 sec/batch
2019-05-04 13:03:10.850191 #train# step  662, loss = 1.1774, cross_entropy loss = 1.1774, 1.3 sec/batch
2019-05-04 13:03:12.161756 #train# step  663, loss = 1.1010, cross_entropy loss = 1.1010, 1.3 sec/batch
2019-05-04 13:03:13.462770 #train# step  664, loss = 1.1332, cross_entropy loss = 1.1332, 1.3 sec/batch
2019-05-04 13:03:14.754177 #train# step  665, loss = 1.1195, cross_entropy loss = 1.1195, 1.3 sec/batch
2019-05-04 13:03:16.053241 #train# step  666, loss = 1.1006, cross_entropy loss = 1.1006, 1.3 sec/batch
2019-05-04 13:03:17.358050 #train# step  667, loss = 1.1805, cross_entropy loss = 1.1805, 1.3 sec/batch
2019-05-04 13:03:18.674560 #train# step  668, loss = 1.1394, cross_entropy loss = 1.1394, 1.3 sec/batch
2019-05-04 13:03:19.977359 #train# step  669, loss = 1.1608, cross_entropy loss = 1.1608, 1.3 sec/batch
2019-05-04 13:03:21.264783 #train# step  670, loss = 1.1886, cross_entropy loss = 1.1886, 1.3 sec/batch
2019-05-04 13:03:22.572905 #train# step  671, loss = 1.1787, cross_entropy loss = 1.1787, 1.3 sec/batch
2019-05-04 13:03:23.879239 #train# step  672, loss = 1.1613, cross_entropy loss = 1.1613, 1.3 sec/batch
2019-05-04 13:03:25.195906 #train# step  673, loss = 1.1496, cross_entropy loss = 1.1496, 1.3 sec/batch
2019-05-04 13:03:26.482664 #train# step  674, loss = 1.1661, cross_entropy loss = 1.1661, 1.3 sec/batch
2019-05-04 13:03:27.797614 #train# step  675, loss = 1.1443, cross_entropy loss = 1.1443, 1.3 sec/batch
2019-05-04 13:03:29.102112 #train# step  676, loss = 1.0660, cross_entropy loss = 1.0660, 1.3 sec/batch
2019-05-04 13:03:30.409258 #train# step  677, loss = 1.2086, cross_entropy loss = 1.2086, 1.3 sec/batch
2019-05-04 13:03:31.716955 #train# step  678, loss = 1.1207, cross_entropy loss = 1.1207, 1.3 sec/batch
2019-05-04 13:03:33.029052 #train# step  679, loss = 1.2220, cross_entropy loss = 1.2220, 1.3 sec/batch
2019-05-04 13:03:34.341072 #train# step  680, loss = 1.0414, cross_entropy loss = 1.0414, 1.3 sec/batch
2019-05-04 13:03:35.655046 #train# step  681, loss = 1.1992, cross_entropy loss = 1.1992, 1.3 sec/batch
2019-05-04 13:03:36.979831 #train# step  682, loss = 1.1954, cross_entropy loss = 1.1954, 1.3 sec/batch
2019-05-04 13:03:38.259269 #train# step  683, loss = 1.1311, cross_entropy loss = 1.1311, 1.2 sec/batch
2019-05-04 13:03:39.570439 #train# step  684, loss = 1.0865, cross_entropy loss = 1.0865, 1.3 sec/batch
2019-05-04 13:03:40.881179 #train# step  685, loss = 1.1756, cross_entropy loss = 1.1756, 1.3 sec/batch
2019-05-04 13:03:42.193577 #train# step  686, loss = 1.1281, cross_entropy loss = 1.1281, 1.3 sec/batch
2019-05-04 13:03:43.512062 #train# step  687, loss = 1.0809, cross_entropy loss = 1.0809, 1.3 sec/batch
2019-05-04 13:03:44.796251 #train# step  688, loss = 1.1367, cross_entropy loss = 1.1367, 1.2 sec/batch
2019-05-04 13:03:46.104262 #train# step  689, loss = 1.1105, cross_entropy loss = 1.1105, 1.3 sec/batch
2019-05-04 13:03:47.421005 #train# step  690, loss = 1.1478, cross_entropy loss = 1.1478, 1.3 sec/batch
2019-05-04 13:03:48.752904 #train# step  691, loss = 1.1478, cross_entropy loss = 1.1478, 1.3 sec/batch
2019-05-04 13:03:50.071034 #train# step  692, loss = 1.1249, cross_entropy loss = 1.1249, 1.3 sec/batch
2019-05-04 13:03:51.385422 #train# step  693, loss = 1.1825, cross_entropy loss = 1.1825, 1.3 sec/batch
2019-05-04 13:03:52.690855 #train# step  694, loss = 1.0734, cross_entropy loss = 1.0734, 1.3 sec/batch
2019-05-04 13:03:54.007543 #train# step  695, loss = 1.0873, cross_entropy loss = 1.0873, 1.3 sec/batch
2019-05-04 13:03:55.339240 #train# step  696, loss = 1.1408, cross_entropy loss = 1.1408, 1.3 sec/batch
2019-05-04 13:03:56.660741 #train# step  697, loss = 1.1612, cross_entropy loss = 1.1612, 1.3 sec/batch
2019-05-04 13:03:57.984076 #train# step  698, loss = 1.1107, cross_entropy loss = 1.1107, 1.3 sec/batch
2019-05-04 13:03:59.297724 #train# step  699, loss = 1.1162, cross_entropy loss = 1.1162, 1.3 sec/batch
2019-05-04 13:04:00.605006 #train# step  700, loss = 1.1703, cross_entropy loss = 1.1703, 1.3 sec/batch
2019-05-04 13:04:01.920043 #train# step  701, loss = 1.1161, cross_entropy loss = 1.1161, 1.3 sec/batch
2019-05-04 13:04:03.207694 #train# step  702, loss = 1.1355, cross_entropy loss = 1.1355, 1.2 sec/batch
2019-05-04 13:04:04.521159 #train# step  703, loss = 1.1830, cross_entropy loss = 1.1830, 1.3 sec/batch
2019-05-04 13:04:05.794574 #train# step  704, loss = 1.1406, cross_entropy loss = 1.1406, 1.2 sec/batch
2019-05-04 13:04:07.087641 #train# step  705, loss = 1.0757, cross_entropy loss = 1.0757, 1.3 sec/batch
2019-05-04 13:04:08.379886 #train# step  706, loss = 1.0960, cross_entropy loss = 1.0960, 1.3 sec/batch
2019-05-04 13:04:09.668798 #train# step  707, loss = 1.1374, cross_entropy loss = 1.1374, 1.3 sec/batch
2019-05-04 13:04:10.977496 #train# step  708, loss = 1.1389, cross_entropy loss = 1.1389, 1.3 sec/batch
2019-05-04 13:04:12.281631 #train# step  709, loss = 1.1563, cross_entropy loss = 1.1563, 1.3 sec/batch
2019-05-04 13:04:13.562098 #train# step  710, loss = 1.1384, cross_entropy loss = 1.1384, 1.2 sec/batch
2019-05-04 13:04:14.868319 #train# step  711, loss = 1.1415, cross_entropy loss = 1.1415, 1.3 sec/batch
2019-05-04 13:04:16.165281 #train# step  712, loss = 1.0818, cross_entropy loss = 1.0818, 1.3 sec/batch
2019-05-04 13:04:17.463627 #train# step  713, loss = 1.1161, cross_entropy loss = 1.1161, 1.3 sec/batch
2019-05-04 13:04:18.771234 #train# step  714, loss = 1.1424, cross_entropy loss = 1.1424, 1.3 sec/batch
2019-05-04 13:04:20.074130 #train# step  715, loss = 1.0994, cross_entropy loss = 1.0994, 1.3 sec/batch
2019-05-04 13:04:21.366322 #train# step  716, loss = 1.1178, cross_entropy loss = 1.1178, 1.3 sec/batch
2019-05-04 13:04:22.669013 #train# step  717, loss = 1.1758, cross_entropy loss = 1.1758, 1.3 sec/batch
2019-05-04 13:04:23.967994 #train# step  718, loss = 1.1191, cross_entropy loss = 1.1191, 1.3 sec/batch
2019-05-04 13:04:25.260936 #train# step  719, loss = 1.1518, cross_entropy loss = 1.1518, 1.3 sec/batch
2019-05-04 13:04:26.563634 #train# step  720, loss = 1.1123, cross_entropy loss = 1.1123, 1.3 sec/batch
2019-05-04 13:04:27.875084 #train# step  721, loss = 1.1596, cross_entropy loss = 1.1596, 1.3 sec/batch
2019-05-04 13:04:29.175564 #train# step  722, loss = 1.1166, cross_entropy loss = 1.1166, 1.3 sec/batch
2019-05-04 13:04:30.476920 #train# step  723, loss = 1.1535, cross_entropy loss = 1.1535, 1.3 sec/batch
2019-05-04 13:04:31.746805 #train# step  724, loss = 1.1301, cross_entropy loss = 1.1301, 1.2 sec/batch
2019-05-04 13:04:33.053653 #train# step  725, loss = 1.1016, cross_entropy loss = 1.1016, 1.3 sec/batch
2019-05-04 13:04:34.365272 #train# step  726, loss = 1.1057, cross_entropy loss = 1.1057, 1.3 sec/batch
2019-05-04 13:04:35.672194 #train# step  727, loss = 1.1247, cross_entropy loss = 1.1247, 1.3 sec/batch
2019-05-04 13:04:36.992314 #train# step  728, loss = 1.1664, cross_entropy loss = 1.1664, 1.3 sec/batch
2019-05-04 13:04:38.300111 #train# step  729, loss = 1.1652, cross_entropy loss = 1.1652, 1.3 sec/batch
2019-05-04 13:04:39.575910 #train# step  730, loss = 1.1875, cross_entropy loss = 1.1875, 1.2 sec/batch
2019-05-04 13:04:40.857796 #train# step  731, loss = 1.1435, cross_entropy loss = 1.1435, 1.2 sec/batch
2019-05-04 13:04:42.147608 #train# step  732, loss = 1.0733, cross_entropy loss = 1.0733, 1.3 sec/batch
2019-05-04 13:04:43.430747 #train# step  733, loss = 1.0693, cross_entropy loss = 1.0693, 1.2 sec/batch
2019-05-04 13:04:44.740957 #train# step  734, loss = 1.1519, cross_entropy loss = 1.1519, 1.3 sec/batch
2019-05-04 13:04:46.031038 #train# step  735, loss = 1.1069, cross_entropy loss = 1.1069, 1.3 sec/batch
2019-05-04 13:04:47.332191 #train# step  736, loss = 1.1676, cross_entropy loss = 1.1676, 1.3 sec/batch
2019-05-04 13:04:48.618678 #train# step  737, loss = 1.0566, cross_entropy loss = 1.0566, 1.3 sec/batch
2019-05-04 13:04:49.923550 #train# step  738, loss = 1.1336, cross_entropy loss = 1.1336, 1.3 sec/batch
2019-05-04 13:04:51.231313 #train# step  739, loss = 1.1270, cross_entropy loss = 1.1270, 1.3 sec/batch
2019-05-04 13:04:52.536636 #train# step  740, loss = 1.0887, cross_entropy loss = 1.0887, 1.3 sec/batch
2019-05-04 13:04:53.826755 #train# step  741, loss = 1.1098, cross_entropy loss = 1.1098, 1.3 sec/batch
2019-05-04 13:04:55.120404 #train# step  742, loss = 1.1183, cross_entropy loss = 1.1183, 1.3 sec/batch
2019-05-04 13:04:56.420450 #train# step  743, loss = 1.1018, cross_entropy loss = 1.1018, 1.3 sec/batch
2019-05-04 13:04:57.710230 #train# step  744, loss = 1.1090, cross_entropy loss = 1.1090, 1.3 sec/batch
2019-05-04 13:04:59.026762 #train# step  745, loss = 1.1581, cross_entropy loss = 1.1581, 1.3 sec/batch
2019-05-04 13:05:00.329641 #train# step  746, loss = 1.1753, cross_entropy loss = 1.1753, 1.3 sec/batch
2019-05-04 13:05:01.618713 #train# step  747, loss = 1.1299, cross_entropy loss = 1.1299, 1.3 sec/batch
2019-05-04 13:05:02.921993 #train# step  748, loss = 1.1339, cross_entropy loss = 1.1339, 1.3 sec/batch
2019-05-04 13:05:04.226688 #train# step  749, loss = 1.0921, cross_entropy loss = 1.0921, 1.3 sec/batch
2019-05-04 13:05:05.527592 #train# step  750, loss = 1.1437, cross_entropy loss = 1.1437, 1.3 sec/batch
2019-05-04 13:05:06.806880 #train# step  751, loss = 1.1912, cross_entropy loss = 1.1912, 1.2 sec/batch
2019-05-04 13:05:08.118404 #train# step  752, loss = 1.1412, cross_entropy loss = 1.1412, 1.3 sec/batch
2019-05-04 13:05:09.437781 #train# step  753, loss = 1.1305, cross_entropy loss = 1.1305, 1.3 sec/batch
2019-05-04 13:05:10.737951 #train# step  754, loss = 1.0858, cross_entropy loss = 1.0858, 1.3 sec/batch
2019-05-04 13:05:12.046053 #train# step  755, loss = 1.1558, cross_entropy loss = 1.1558, 1.3 sec/batch
2019-05-04 13:05:13.329846 #train# step  756, loss = 1.1403, cross_entropy loss = 1.1403, 1.2 sec/batch
2019-05-04 13:05:14.640128 #train# step  757, loss = 1.0975, cross_entropy loss = 1.0975, 1.3 sec/batch
2019-05-04 13:05:15.918850 #train# step  758, loss = 1.1122, cross_entropy loss = 1.1122, 1.2 sec/batch
2019-05-04 13:05:17.221879 #train# step  759, loss = 1.1797, cross_entropy loss = 1.1797, 1.3 sec/batch
2019-05-04 13:05:18.518288 #train# step  760, loss = 1.1208, cross_entropy loss = 1.1208, 1.3 sec/batch
2019-05-04 13:05:19.815237 #train# step  761, loss = 1.1308, cross_entropy loss = 1.1308, 1.3 sec/batch
2019-05-04 13:05:21.124968 #train# step  762, loss = 1.1245, cross_entropy loss = 1.1245, 1.3 sec/batch
2019-05-04 13:05:22.443227 #train# step  763, loss = 1.1561, cross_entropy loss = 1.1561, 1.3 sec/batch
2019-05-04 13:05:23.765834 #train# step  764, loss = 1.1589, cross_entropy loss = 1.1589, 1.3 sec/batch
2019-05-04 13:05:25.099533 #train# step  765, loss = 1.1008, cross_entropy loss = 1.1008, 1.3 sec/batch
2019-05-04 13:05:26.397885 #train# step  766, loss = 1.1212, cross_entropy loss = 1.1212, 1.2 sec/batch
2019-05-04 13:05:27.711034 #train# step  767, loss = 1.0873, cross_entropy loss = 1.0873, 1.3 sec/batch
2019-05-04 13:05:29.031991 #train# step  768, loss = 1.1053, cross_entropy loss = 1.1053, 1.3 sec/batch
2019-05-04 13:05:30.349362 #train# step  769, loss = 1.0830, cross_entropy loss = 1.0830, 1.3 sec/batch
2019-05-04 13:05:31.667412 #train# step  770, loss = 1.1749, cross_entropy loss = 1.1749, 1.3 sec/batch
2019-05-04 13:05:32.985703 #train# step  771, loss = 1.1291, cross_entropy loss = 1.1291, 1.3 sec/batch
2019-05-04 13:05:34.310320 #train# step  772, loss = 1.1999, cross_entropy loss = 1.1999, 1.3 sec/batch
2019-05-04 13:05:35.634633 #train# step  773, loss = 1.1046, cross_entropy loss = 1.1046, 1.3 sec/batch
2019-05-04 13:05:36.952742 #train# step  774, loss = 1.1039, cross_entropy loss = 1.1039, 1.3 sec/batch
2019-05-04 13:05:38.273746 #train# step  775, loss = 1.0665, cross_entropy loss = 1.0665, 1.3 sec/batch
2019-05-04 13:05:39.596495 #train# step  776, loss = 1.0959, cross_entropy loss = 1.0959, 1.3 sec/batch
2019-05-04 13:05:40.925654 #train# step  777, loss = 1.1059, cross_entropy loss = 1.1059, 1.3 sec/batch
2019-05-04 13:05:42.244744 #train# step  778, loss = 1.0905, cross_entropy loss = 1.0905, 1.3 sec/batch
2019-05-04 13:05:43.563747 #train# step  779, loss = 1.1037, cross_entropy loss = 1.1037, 1.3 sec/batch
2019-05-04 13:05:44.867328 #train# step  780, loss = 1.0415, cross_entropy loss = 1.0415, 1.3 sec/batch
2019-05-04 13:05:46.171305 #train# step  781, loss = 1.1227, cross_entropy loss = 1.1227, 1.3 sec/batch
2019-05-04 13:05:47.469366 #train# step  782, loss = 1.1035, cross_entropy loss = 1.1035, 1.3 sec/batch
2019-05-04 13:05:48.780304 #train# step  783, loss = 1.0792, cross_entropy loss = 1.0792, 1.3 sec/batch
2019-05-04 13:05:50.096242 #train# step  784, loss = 1.0733, cross_entropy loss = 1.0733, 1.3 sec/batch
2019-05-04 13:05:51.409780 #train# step  785, loss = 1.0862, cross_entropy loss = 1.0862, 1.3 sec/batch
2019-05-04 13:05:52.700753 #train# step  786, loss = 1.1068, cross_entropy loss = 1.1068, 1.3 sec/batch
2019-05-04 13:05:54.012840 #train# step  787, loss = 1.1514, cross_entropy loss = 1.1514, 1.3 sec/batch
2019-05-04 13:05:55.293872 #train# step  788, loss = 1.1039, cross_entropy loss = 1.1039, 1.2 sec/batch
2019-05-04 13:05:56.599254 #train# step  789, loss = 1.0762, cross_entropy loss = 1.0762, 1.3 sec/batch
2019-05-04 13:05:57.906712 #train# step  790, loss = 1.1730, cross_entropy loss = 1.1730, 1.3 sec/batch
2019-05-04 13:05:59.215953 #train# step  791, loss = 1.1775, cross_entropy loss = 1.1775, 1.3 sec/batch
2019-05-04 13:06:00.515508 #train# step  792, loss = 1.0856, cross_entropy loss = 1.0856, 1.3 sec/batch
2019-05-04 13:06:01.817182 #train# step  793, loss = 1.0804, cross_entropy loss = 1.0804, 1.3 sec/batch
2019-05-04 13:06:03.139755 #train# step  794, loss = 1.1606, cross_entropy loss = 1.1606, 1.3 sec/batch
2019-05-04 13:06:04.443002 #train# step  795, loss = 1.1773, cross_entropy loss = 1.1773, 1.3 sec/batch
2019-05-04 13:06:05.771188 #train# step  796, loss = 1.1113, cross_entropy loss = 1.1113, 1.3 sec/batch
2019-05-04 13:06:07.081710 #train# step  797, loss = 1.1110, cross_entropy loss = 1.1110, 1.3 sec/batch
2019-05-04 13:06:08.389640 #train# step  798, loss = 1.1190, cross_entropy loss = 1.1190, 1.3 sec/batch
2019-05-04 13:06:09.709120 #train# step  799, loss = 1.0546, cross_entropy loss = 1.0546, 1.3 sec/batch
2019-05-04 13:06:11.017225 #train# step  800, loss = 1.1521, cross_entropy loss = 1.1521, 1.3 sec/batch
2019-05-04 13:06:12.342524 #train# step  801, loss = 1.0914, cross_entropy loss = 1.0914, 1.3 sec/batch
2019-05-04 13:06:13.654932 #train# step  802, loss = 1.1039, cross_entropy loss = 1.1039, 1.3 sec/batch
2019-05-04 13:06:14.954389 #train# step  803, loss = 1.1046, cross_entropy loss = 1.1046, 1.3 sec/batch
2019-05-04 13:06:16.254903 #train# step  804, loss = 1.1795, cross_entropy loss = 1.1795, 1.3 sec/batch
2019-05-04 13:06:17.566673 #train# step  805, loss = 1.0849, cross_entropy loss = 1.0849, 1.3 sec/batch
2019-05-04 13:06:18.873374 #train# step  806, loss = 1.0907, cross_entropy loss = 1.0907, 1.3 sec/batch
2019-05-04 13:06:20.181880 #train# step  807, loss = 1.0419, cross_entropy loss = 1.0419, 1.3 sec/batch
2019-05-04 13:06:21.502937 #train# step  808, loss = 1.1826, cross_entropy loss = 1.1826, 1.3 sec/batch
2019-05-04 13:06:22.820060 #train# step  809, loss = 1.0880, cross_entropy loss = 1.0880, 1.3 sec/batch
2019-05-04 13:06:24.146610 #train# step  810, loss = 1.1278, cross_entropy loss = 1.1278, 1.3 sec/batch
2019-05-04 13:06:25.451892 #train# step  811, loss = 1.0961, cross_entropy loss = 1.0961, 1.3 sec/batch
2019-05-04 13:06:26.765402 #train# step  812, loss = 1.0691, cross_entropy loss = 1.0691, 1.3 sec/batch
2019-05-04 13:06:28.092822 #train# step  813, loss = 1.0685, cross_entropy loss = 1.0685, 1.3 sec/batch
2019-05-04 13:06:29.408151 #train# step  814, loss = 1.0592, cross_entropy loss = 1.0592, 1.3 sec/batch
2019-05-04 13:06:30.735270 #train# step  815, loss = 1.0610, cross_entropy loss = 1.0610, 1.3 sec/batch
2019-05-04 13:06:32.080840 #train# step  816, loss = 1.1327, cross_entropy loss = 1.1327, 1.3 sec/batch
2019-05-04 13:06:33.422549 #train# step  817, loss = 1.1221, cross_entropy loss = 1.1221, 1.3 sec/batch
2019-05-04 13:06:34.734761 #train# step  818, loss = 1.1028, cross_entropy loss = 1.1028, 1.3 sec/batch
2019-05-04 13:06:36.051942 #train# step  819, loss = 1.0539, cross_entropy loss = 1.0539, 1.3 sec/batch
2019-05-04 13:06:37.364874 #train# step  820, loss = 1.0911, cross_entropy loss = 1.0911, 1.3 sec/batch
2019-05-04 13:06:38.701407 #train# step  821, loss = 1.1254, cross_entropy loss = 1.1254, 1.3 sec/batch
2019-05-04 13:06:40.015468 #train# step  822, loss = 1.1143, cross_entropy loss = 1.1143, 1.3 sec/batch
2019-05-04 13:06:41.328600 #train# step  823, loss = 1.1125, cross_entropy loss = 1.1125, 1.3 sec/batch
2019-05-04 13:06:42.647002 #train# step  824, loss = 1.0736, cross_entropy loss = 1.0736, 1.3 sec/batch
2019-05-04 13:06:43.968555 #train# step  825, loss = 1.0943, cross_entropy loss = 1.0943, 1.3 sec/batch
2019-05-04 13:06:45.283953 #train# step  826, loss = 1.0875, cross_entropy loss = 1.0875, 1.3 sec/batch
2019-05-04 13:06:46.579767 #train# step  827, loss = 1.1154, cross_entropy loss = 1.1154, 1.3 sec/batch
2019-05-04 13:06:47.885403 #train# step  828, loss = 1.0939, cross_entropy loss = 1.0939, 1.3 sec/batch
2019-05-04 13:06:49.209876 #train# step  829, loss = 1.1105, cross_entropy loss = 1.1105, 1.3 sec/batch
2019-05-04 13:06:50.540670 #train# step  830, loss = 1.0542, cross_entropy loss = 1.0542, 1.3 sec/batch
2019-05-04 13:06:51.849100 #train# step  831, loss = 1.0759, cross_entropy loss = 1.0759, 1.3 sec/batch
2019-05-04 13:06:53.161655 #train# step  832, loss = 1.1783, cross_entropy loss = 1.1783, 1.3 sec/batch
2019-05-04 13:06:54.461741 #train# step  833, loss = 1.0824, cross_entropy loss = 1.0824, 1.3 sec/batch
2019-05-04 13:06:55.764191 #train# step  834, loss = 1.1333, cross_entropy loss = 1.1333, 1.3 sec/batch
2019-05-04 13:06:57.080847 #train# step  835, loss = 1.0786, cross_entropy loss = 1.0786, 1.3 sec/batch
2019-05-04 13:06:58.390661 #train# step  836, loss = 1.1297, cross_entropy loss = 1.1297, 1.3 sec/batch
2019-05-04 13:06:59.702357 #train# step  837, loss = 1.0895, cross_entropy loss = 1.0895, 1.3 sec/batch
2019-05-04 13:07:01.011789 #train# step  838, loss = 1.1109, cross_entropy loss = 1.1109, 1.3 sec/batch
2019-05-04 13:07:02.319798 #train# step  839, loss = 1.1282, cross_entropy loss = 1.1282, 1.3 sec/batch
2019-05-04 13:07:03.633874 #train# step  840, loss = 1.1035, cross_entropy loss = 1.1035, 1.3 sec/batch
2019-05-04 13:07:04.940493 #train# step  841, loss = 1.1022, cross_entropy loss = 1.1022, 1.3 sec/batch
2019-05-04 13:07:06.250396 #train# step  842, loss = 1.1071, cross_entropy loss = 1.1071, 1.3 sec/batch
2019-05-04 13:07:07.564933 #train# step  843, loss = 1.0611, cross_entropy loss = 1.0611, 1.3 sec/batch
2019-05-04 13:07:08.873972 #train# step  844, loss = 1.1337, cross_entropy loss = 1.1337, 1.3 sec/batch
2019-05-04 13:07:10.184915 #train# step  845, loss = 1.0956, cross_entropy loss = 1.0956, 1.3 sec/batch
2019-05-04 13:07:11.487138 #train# step  846, loss = 1.1937, cross_entropy loss = 1.1937, 1.3 sec/batch
2019-05-04 13:07:12.791838 #train# step  847, loss = 1.1216, cross_entropy loss = 1.1216, 1.3 sec/batch
2019-05-04 13:07:14.110518 #train# step  848, loss = 1.1370, cross_entropy loss = 1.1370, 1.3 sec/batch
2019-05-04 13:07:15.413721 #train# step  849, loss = 1.1168, cross_entropy loss = 1.1168, 1.3 sec/batch
2019-05-04 13:07:16.722866 #train# step  850, loss = 1.0534, cross_entropy loss = 1.0534, 1.3 sec/batch
2019-05-04 13:07:18.036934 #train# step  851, loss = 1.1114, cross_entropy loss = 1.1114, 1.3 sec/batch
2019-05-04 13:07:19.358148 #train# step  852, loss = 1.0737, cross_entropy loss = 1.0737, 1.3 sec/batch
2019-05-04 13:07:20.664905 #train# step  853, loss = 1.1283, cross_entropy loss = 1.1283, 1.3 sec/batch
2019-05-04 13:07:21.979203 #train# step  854, loss = 1.0949, cross_entropy loss = 1.0949, 1.3 sec/batch
2019-05-04 13:07:23.272573 #train# step  855, loss = 1.0696, cross_entropy loss = 1.0696, 1.3 sec/batch
2019-05-04 13:07:24.568464 #train# step  856, loss = 1.0468, cross_entropy loss = 1.0468, 1.3 sec/batch
2019-05-04 13:07:25.879820 #train# step  857, loss = 1.1018, cross_entropy loss = 1.1018, 1.3 sec/batch
2019-05-04 13:07:27.180575 #train# step  858, loss = 1.1039, cross_entropy loss = 1.1039, 1.3 sec/batch
2019-05-04 13:07:28.491711 #train# step  859, loss = 1.0831, cross_entropy loss = 1.0831, 1.3 sec/batch
2019-05-04 13:07:29.778264 #train# step  860, loss = 1.0274, cross_entropy loss = 1.0274, 1.3 sec/batch
2019-05-04 13:07:31.091390 #train# step  861, loss = 1.0641, cross_entropy loss = 1.0641, 1.3 sec/batch
2019-05-04 13:07:32.398520 #train# step  862, loss = 1.0909, cross_entropy loss = 1.0909, 1.3 sec/batch
2019-05-04 13:07:33.702451 #train# step  863, loss = 1.1056, cross_entropy loss = 1.1056, 1.3 sec/batch
2019-05-04 13:07:35.012412 #train# step  864, loss = 1.0407, cross_entropy loss = 1.0407, 1.3 sec/batch
2019-05-04 13:07:36.311973 #train# step  865, loss = 1.1566, cross_entropy loss = 1.1566, 1.3 sec/batch
2019-05-04 13:07:37.621207 #train# step  866, loss = 1.0893, cross_entropy loss = 1.0893, 1.3 sec/batch
2019-05-04 13:07:38.929405 #train# step  867, loss = 1.1283, cross_entropy loss = 1.1283, 1.3 sec/batch
2019-05-04 13:07:40.255469 #train# step  868, loss = 1.1550, cross_entropy loss = 1.1550, 1.3 sec/batch
2019-05-04 13:07:41.563718 #train# step  869, loss = 1.0724, cross_entropy loss = 1.0724, 1.3 sec/batch
2019-05-04 13:07:42.868843 #train# step  870, loss = 1.0823, cross_entropy loss = 1.0823, 1.3 sec/batch
2019-05-04 13:07:44.188446 #train# step  871, loss = 1.0994, cross_entropy loss = 1.0994, 1.3 sec/batch
2019-05-04 13:07:45.498531 #train# step  872, loss = 1.0403, cross_entropy loss = 1.0403, 1.3 sec/batch
2019-05-04 13:07:46.814894 #train# step  873, loss = 1.1419, cross_entropy loss = 1.1419, 1.3 sec/batch
2019-05-04 13:07:48.125893 #train# step  874, loss = 1.0866, cross_entropy loss = 1.0866, 1.3 sec/batch
2019-05-04 13:07:49.443593 #train# step  875, loss = 1.0984, cross_entropy loss = 1.0984, 1.3 sec/batch
2019-05-04 13:07:50.762818 #train# step  876, loss = 1.0931, cross_entropy loss = 1.0931, 1.3 sec/batch
2019-05-04 13:07:52.075614 #train# step  877, loss = 1.1256, cross_entropy loss = 1.1256, 1.3 sec/batch
2019-05-04 13:07:53.391377 #train# step  878, loss = 1.1316, cross_entropy loss = 1.1316, 1.3 sec/batch
2019-05-04 13:07:54.664141 #train# step  879, loss = 1.0407, cross_entropy loss = 1.0407, 1.2 sec/batch
2019-05-04 13:07:55.933821 #train# step  880, loss = 1.0922, cross_entropy loss = 1.0922, 1.2 sec/batch
2019-05-04 13:07:57.240907 #train# step  881, loss = 1.0565, cross_entropy loss = 1.0565, 1.3 sec/batch
2019-05-04 13:07:58.542243 #train# step  882, loss = 1.0652, cross_entropy loss = 1.0652, 1.3 sec/batch
2019-05-04 13:07:59.851772 #train# step  883, loss = 1.0736, cross_entropy loss = 1.0736, 1.3 sec/batch
2019-05-04 13:08:01.169182 #train# step  884, loss = 1.1164, cross_entropy loss = 1.1164, 1.3 sec/batch
2019-05-04 13:08:02.489340 #train# step  885, loss = 1.0508, cross_entropy loss = 1.0508, 1.3 sec/batch
2019-05-04 13:08:03.808152 #train# step  886, loss = 1.0978, cross_entropy loss = 1.0978, 1.3 sec/batch
2019-05-04 13:08:05.133911 #train# step  887, loss = 1.0993, cross_entropy loss = 1.0993, 1.3 sec/batch
2019-05-04 13:08:06.452546 #train# step  888, loss = 1.1091, cross_entropy loss = 1.1091, 1.3 sec/batch
2019-05-04 13:08:07.777456 #train# step  889, loss = 1.1580, cross_entropy loss = 1.1580, 1.3 sec/batch
2019-05-04 13:08:09.165659 #train# step  890, loss = 1.0895, cross_entropy loss = 1.0895, 1.3 sec/batch
2019-05-04 13:08:10.492330 #train# step  891, loss = 1.0755, cross_entropy loss = 1.0755, 1.3 sec/batch
2019-05-04 13:08:11.809154 #train# step  892, loss = 1.1527, cross_entropy loss = 1.1527, 1.3 sec/batch
2019-05-04 13:08:13.132640 #train# step  893, loss = 1.0976, cross_entropy loss = 1.0976, 1.3 sec/batch
2019-05-04 13:08:14.454923 #train# step  894, loss = 1.0384, cross_entropy loss = 1.0384, 1.3 sec/batch
2019-05-04 13:08:15.820759 #train# step  895, loss = 1.0803, cross_entropy loss = 1.0803, 1.3 sec/batch
2019-05-04 13:08:17.129941 #train# step  896, loss = 1.0365, cross_entropy loss = 1.0365, 1.3 sec/batch
2019-05-04 13:08:18.416699 #train# step  897, loss = 1.0783, cross_entropy loss = 1.0783, 1.2 sec/batch
2019-05-04 13:08:19.758015 #train# step  898, loss = 1.0984, cross_entropy loss = 1.0984, 1.3 sec/batch
2019-05-04 13:08:21.058240 #train# step  899, loss = 1.0826, cross_entropy loss = 1.0826, 1.3 sec/batch
2019-05-04 13:08:22.403460 #train# step  900, loss = 1.1558, cross_entropy loss = 1.1558, 1.3 sec/batch
2019-05-04 13:08:23.728550 #train# step  901, loss = 1.0996, cross_entropy loss = 1.0996, 1.3 sec/batch
2019-05-04 13:08:25.034796 #train# step  902, loss = 1.0728, cross_entropy loss = 1.0728, 1.3 sec/batch
2019-05-04 13:08:26.326241 #train# step  903, loss = 1.1121, cross_entropy loss = 1.1121, 1.2 sec/batch
2019-05-04 13:08:27.651321 #train# step  904, loss = 1.0642, cross_entropy loss = 1.0642, 1.3 sec/batch
2019-05-04 13:08:28.969948 #train# step  905, loss = 1.1442, cross_entropy loss = 1.1442, 1.3 sec/batch
2019-05-04 13:08:30.275425 #train# step  906, loss = 1.1147, cross_entropy loss = 1.1147, 1.3 sec/batch
2019-05-04 13:08:31.595910 #train# step  907, loss = 1.0790, cross_entropy loss = 1.0790, 1.3 sec/batch
2019-05-04 13:08:32.926479 #train# step  908, loss = 1.0447, cross_entropy loss = 1.0447, 1.3 sec/batch
2019-05-04 13:08:34.252912 #train# step  909, loss = 1.0299, cross_entropy loss = 1.0299, 1.3 sec/batch
2019-05-04 13:08:35.572944 #train# step  910, loss = 1.0472, cross_entropy loss = 1.0472, 1.3 sec/batch
2019-05-04 13:08:36.892672 #train# step  911, loss = 1.1460, cross_entropy loss = 1.1460, 1.3 sec/batch
2019-05-04 13:08:38.214924 #train# step  912, loss = 1.1090, cross_entropy loss = 1.1090, 1.3 sec/batch
2019-05-04 13:08:39.524277 #train# step  913, loss = 1.1109, cross_entropy loss = 1.1109, 1.3 sec/batch
2019-05-04 13:08:40.833186 #train# step  914, loss = 1.0798, cross_entropy loss = 1.0798, 1.3 sec/batch
2019-05-04 13:08:42.134045 #train# step  915, loss = 1.1007, cross_entropy loss = 1.1007, 1.3 sec/batch
2019-05-04 13:08:43.425759 #train# step  916, loss = 1.1101, cross_entropy loss = 1.1101, 1.3 sec/batch
2019-05-04 13:08:44.715305 #train# step  917, loss = 1.0787, cross_entropy loss = 1.0787, 1.3 sec/batch
2019-05-04 13:08:46.024306 #train# step  918, loss = 1.0831, cross_entropy loss = 1.0831, 1.3 sec/batch
2019-05-04 13:08:47.335245 #train# step  919, loss = 1.0907, cross_entropy loss = 1.0907, 1.3 sec/batch
2019-05-04 13:08:48.618147 #train# step  920, loss = 1.0803, cross_entropy loss = 1.0803, 1.2 sec/batch
2019-05-04 13:08:49.917580 #train# step  921, loss = 1.1488, cross_entropy loss = 1.1488, 1.3 sec/batch
2019-05-04 13:08:51.198381 #train# step  922, loss = 1.1133, cross_entropy loss = 1.1133, 1.2 sec/batch
2019-05-04 13:08:52.499237 #train# step  923, loss = 1.0317, cross_entropy loss = 1.0317, 1.3 sec/batch
2019-05-04 13:08:53.786311 #train# step  924, loss = 1.0976, cross_entropy loss = 1.0976, 1.3 sec/batch
2019-05-04 13:08:55.097191 #train# step  925, loss = 1.0658, cross_entropy loss = 1.0658, 1.3 sec/batch
2019-05-04 13:08:56.406276 #train# step  926, loss = 1.0876, cross_entropy loss = 1.0876, 1.3 sec/batch
2019-05-04 13:08:57.720395 #train# step  927, loss = 1.1275, cross_entropy loss = 1.1275, 1.3 sec/batch
2019-05-04 13:08:59.030723 #train# step  928, loss = 1.0378, cross_entropy loss = 1.0378, 1.3 sec/batch
2019-05-04 13:09:00.345208 #train# step  929, loss = 1.1145, cross_entropy loss = 1.1145, 1.3 sec/batch
2019-05-04 13:09:01.648695 #train# step  930, loss = 1.0737, cross_entropy loss = 1.0737, 1.3 sec/batch
2019-05-04 13:09:02.956069 #train# step  931, loss = 1.0860, cross_entropy loss = 1.0860, 1.3 sec/batch
2019-05-04 13:09:04.255441 #train# step  932, loss = 1.0901, cross_entropy loss = 1.0901, 1.3 sec/batch
2019-05-04 13:09:05.567660 #train# step  933, loss = 1.0822, cross_entropy loss = 1.0822, 1.3 sec/batch
2019-05-04 13:09:06.888820 #train# step  934, loss = 1.1606, cross_entropy loss = 1.1606, 1.3 sec/batch
2019-05-04 13:09:08.195360 #train# step  935, loss = 1.0587, cross_entropy loss = 1.0587, 1.3 sec/batch
2019-05-04 13:09:09.513048 #train# step  936, loss = 1.0978, cross_entropy loss = 1.0978, 1.3 sec/batch
2019-05-04 13:09:10.819777 #train# step  937, loss = 1.1073, cross_entropy loss = 1.1073, 1.3 sec/batch
2019-05-04 13:09:12.136273 #train# step  938, loss = 1.0733, cross_entropy loss = 1.0733, 1.3 sec/batch
2019-05-04 13:09:13.451039 #train# step  939, loss = 1.0740, cross_entropy loss = 1.0740, 1.3 sec/batch
2019-05-04 13:09:14.768891 #train# step  940, loss = 1.0839, cross_entropy loss = 1.0839, 1.3 sec/batch
2019-05-04 13:09:16.089364 #train# step  941, loss = 1.0268, cross_entropy loss = 1.0268, 1.3 sec/batch
2019-05-04 13:09:17.404967 #train# step  942, loss = 1.0666, cross_entropy loss = 1.0666, 1.3 sec/batch
2019-05-04 13:09:18.713903 #train# step  943, loss = 1.1627, cross_entropy loss = 1.1627, 1.3 sec/batch
2019-05-04 13:09:20.029188 #train# step  944, loss = 1.0672, cross_entropy loss = 1.0672, 1.3 sec/batch
2019-05-04 13:09:21.318140 #train# step  945, loss = 1.0754, cross_entropy loss = 1.0754, 1.3 sec/batch
2019-05-04 13:09:22.640727 #train# step  946, loss = 1.0681, cross_entropy loss = 1.0681, 1.3 sec/batch
2019-05-04 13:09:23.952791 #train# step  947, loss = 1.0981, cross_entropy loss = 1.0981, 1.3 sec/batch
2019-05-04 13:09:25.273971 #train# step  948, loss = 1.1143, cross_entropy loss = 1.1143, 1.3 sec/batch
2019-05-04 13:09:26.596970 #train# step  949, loss = 1.0794, cross_entropy loss = 1.0794, 1.3 sec/batch
2019-05-04 13:09:27.873859 #train# step  950, loss = 1.0840, cross_entropy loss = 1.0840, 1.2 sec/batch
2019-05-04 13:09:29.200593 #train# step  951, loss = 1.1273, cross_entropy loss = 1.1273, 1.3 sec/batch
2019-05-04 13:09:30.503840 #train# step  952, loss = 1.0872, cross_entropy loss = 1.0872, 1.3 sec/batch
2019-05-04 13:09:31.823201 #train# step  953, loss = 1.0802, cross_entropy loss = 1.0802, 1.3 sec/batch
2019-05-04 13:09:33.148261 #train# step  954, loss = 0.9737, cross_entropy loss = 0.9737, 1.3 sec/batch
2019-05-04 13:09:34.451387 #train# step  955, loss = 1.0986, cross_entropy loss = 1.0986, 1.3 sec/batch
2019-05-04 13:09:35.784541 #train# step  956, loss = 1.0054, cross_entropy loss = 1.0054, 1.3 sec/batch
2019-05-04 13:09:37.066610 #train# step  957, loss = 1.0702, cross_entropy loss = 1.0702, 1.2 sec/batch
2019-05-04 13:09:38.363445 #train# step  958, loss = 1.0671, cross_entropy loss = 1.0671, 1.3 sec/batch
2019-05-04 13:09:39.666193 #train# step  959, loss = 1.1334, cross_entropy loss = 1.1334, 1.3 sec/batch
2019-05-04 13:09:40.962997 #train# step  960, loss = 1.0842, cross_entropy loss = 1.0842, 1.3 sec/batch
2019-05-04 13:09:42.280497 #train# step  961, loss = 1.1356, cross_entropy loss = 1.1356, 1.3 sec/batch
2019-05-04 13:09:43.586393 #train# step  962, loss = 1.0786, cross_entropy loss = 1.0786, 1.3 sec/batch
2019-05-04 13:09:44.889732 #train# step  963, loss = 1.1075, cross_entropy loss = 1.1075, 1.3 sec/batch
2019-05-04 13:09:46.204320 #train# step  964, loss = 1.1413, cross_entropy loss = 1.1413, 1.3 sec/batch
2019-05-04 13:09:47.507132 #train# step  965, loss = 1.0593, cross_entropy loss = 1.0593, 1.3 sec/batch
2019-05-04 13:09:48.812585 #train# step  966, loss = 1.0883, cross_entropy loss = 1.0883, 1.3 sec/batch
2019-05-04 13:09:50.130314 #train# step  967, loss = 1.1134, cross_entropy loss = 1.1134, 1.3 sec/batch
2019-05-04 13:09:51.444678 #train# step  968, loss = 1.0574, cross_entropy loss = 1.0574, 1.3 sec/batch
2019-05-04 13:09:52.765533 #train# step  969, loss = 1.0427, cross_entropy loss = 1.0427, 1.3 sec/batch
2019-05-04 13:09:54.058289 #train# step  970, loss = 1.1253, cross_entropy loss = 1.1253, 1.2 sec/batch
2019-05-04 13:09:55.386314 #train# step  971, loss = 1.0447, cross_entropy loss = 1.0447, 1.3 sec/batch
2019-05-04 13:09:56.714020 #train# step  972, loss = 1.1088, cross_entropy loss = 1.1088, 1.3 sec/batch
2019-05-04 13:09:58.043574 #train# step  973, loss = 1.0625, cross_entropy loss = 1.0625, 1.3 sec/batch
2019-05-04 13:09:59.359826 #train# step  974, loss = 1.1200, cross_entropy loss = 1.1200, 1.3 sec/batch
2019-05-04 13:10:00.660977 #train# step  975, loss = 1.0737, cross_entropy loss = 1.0737, 1.3 sec/batch
2019-05-04 13:10:01.983800 #train# step  976, loss = 1.0707, cross_entropy loss = 1.0707, 1.3 sec/batch
2019-05-04 13:10:03.308945 #train# step  977, loss = 1.0370, cross_entropy loss = 1.0370, 1.3 sec/batch
2019-05-04 13:10:04.611338 #train# step  978, loss = 1.1015, cross_entropy loss = 1.1015, 1.3 sec/batch
2019-05-04 13:10:05.905196 #train# step  979, loss = 1.1243, cross_entropy loss = 1.1243, 1.2 sec/batch
2019-05-04 13:10:07.239383 #train# step  980, loss = 1.0745, cross_entropy loss = 1.0745, 1.3 sec/batch
2019-05-04 13:10:08.599033 #train# step  981, loss = 1.0468, cross_entropy loss = 1.0468, 1.3 sec/batch
2019-05-04 13:10:09.913829 #train# step  982, loss = 1.1139, cross_entropy loss = 1.1139, 1.3 sec/batch
2019-05-04 13:10:11.215213 #train# step  983, loss = 1.1016, cross_entropy loss = 1.1016, 1.3 sec/batch
2019-05-04 13:10:12.550901 #train# step  984, loss = 1.0235, cross_entropy loss = 1.0235, 1.3 sec/batch
2019-05-04 13:10:13.889372 #train# step  985, loss = 1.0665, cross_entropy loss = 1.0665, 1.3 sec/batch
2019-05-04 13:10:15.213317 #train# step  986, loss = 1.0793, cross_entropy loss = 1.0793, 1.3 sec/batch
2019-05-04 13:10:16.526992 #train# step  987, loss = 1.0764, cross_entropy loss = 1.0764, 1.3 sec/batch
2019-05-04 13:10:17.863749 #train# step  988, loss = 1.0801, cross_entropy loss = 1.0801, 1.3 sec/batch
2019-05-04 13:10:19.175865 #train# step  989, loss = 1.0217, cross_entropy loss = 1.0217, 1.3 sec/batch
2019-05-04 13:10:20.465900 #train# step  990, loss = 1.0756, cross_entropy loss = 1.0756, 1.3 sec/batch
2019-05-04 13:10:21.788150 #train# step  991, loss = 1.0707, cross_entropy loss = 1.0707, 1.3 sec/batch
2019-05-04 13:10:23.094506 #train# step  992, loss = 1.0535, cross_entropy loss = 1.0535, 1.3 sec/batch
2019-05-04 13:10:24.382929 #train# step  993, loss = 1.0842, cross_entropy loss = 1.0842, 1.3 sec/batch
2019-05-04 13:10:25.701005 #train# step  994, loss = 1.1266, cross_entropy loss = 1.1266, 1.3 sec/batch
2019-05-04 13:10:27.012236 #train# step  995, loss = 1.0307, cross_entropy loss = 1.0307, 1.3 sec/batch
2019-05-04 13:10:28.286673 #train# step  996, loss = 1.0780, cross_entropy loss = 1.0780, 1.2 sec/batch
2019-05-04 13:10:29.607932 #train# step  997, loss = 1.1102, cross_entropy loss = 1.1102, 1.3 sec/batch
2019-05-04 13:10:30.916409 #train# step  998, loss = 1.0776, cross_entropy loss = 1.0776, 1.3 sec/batch
2019-05-04 13:10:32.227016 #train# step  999, loss = 1.0792, cross_entropy loss = 1.0792, 1.3 sec/batch
2019-05-04 13:10:33.542498 #train# step 1000, loss = 1.1555, cross_entropy loss = 1.1555, 1.3 sec/batch
2019-05-04 13:10:34.830644 #train# step 1001, loss = 1.1279, cross_entropy loss = 1.1279, 1.3 sec/batch
2019-05-04 13:10:36.115036 #train# step 1002, loss = 1.0918, cross_entropy loss = 1.0918, 1.3 sec/batch
2019-05-04 13:10:37.420474 #train# step 1003, loss = 1.0395, cross_entropy loss = 1.0395, 1.3 sec/batch
2019-05-04 13:10:38.723905 #train# step 1004, loss = 1.0436, cross_entropy loss = 1.0436, 1.3 sec/batch
2019-05-04 13:10:40.036250 #train# step 1005, loss = 1.0620, cross_entropy loss = 1.0620, 1.3 sec/batch
2019-05-04 13:10:41.357177 #train# step 1006, loss = 1.1072, cross_entropy loss = 1.1072, 1.3 sec/batch
2019-05-04 13:10:42.669339 #train# step 1007, loss = 1.1041, cross_entropy loss = 1.1041, 1.3 sec/batch
2019-05-04 13:10:43.957241 #train# step 1008, loss = 1.0683, cross_entropy loss = 1.0683, 1.3 sec/batch
2019-05-04 13:10:45.272635 #train# step 1009, loss = 1.1052, cross_entropy loss = 1.1052, 1.3 sec/batch
2019-05-04 13:10:46.586707 #train# step 1010, loss = 1.0234, cross_entropy loss = 1.0234, 1.3 sec/batch
2019-05-04 13:10:47.898548 #train# step 1011, loss = 1.0415, cross_entropy loss = 1.0415, 1.3 sec/batch
2019-05-04 13:10:49.173963 #train# step 1012, loss = 1.0326, cross_entropy loss = 1.0326, 1.2 sec/batch
2019-05-04 13:10:50.491114 #train# step 1013, loss = 1.0992, cross_entropy loss = 1.0992, 1.3 sec/batch
2019-05-04 13:10:51.797125 #train# step 1014, loss = 1.0383, cross_entropy loss = 1.0383, 1.3 sec/batch
2019-05-04 13:10:53.100130 #train# step 1015, loss = 1.0431, cross_entropy loss = 1.0431, 1.3 sec/batch
2019-05-04 13:10:54.382941 #train# step 1016, loss = 1.1511, cross_entropy loss = 1.1511, 1.2 sec/batch
2019-05-04 13:10:55.699414 #train# step 1017, loss = 1.0484, cross_entropy loss = 1.0484, 1.3 sec/batch
2019-05-04 13:10:57.019066 #train# step 1018, loss = 1.0847, cross_entropy loss = 1.0847, 1.3 sec/batch
2019-05-04 13:10:58.336641 #train# step 1019, loss = 1.0877, cross_entropy loss = 1.0877, 1.3 sec/batch
2019-05-04 13:10:59.656721 #train# step 1020, loss = 1.1529, cross_entropy loss = 1.1529, 1.3 sec/batch
2019-05-04 13:11:00.950124 #train# step 1021, loss = 1.0614, cross_entropy loss = 1.0614, 1.3 sec/batch
2019-05-04 13:11:02.270653 #train# step 1022, loss = 1.1235, cross_entropy loss = 1.1235, 1.3 sec/batch
2019-05-04 13:11:03.565320 #train# step 1023, loss = 1.0195, cross_entropy loss = 1.0195, 1.3 sec/batch
2019-05-04 13:11:04.886813 #train# step 1024, loss = 0.9838, cross_entropy loss = 0.9838, 1.3 sec/batch
2019-05-04 13:11:06.179014 #train# step 1025, loss = 1.0674, cross_entropy loss = 1.0674, 1.3 sec/batch
2019-05-04 13:11:07.476217 #train# step 1026, loss = 1.1429, cross_entropy loss = 1.1429, 1.3 sec/batch
2019-05-04 13:11:08.790612 #train# step 1027, loss = 1.0183, cross_entropy loss = 1.0183, 1.3 sec/batch
2019-05-04 13:11:10.071124 #train# step 1028, loss = 1.1099, cross_entropy loss = 1.1099, 1.2 sec/batch
2019-05-04 13:11:11.372465 #train# step 1029, loss = 1.0880, cross_entropy loss = 1.0880, 1.3 sec/batch
2019-05-04 13:11:12.680143 #train# step 1030, loss = 1.0650, cross_entropy loss = 1.0650, 1.3 sec/batch
2019-05-04 13:11:13.989346 #train# step 1031, loss = 1.0751, cross_entropy loss = 1.0751, 1.3 sec/batch
2019-05-04 13:11:15.305696 #train# step 1032, loss = 1.0301, cross_entropy loss = 1.0301, 1.3 sec/batch
2019-05-04 13:11:16.627898 #train# step 1033, loss = 1.0986, cross_entropy loss = 1.0986, 1.3 sec/batch
2019-05-04 13:11:17.946530 #train# step 1034, loss = 1.0739, cross_entropy loss = 1.0739, 1.3 sec/batch
2019-05-04 13:11:19.248286 #train# step 1035, loss = 1.0927, cross_entropy loss = 1.0927, 1.3 sec/batch
2019-05-04 13:11:20.562788 #train# step 1036, loss = 1.0770, cross_entropy loss = 1.0770, 1.3 sec/batch
2019-05-04 13:11:21.888337 #train# step 1037, loss = 1.0382, cross_entropy loss = 1.0382, 1.3 sec/batch
2019-05-04 13:11:23.202375 #train# step 1038, loss = 1.0514, cross_entropy loss = 1.0514, 1.3 sec/batch
2019-05-04 13:11:24.532973 #train# step 1039, loss = 1.0137, cross_entropy loss = 1.0137, 1.3 sec/batch
2019-05-04 13:11:25.821512 #train# step 1040, loss = 1.0570, cross_entropy loss = 1.0570, 1.3 sec/batch
2019-05-04 13:11:27.142947 #train# step 1041, loss = 1.0987, cross_entropy loss = 1.0987, 1.3 sec/batch
2019-05-04 13:11:28.458841 #train# step 1042, loss = 1.1109, cross_entropy loss = 1.1109, 1.3 sec/batch
2019-05-04 13:11:29.762873 #train# step 1043, loss = 1.0511, cross_entropy loss = 1.0511, 1.3 sec/batch
2019-05-04 13:11:31.093753 #train# step 1044, loss = 1.1212, cross_entropy loss = 1.1212, 1.3 sec/batch
2019-05-04 13:11:32.417661 #train# step 1045, loss = 0.9705, cross_entropy loss = 0.9705, 1.3 sec/batch
2019-05-04 13:11:33.729238 #train# step 1046, loss = 1.0567, cross_entropy loss = 1.0567, 1.3 sec/batch
2019-05-04 13:11:35.052005 #train# step 1047, loss = 1.0553, cross_entropy loss = 1.0553, 1.3 sec/batch
2019-05-04 13:11:36.368739 #train# step 1048, loss = 1.0603, cross_entropy loss = 1.0603, 1.3 sec/batch
2019-05-04 13:11:37.691073 #train# step 1049, loss = 1.0741, cross_entropy loss = 1.0741, 1.3 sec/batch
2019-05-04 13:11:39.013903 #train# step 1050, loss = 1.0387, cross_entropy loss = 1.0387, 1.3 sec/batch
2019-05-04 13:11:40.321194 #train# step 1051, loss = 1.0611, cross_entropy loss = 1.0611, 1.3 sec/batch
2019-05-04 13:11:41.653141 #train# step 1052, loss = 1.0639, cross_entropy loss = 1.0639, 1.3 sec/batch
2019-05-04 13:11:42.984338 #train# step 1053, loss = 1.0813, cross_entropy loss = 1.0813, 1.3 sec/batch
2019-05-04 13:11:44.311439 #train# step 1054, loss = 1.0629, cross_entropy loss = 1.0629, 1.3 sec/batch
2019-05-04 13:11:45.643129 #train# step 1055, loss = 1.0461, cross_entropy loss = 1.0461, 1.3 sec/batch
2019-05-04 13:11:47.031382 #train# step 1056, loss = 1.0702, cross_entropy loss = 1.0702, 1.3 sec/batch
2019-05-04 13:11:48.368369 #train# step 1057, loss = 1.0241, cross_entropy loss = 1.0241, 1.3 sec/batch
2019-05-04 13:11:49.705118 #train# step 1058, loss = 1.0863, cross_entropy loss = 1.0863, 1.3 sec/batch
2019-05-04 13:11:51.027262 #train# step 1059, loss = 1.0349, cross_entropy loss = 1.0349, 1.3 sec/batch
2019-05-04 13:11:52.339277 #train# step 1060, loss = 1.0480, cross_entropy loss = 1.0480, 1.3 sec/batch
2019-05-04 13:11:53.708497 #train# step 1061, loss = 1.0573, cross_entropy loss = 1.0573, 1.3 sec/batch
2019-05-04 13:11:55.039148 #train# step 1062, loss = 1.0677, cross_entropy loss = 1.0677, 1.3 sec/batch
2019-05-04 13:11:56.368486 #train# step 1063, loss = 1.0423, cross_entropy loss = 1.0423, 1.3 sec/batch
2019-05-04 13:11:57.702873 #train# step 1064, loss = 1.0973, cross_entropy loss = 1.0973, 1.3 sec/batch
2019-05-04 13:11:59.039118 #train# step 1065, loss = 1.0645, cross_entropy loss = 1.0645, 1.3 sec/batch
2019-05-04 13:12:00.364208 #train# step 1066, loss = 1.0675, cross_entropy loss = 1.0675, 1.3 sec/batch
2019-05-04 13:12:01.691710 #train# step 1067, loss = 1.0818, cross_entropy loss = 1.0818, 1.3 sec/batch
2019-05-04 13:12:03.014336 #train# step 1068, loss = 1.1093, cross_entropy loss = 1.1093, 1.3 sec/batch
2019-05-04 13:12:04.341090 #train# step 1069, loss = 1.0287, cross_entropy loss = 1.0287, 1.3 sec/batch
2019-05-04 13:12:05.672267 #train# step 1070, loss = 1.0384, cross_entropy loss = 1.0384, 1.3 sec/batch
2019-05-04 13:12:06.997452 #train# step 1071, loss = 1.0451, cross_entropy loss = 1.0451, 1.3 sec/batch
2019-05-04 13:12:08.315206 #train# step 1072, loss = 1.0805, cross_entropy loss = 1.0805, 1.3 sec/batch
2019-05-04 13:12:09.645592 #train# step 1073, loss = 1.0095, cross_entropy loss = 1.0095, 1.3 sec/batch
2019-05-04 13:12:10.967487 #train# step 1074, loss = 1.0725, cross_entropy loss = 1.0725, 1.3 sec/batch
2019-05-04 13:12:12.288739 #train# step 1075, loss = 0.9940, cross_entropy loss = 0.9940, 1.3 sec/batch
2019-05-04 13:12:13.617141 #train# step 1076, loss = 1.0736, cross_entropy loss = 1.0736, 1.3 sec/batch
2019-05-04 13:12:14.931827 #train# step 1077, loss = 1.0604, cross_entropy loss = 1.0604, 1.3 sec/batch
2019-05-04 13:12:16.234941 #train# step 1078, loss = 1.0305, cross_entropy loss = 1.0305, 1.3 sec/batch
2019-05-04 13:12:17.556723 #train# step 1079, loss = 1.0700, cross_entropy loss = 1.0700, 1.3 sec/batch
2019-05-04 13:12:18.875712 #train# step 1080, loss = 1.0632, cross_entropy loss = 1.0632, 1.3 sec/batch
2019-05-04 13:12:20.207115 #train# step 1081, loss = 1.1208, cross_entropy loss = 1.1208, 1.3 sec/batch
2019-05-04 13:12:21.528268 #train# step 1082, loss = 1.0674, cross_entropy loss = 1.0674, 1.3 sec/batch
2019-05-04 13:12:22.847068 #train# step 1083, loss = 1.1391, cross_entropy loss = 1.1391, 1.3 sec/batch
2019-05-04 13:12:24.173463 #train# step 1084, loss = 1.0830, cross_entropy loss = 1.0830, 1.3 sec/batch
2019-05-04 13:12:25.461156 #train# step 1085, loss = 1.0776, cross_entropy loss = 1.0776, 1.3 sec/batch
2019-05-04 13:12:26.773933 #train# step 1086, loss = 1.0056, cross_entropy loss = 1.0056, 1.3 sec/batch
2019-05-04 13:12:28.095149 #train# step 1087, loss = 1.0229, cross_entropy loss = 1.0229, 1.3 sec/batch
2019-05-04 13:12:29.411162 #train# step 1088, loss = 1.0674, cross_entropy loss = 1.0674, 1.3 sec/batch
2019-05-04 13:12:30.737780 #train# step 1089, loss = 1.0733, cross_entropy loss = 1.0733, 1.3 sec/batch
2019-05-04 13:12:32.050007 #train# step 1090, loss = 1.0527, cross_entropy loss = 1.0527, 1.3 sec/batch
2019-05-04 13:12:33.360178 #train# step 1091, loss = 1.0896, cross_entropy loss = 1.0896, 1.3 sec/batch
2019-05-04 13:12:34.679096 #train# step 1092, loss = 1.1308, cross_entropy loss = 1.1308, 1.3 sec/batch
2019-05-04 13:12:35.995816 #train# step 1093, loss = 0.9955, cross_entropy loss = 0.9955, 1.3 sec/batch
2019-05-04 13:12:37.317246 #train# step 1094, loss = 1.0335, cross_entropy loss = 1.0335, 1.3 sec/batch
2019-05-04 13:12:38.639383 #train# step 1095, loss = 1.0744, cross_entropy loss = 1.0744, 1.3 sec/batch
2019-05-04 13:12:39.964263 #train# step 1096, loss = 1.0318, cross_entropy loss = 1.0318, 1.3 sec/batch
2019-05-04 13:12:41.287755 #train# step 1097, loss = 1.0336, cross_entropy loss = 1.0336, 1.3 sec/batch
2019-05-04 13:12:42.601540 #train# step 1098, loss = 1.0537, cross_entropy loss = 1.0537, 1.3 sec/batch
2019-05-04 13:12:43.919302 #train# step 1099, loss = 1.0137, cross_entropy loss = 1.0137, 1.3 sec/batch
2019-05-04 13:12:45.241435 #train# step 1100, loss = 1.0184, cross_entropy loss = 1.0184, 1.3 sec/batch
2019-05-04 13:12:46.565820 #train# step 1101, loss = 1.1459, cross_entropy loss = 1.1459, 1.3 sec/batch
2019-05-04 13:12:47.874077 #train# step 1102, loss = 1.0540, cross_entropy loss = 1.0540, 1.3 sec/batch
2019-05-04 13:12:49.194587 #train# step 1103, loss = 1.0107, cross_entropy loss = 1.0107, 1.3 sec/batch
2019-05-04 13:12:50.512879 #train# step 1104, loss = 1.0823, cross_entropy loss = 1.0823, 1.3 sec/batch
2019-05-04 13:12:51.836635 #train# step 1105, loss = 1.0428, cross_entropy loss = 1.0428, 1.3 sec/batch
2019-05-04 13:12:53.159131 #train# step 1106, loss = 1.0690, cross_entropy loss = 1.0690, 1.3 sec/batch
2019-05-04 13:12:54.483643 #train# step 1107, loss = 1.1245, cross_entropy loss = 1.1245, 1.3 sec/batch
2019-05-04 13:12:55.812983 #train# step 1108, loss = 1.0800, cross_entropy loss = 1.0800, 1.3 sec/batch
2019-05-04 13:12:57.133172 #train# step 1109, loss = 1.0769, cross_entropy loss = 1.0769, 1.3 sec/batch
2019-05-04 13:12:58.450734 #train# step 1110, loss = 1.0558, cross_entropy loss = 1.0558, 1.3 sec/batch
2019-05-04 13:12:59.772381 #train# step 1111, loss = 1.0319, cross_entropy loss = 1.0319, 1.3 sec/batch
2019-05-04 13:13:01.088874 #train# step 1112, loss = 0.9960, cross_entropy loss = 0.9960, 1.3 sec/batch
2019-05-04 13:13:02.417021 #train# step 1113, loss = 1.0478, cross_entropy loss = 1.0478, 1.3 sec/batch
2019-05-04 13:13:03.747346 #train# step 1114, loss = 1.0797, cross_entropy loss = 1.0797, 1.3 sec/batch
2019-05-04 13:13:05.073112 #train# step 1115, loss = 1.0044, cross_entropy loss = 1.0044, 1.3 sec/batch
2019-05-04 13:13:06.411032 #train# step 1116, loss = 1.0396, cross_entropy loss = 1.0396, 1.3 sec/batch
2019-05-04 13:13:07.729429 #train# step 1117, loss = 1.0504, cross_entropy loss = 1.0504, 1.3 sec/batch
2019-05-04 13:13:09.025617 #train# step 1118, loss = 1.0551, cross_entropy loss = 1.0551, 1.3 sec/batch
2019-05-04 13:13:10.347563 #train# step 1119, loss = 1.0843, cross_entropy loss = 1.0843, 1.3 sec/batch
2019-05-04 13:13:11.636172 #train# step 1120, loss = 1.1273, cross_entropy loss = 1.1273, 1.3 sec/batch
2019-05-04 13:13:12.970552 #train# step 1121, loss = 1.0670, cross_entropy loss = 1.0670, 1.3 sec/batch
2019-05-04 13:13:14.295986 #train# step 1122, loss = 1.0356, cross_entropy loss = 1.0356, 1.3 sec/batch
2019-05-04 13:13:15.619060 #train# step 1123, loss = 1.0593, cross_entropy loss = 1.0593, 1.3 sec/batch
2019-05-04 13:13:16.947868 #train# step 1124, loss = 1.0854, cross_entropy loss = 1.0854, 1.3 sec/batch
2019-05-04 13:13:18.271747 #train# step 1125, loss = 1.0228, cross_entropy loss = 1.0228, 1.3 sec/batch
2019-05-04 13:13:19.608639 #train# step 1126, loss = 1.0746, cross_entropy loss = 1.0746, 1.3 sec/batch
2019-05-04 13:13:20.946849 #train# step 1127, loss = 1.0350, cross_entropy loss = 1.0350, 1.3 sec/batch
2019-05-04 13:13:22.269868 #train# step 1128, loss = 1.0436, cross_entropy loss = 1.0436, 1.3 sec/batch
2019-05-04 13:13:23.594101 #train# step 1129, loss = 1.0765, cross_entropy loss = 1.0765, 1.3 sec/batch
2019-05-04 13:13:24.929777 #train# step 1130, loss = 1.0752, cross_entropy loss = 1.0752, 1.3 sec/batch
2019-05-04 13:13:26.246928 #train# step 1131, loss = 1.0809, cross_entropy loss = 1.0809, 1.3 sec/batch
2019-05-04 13:13:27.585817 #train# step 1132, loss = 1.0653, cross_entropy loss = 1.0653, 1.3 sec/batch
2019-05-04 13:13:28.919312 #train# step 1133, loss = 1.0437, cross_entropy loss = 1.0437, 1.3 sec/batch
2019-05-04 13:13:30.247506 #train# step 1134, loss = 1.0712, cross_entropy loss = 1.0712, 1.3 sec/batch
2019-05-04 13:13:31.543685 #train# step 1135, loss = 0.9880, cross_entropy loss = 0.9880, 1.2 sec/batch
2019-05-04 13:13:32.932400 #train# step 1136, loss = 1.0638, cross_entropy loss = 1.0638, 1.3 sec/batch
2019-05-04 13:13:34.270865 #train# step 1137, loss = 1.1221, cross_entropy loss = 1.1221, 1.3 sec/batch
2019-05-04 13:13:35.610506 #train# step 1138, loss = 1.0248, cross_entropy loss = 1.0248, 1.3 sec/batch
2019-05-04 13:13:36.950147 #train# step 1139, loss = 1.0860, cross_entropy loss = 1.0860, 1.3 sec/batch
2019-05-04 13:13:38.258167 #train# step 1140, loss = 1.0429, cross_entropy loss = 1.0429, 1.3 sec/batch
2019-05-04 13:13:39.594735 #train# step 1141, loss = 1.1495, cross_entropy loss = 1.1495, 1.3 sec/batch
2019-05-04 13:13:40.927365 #train# step 1142, loss = 0.9990, cross_entropy loss = 0.9990, 1.3 sec/batch
2019-05-04 13:13:42.250753 #train# step 1143, loss = 1.0681, cross_entropy loss = 1.0681, 1.3 sec/batch
2019-05-04 13:13:43.638626 #train# step 1144, loss = 1.0818, cross_entropy loss = 1.0818, 1.3 sec/batch
2019-05-04 13:13:44.968497 #train# step 1145, loss = 1.1273, cross_entropy loss = 1.1273, 1.3 sec/batch
2019-05-04 13:13:46.320031 #train# step 1146, loss = 1.0041, cross_entropy loss = 1.0041, 1.3 sec/batch
2019-05-04 13:13:47.669304 #train# step 1147, loss = 1.0727, cross_entropy loss = 1.0727, 1.3 sec/batch
2019-05-04 13:13:48.997465 #train# step 1148, loss = 1.0138, cross_entropy loss = 1.0138, 1.3 sec/batch
2019-05-04 13:13:50.352194 #train# step 1149, loss = 1.0136, cross_entropy loss = 1.0136, 1.3 sec/batch
2019-05-04 13:13:51.678789 #train# step 1150, loss = 1.0150, cross_entropy loss = 1.0150, 1.3 sec/batch
2019-05-04 13:13:53.020617 #train# step 1151, loss = 1.0568, cross_entropy loss = 1.0568, 1.3 sec/batch
2019-05-04 13:13:54.320500 #train# step 1152, loss = 1.0562, cross_entropy loss = 1.0562, 1.3 sec/batch
2019-05-04 13:13:55.653210 #train# step 1153, loss = 1.0627, cross_entropy loss = 1.0627, 1.3 sec/batch
2019-05-04 13:13:56.985941 #train# step 1154, loss = 1.0774, cross_entropy loss = 1.0774, 1.3 sec/batch
2019-05-04 13:13:58.322243 #train# step 1155, loss = 1.0515, cross_entropy loss = 1.0515, 1.3 sec/batch
2019-05-04 13:13:59.639906 #train# step 1156, loss = 1.0098, cross_entropy loss = 1.0098, 1.3 sec/batch
2019-05-04 13:14:00.958433 #train# step 1157, loss = 1.0598, cross_entropy loss = 1.0598, 1.3 sec/batch
2019-05-04 13:14:02.295118 #train# step 1158, loss = 1.0592, cross_entropy loss = 1.0592, 1.3 sec/batch
2019-05-04 13:14:03.623895 #train# step 1159, loss = 1.0690, cross_entropy loss = 1.0690, 1.3 sec/batch
2019-05-04 13:14:04.953250 #train# step 1160, loss = 1.0192, cross_entropy loss = 1.0192, 1.3 sec/batch
2019-05-04 13:14:06.283767 #train# step 1161, loss = 1.0996, cross_entropy loss = 1.0996, 1.3 sec/batch
2019-05-04 13:14:07.608658 #train# step 1162, loss = 1.0199, cross_entropy loss = 1.0199, 1.3 sec/batch
2019-05-04 13:14:08.944692 #train# step 1163, loss = 1.1043, cross_entropy loss = 1.1043, 1.3 sec/batch
2019-05-04 13:14:10.278783 #train# step 1164, loss = 1.0573, cross_entropy loss = 1.0573, 1.3 sec/batch
2019-05-04 13:14:11.596344 #train# step 1165, loss = 1.0393, cross_entropy loss = 1.0393, 1.3 sec/batch
2019-05-04 13:14:12.923125 #train# step 1166, loss = 1.1024, cross_entropy loss = 1.1024, 1.3 sec/batch
2019-05-04 13:14:14.249988 #train# step 1167, loss = 1.0190, cross_entropy loss = 1.0190, 1.3 sec/batch
2019-05-04 13:14:15.561243 #train# step 1168, loss = 1.0228, cross_entropy loss = 1.0228, 1.3 sec/batch
2019-05-04 13:14:16.880139 #train# step 1169, loss = 1.0652, cross_entropy loss = 1.0652, 1.3 sec/batch
2019-05-04 13:14:18.204877 #train# step 1170, loss = 1.0646, cross_entropy loss = 1.0646, 1.3 sec/batch
2019-05-04 13:14:19.531579 #train# step 1171, loss = 1.0640, cross_entropy loss = 1.0640, 1.3 sec/batch
2019-05-04 13:14:20.849377 #train# step 1172, loss = 0.9884, cross_entropy loss = 0.9884, 1.3 sec/batch
2019-05-04 13:14:22.188665 #train# step 1173, loss = 1.0666, cross_entropy loss = 1.0666, 1.3 sec/batch
2019-05-04 13:14:23.502680 #train# step 1174, loss = 0.9696, cross_entropy loss = 0.9696, 1.3 sec/batch
2019-05-04 13:14:24.824635 #train# step 1175, loss = 1.0743, cross_entropy loss = 1.0743, 1.3 sec/batch
2019-05-04 13:14:26.160590 #train# step 1176, loss = 1.0190, cross_entropy loss = 1.0190, 1.3 sec/batch
2019-05-04 13:14:27.487894 #train# step 1177, loss = 1.0181, cross_entropy loss = 1.0181, 1.3 sec/batch
2019-05-04 13:14:28.824814 #train# step 1178, loss = 1.0285, cross_entropy loss = 1.0285, 1.3 sec/batch
2019-05-04 13:14:30.119521 #train# step 1179, loss = 1.0761, cross_entropy loss = 1.0761, 1.3 sec/batch
2019-05-04 13:14:31.451547 #train# step 1180, loss = 0.9940, cross_entropy loss = 0.9940, 1.3 sec/batch
2019-05-04 13:14:32.794083 #train# step 1181, loss = 1.0608, cross_entropy loss = 1.0608, 1.3 sec/batch
2019-05-04 13:14:34.126131 #train# step 1182, loss = 1.0675, cross_entropy loss = 1.0675, 1.3 sec/batch
2019-05-04 13:14:35.466148 #train# step 1183, loss = 1.0641, cross_entropy loss = 1.0641, 1.3 sec/batch
2019-05-04 13:14:36.802668 #train# step 1184, loss = 1.0053, cross_entropy loss = 1.0053, 1.3 sec/batch
2019-05-04 13:14:38.149817 #train# step 1185, loss = 1.0159, cross_entropy loss = 1.0159, 1.3 sec/batch
2019-05-04 13:14:39.486225 #train# step 1186, loss = 1.0137, cross_entropy loss = 1.0137, 1.3 sec/batch
2019-05-04 13:14:40.824685 #train# step 1187, loss = 1.0637, cross_entropy loss = 1.0637, 1.3 sec/batch
2019-05-04 13:14:42.173068 #train# step 1188, loss = 1.0908, cross_entropy loss = 1.0908, 1.3 sec/batch
2019-05-04 13:14:43.519474 #train# step 1189, loss = 0.9789, cross_entropy loss = 0.9789, 1.3 sec/batch
2019-05-04 13:14:44.861961 #train# step 1190, loss = 1.0092, cross_entropy loss = 1.0092, 1.3 sec/batch
2019-05-04 13:14:46.200640 #train# step 1191, loss = 1.0388, cross_entropy loss = 1.0388, 1.3 sec/batch
2019-05-04 13:14:47.543240 #train# step 1192, loss = 1.0173, cross_entropy loss = 1.0173, 1.3 sec/batch
2019-05-04 13:14:48.879158 #train# step 1193, loss = 1.0611, cross_entropy loss = 1.0611, 1.3 sec/batch
2019-05-04 13:14:50.199246 #train# step 1194, loss = 1.0020, cross_entropy loss = 1.0020, 1.3 sec/batch
2019-05-04 13:14:51.567559 #train# step 1195, loss = 1.0623, cross_entropy loss = 1.0623, 1.3 sec/batch
2019-05-04 13:14:52.906617 #train# step 1196, loss = 1.0996, cross_entropy loss = 1.0996, 1.3 sec/batch
2019-05-04 13:14:54.247008 #train# step 1197, loss = 0.9597, cross_entropy loss = 0.9597, 1.3 sec/batch
2019-05-04 13:14:55.580084 #train# step 1198, loss = 1.0643, cross_entropy loss = 1.0643, 1.3 sec/batch
2019-05-04 13:14:56.924341 #train# step 1199, loss = 1.0116, cross_entropy loss = 1.0116, 1.3 sec/batch
2019-05-04 13:14:58.223829 #train# step 1200, loss = 0.9772, cross_entropy loss = 0.9772, 1.3 sec/batch
2019-05-04 13:14:59.526385 #train# step 1201, loss = 1.0285, cross_entropy loss = 1.0285, 1.3 sec/batch
2019-05-04 13:15:00.824262 #train# step 1202, loss = 1.0876, cross_entropy loss = 1.0876, 1.3 sec/batch
2019-05-04 13:15:02.144908 #train# step 1203, loss = 0.9996, cross_entropy loss = 0.9996, 1.3 sec/batch
2019-05-04 13:15:03.468929 #train# step 1204, loss = 1.0469, cross_entropy loss = 1.0469, 1.3 sec/batch
2019-05-04 13:15:04.807907 #train# step 1205, loss = 1.1538, cross_entropy loss = 1.1538, 1.3 sec/batch
2019-05-04 13:15:06.130235 #train# step 1206, loss = 1.0490, cross_entropy loss = 1.0490, 1.3 sec/batch
2019-05-04 13:15:07.423011 #train# step 1207, loss = 1.0359, cross_entropy loss = 1.0359, 1.3 sec/batch
2019-05-04 13:15:08.753567 #train# step 1208, loss = 1.0500, cross_entropy loss = 1.0500, 1.3 sec/batch
2019-05-04 13:15:10.077511 #train# step 1209, loss = 1.0539, cross_entropy loss = 1.0539, 1.3 sec/batch
2019-05-04 13:15:11.394583 #train# step 1210, loss = 1.0893, cross_entropy loss = 1.0893, 1.3 sec/batch
2019-05-04 13:15:12.733501 #train# step 1211, loss = 0.9791, cross_entropy loss = 0.9791, 1.3 sec/batch
2019-05-04 13:15:14.071479 #train# step 1212, loss = 0.9926, cross_entropy loss = 0.9926, 1.3 sec/batch
2019-05-04 13:15:15.408892 #train# step 1213, loss = 1.0633, cross_entropy loss = 1.0633, 1.3 sec/batch
2019-05-04 13:15:16.751191 #train# step 1214, loss = 1.1057, cross_entropy loss = 1.1057, 1.3 sec/batch
2019-05-04 13:15:18.089518 #train# step 1215, loss = 1.0721, cross_entropy loss = 1.0721, 1.3 sec/batch
2019-05-04 13:15:19.423536 #train# step 1216, loss = 1.0350, cross_entropy loss = 1.0350, 1.3 sec/batch
2019-05-04 13:15:20.744432 #train# step 1217, loss = 0.9647, cross_entropy loss = 0.9647, 1.3 sec/batch
2019-05-04 13:15:22.036076 #train# step 1218, loss = 1.0008, cross_entropy loss = 1.0008, 1.3 sec/batch
2019-05-04 13:15:23.369717 #train# step 1219, loss = 1.0336, cross_entropy loss = 1.0336, 1.3 sec/batch
2019-05-04 13:15:24.691931 #train# step 1220, loss = 0.9941, cross_entropy loss = 0.9941, 1.3 sec/batch
2019-05-04 13:15:26.003055 #train# step 1221, loss = 1.0895, cross_entropy loss = 1.0895, 1.3 sec/batch
2019-05-04 13:15:27.320004 #train# step 1222, loss = 1.0394, cross_entropy loss = 1.0394, 1.3 sec/batch
2019-05-04 13:15:28.646089 #train# step 1223, loss = 1.0950, cross_entropy loss = 1.0950, 1.3 sec/batch
2019-05-04 13:15:29.975394 #train# step 1224, loss = 1.0203, cross_entropy loss = 1.0203, 1.3 sec/batch
2019-05-04 13:15:31.307463 #train# step 1225, loss = 1.1124, cross_entropy loss = 1.1124, 1.3 sec/batch
2019-05-04 13:15:32.634891 #train# step 1226, loss = 1.0146, cross_entropy loss = 1.0146, 1.3 sec/batch
2019-05-04 13:15:33.968119 #train# step 1227, loss = 1.1147, cross_entropy loss = 1.1147, 1.3 sec/batch
2019-05-04 13:15:35.295277 #train# step 1228, loss = 1.0015, cross_entropy loss = 1.0015, 1.3 sec/batch
2019-05-04 13:15:36.612498 #train# step 1229, loss = 0.9930, cross_entropy loss = 0.9930, 1.3 sec/batch
2019-05-04 13:15:37.947391 #train# step 1230, loss = 1.0820, cross_entropy loss = 1.0820, 1.3 sec/batch
2019-05-04 13:15:39.286679 #train# step 1231, loss = 1.0454, cross_entropy loss = 1.0454, 1.3 sec/batch
2019-05-04 13:15:40.622762 #train# step 1232, loss = 1.0220, cross_entropy loss = 1.0220, 1.3 sec/batch
2019-05-04 13:15:41.947463 #train# step 1233, loss = 1.0392, cross_entropy loss = 1.0392, 1.3 sec/batch
2019-05-04 13:15:43.347764 #train# step 1234, loss = 1.0010, cross_entropy loss = 1.0010, 1.4 sec/batch
2019-05-04 13:15:44.705767 #train# step 1235, loss = 1.0992, cross_entropy loss = 1.0992, 1.3 sec/batch
2019-05-04 13:15:46.027886 #train# step 1236, loss = 1.0385, cross_entropy loss = 1.0385, 1.3 sec/batch
2019-05-04 13:15:47.364172 #train# step 1237, loss = 1.0782, cross_entropy loss = 1.0782, 1.3 sec/batch
2019-05-04 13:15:48.705315 #train# step 1238, loss = 0.9913, cross_entropy loss = 0.9913, 1.3 sec/batch
2019-05-04 13:15:50.080432 #train# step 1239, loss = 1.1050, cross_entropy loss = 1.1050, 1.3 sec/batch
2019-05-04 13:15:51.412953 #train# step 1240, loss = 1.0087, cross_entropy loss = 1.0087, 1.3 sec/batch
2019-05-04 13:15:52.751927 #train# step 1241, loss = 1.0247, cross_entropy loss = 1.0247, 1.3 sec/batch
2019-05-04 13:15:54.159373 #train# step 1242, loss = 0.9969, cross_entropy loss = 0.9969, 1.4 sec/batch
2019-05-04 13:15:55.485957 #train# step 1243, loss = 1.0139, cross_entropy loss = 1.0139, 1.3 sec/batch
2019-05-04 13:15:56.817246 #train# step 1244, loss = 1.0391, cross_entropy loss = 1.0391, 1.3 sec/batch
2019-05-04 13:15:58.154691 #train# step 1245, loss = 1.0747, cross_entropy loss = 1.0747, 1.3 sec/batch
2019-05-04 13:15:59.495266 #train# step 1246, loss = 1.0136, cross_entropy loss = 1.0136, 1.3 sec/batch
2019-05-04 13:16:00.829323 #train# step 1247, loss = 1.0060, cross_entropy loss = 1.0060, 1.3 sec/batch
2019-05-04 13:16:02.146867 #train# step 1248, loss = 1.0074, cross_entropy loss = 1.0074, 1.3 sec/batch
2019-05-04 13:16:03.465183 #train# step 1249, loss = 1.0651, cross_entropy loss = 1.0651, 1.3 sec/batch
2019-05-04 13:16:04.781422 #train# step 1250, loss = 1.0229, cross_entropy loss = 1.0229, 1.3 sec/batch
2019-05-04 13:16:06.092617 #train# step 1251, loss = 1.0831, cross_entropy loss = 1.0831, 1.3 sec/batch
2019-05-04 13:16:07.416763 #train# step 1252, loss = 0.9691, cross_entropy loss = 0.9691, 1.3 sec/batch
2019-05-04 13:16:08.740780 #train# step 1253, loss = 1.0707, cross_entropy loss = 1.0707, 1.3 sec/batch
2019-05-04 13:16:10.076651 #train# step 1254, loss = 1.1112, cross_entropy loss = 1.1112, 1.3 sec/batch
2019-05-04 13:16:11.407731 #train# step 1255, loss = 1.0538, cross_entropy loss = 1.0538, 1.3 sec/batch
2019-05-04 13:16:12.731777 #train# step 1256, loss = 1.0248, cross_entropy loss = 1.0248, 1.3 sec/batch
2019-05-04 13:16:14.067215 #train# step 1257, loss = 1.0587, cross_entropy loss = 1.0587, 1.3 sec/batch
2019-05-04 13:16:15.396961 #train# step 1258, loss = 1.0252, cross_entropy loss = 1.0252, 1.3 sec/batch
2019-05-04 13:16:16.725043 #train# step 1259, loss = 1.0556, cross_entropy loss = 1.0556, 1.3 sec/batch
2019-05-04 13:16:18.055438 #train# step 1260, loss = 1.0662, cross_entropy loss = 1.0662, 1.3 sec/batch
2019-05-04 13:16:19.389896 #train# step 1261, loss = 0.9307, cross_entropy loss = 0.9307, 1.3 sec/batch
2019-05-04 13:16:20.724202 #train# step 1262, loss = 1.0208, cross_entropy loss = 1.0208, 1.3 sec/batch
2019-05-04 13:16:22.053555 #train# step 1263, loss = 1.0583, cross_entropy loss = 1.0583, 1.3 sec/batch
2019-05-04 13:16:23.384652 #train# step 1264, loss = 0.9611, cross_entropy loss = 0.9611, 1.3 sec/batch
2019-05-04 13:16:24.716752 #train# step 1265, loss = 0.9742, cross_entropy loss = 0.9742, 1.3 sec/batch
2019-05-04 13:16:26.054165 #train# step 1266, loss = 1.1006, cross_entropy loss = 1.1006, 1.3 sec/batch
2019-05-04 13:16:27.388070 #train# step 1267, loss = 0.9613, cross_entropy loss = 0.9613, 1.3 sec/batch
2019-05-04 13:16:28.696597 #train# step 1268, loss = 1.0409, cross_entropy loss = 1.0409, 1.3 sec/batch
2019-05-04 13:16:30.019045 #train# step 1269, loss = 1.0683, cross_entropy loss = 1.0683, 1.3 sec/batch
2019-05-04 13:16:31.346515 #train# step 1270, loss = 1.1071, cross_entropy loss = 1.1071, 1.3 sec/batch
2019-05-04 13:16:32.672700 #train# step 1271, loss = 1.0339, cross_entropy loss = 1.0339, 1.3 sec/batch
2019-05-04 13:16:33.996676 #train# step 1272, loss = 0.9871, cross_entropy loss = 0.9871, 1.3 sec/batch
2019-05-04 13:16:35.332162 #train# step 1273, loss = 1.0504, cross_entropy loss = 1.0504, 1.3 sec/batch
2019-05-04 13:16:36.663157 #train# step 1274, loss = 1.0102, cross_entropy loss = 1.0102, 1.3 sec/batch
2019-05-04 13:16:37.997310 #train# step 1275, loss = 1.0381, cross_entropy loss = 1.0381, 1.3 sec/batch
2019-05-04 13:16:39.291471 #train# step 1276, loss = 1.0343, cross_entropy loss = 1.0343, 1.3 sec/batch
2019-05-04 13:16:40.619770 #train# step 1277, loss = 1.0569, cross_entropy loss = 1.0569, 1.3 sec/batch
2019-05-04 13:16:41.951002 #train# step 1278, loss = 1.0109, cross_entropy loss = 1.0109, 1.3 sec/batch
2019-05-04 13:16:43.265875 #train# step 1279, loss = 1.0382, cross_entropy loss = 1.0382, 1.3 sec/batch
2019-05-04 13:16:44.602066 #train# step 1280, loss = 1.0141, cross_entropy loss = 1.0141, 1.3 sec/batch
2019-05-04 13:16:45.936952 #train# step 1281, loss = 1.0279, cross_entropy loss = 1.0279, 1.3 sec/batch
2019-05-04 13:16:47.269352 #train# step 1282, loss = 1.1175, cross_entropy loss = 1.1175, 1.3 sec/batch
2019-05-04 13:16:48.594810 #train# step 1283, loss = 1.0679, cross_entropy loss = 1.0679, 1.3 sec/batch
2019-05-04 13:16:49.905427 #train# step 1284, loss = 1.0553, cross_entropy loss = 1.0553, 1.3 sec/batch
2019-05-04 13:16:51.212547 #train# step 1285, loss = 1.0320, cross_entropy loss = 1.0320, 1.3 sec/batch
2019-05-04 13:16:52.535639 #train# step 1286, loss = 1.0824, cross_entropy loss = 1.0824, 1.3 sec/batch
2019-05-04 13:16:53.859111 #train# step 1287, loss = 0.9760, cross_entropy loss = 0.9760, 1.3 sec/batch
2019-05-04 13:16:55.145133 #train# step 1288, loss = 1.0186, cross_entropy loss = 1.0186, 1.3 sec/batch
2019-05-04 13:16:56.461357 #train# step 1289, loss = 1.0071, cross_entropy loss = 1.0071, 1.3 sec/batch
2019-05-04 13:16:57.777676 #train# step 1290, loss = 0.9972, cross_entropy loss = 0.9972, 1.3 sec/batch
2019-05-04 13:16:59.088924 #train# step 1291, loss = 0.9892, cross_entropy loss = 0.9892, 1.3 sec/batch
2019-05-04 13:17:00.373480 #train# step 1292, loss = 1.0082, cross_entropy loss = 1.0082, 1.3 sec/batch
2019-05-04 13:17:01.664525 #train# step 1293, loss = 0.9793, cross_entropy loss = 0.9793, 1.3 sec/batch
2019-05-04 13:17:02.995085 #train# step 1294, loss = 1.0674, cross_entropy loss = 1.0674, 1.3 sec/batch
2019-05-04 13:17:04.295610 #train# step 1295, loss = 1.0355, cross_entropy loss = 1.0355, 1.3 sec/batch
2019-05-04 13:17:05.622189 #train# step 1296, loss = 1.0141, cross_entropy loss = 1.0141, 1.3 sec/batch
2019-05-04 13:17:06.921067 #train# step 1297, loss = 1.0439, cross_entropy loss = 1.0439, 1.3 sec/batch
2019-05-04 13:17:08.242611 #train# step 1298, loss = 0.9987, cross_entropy loss = 0.9987, 1.3 sec/batch
2019-05-04 13:17:09.583434 #train# step 1299, loss = 1.0201, cross_entropy loss = 1.0201, 1.3 sec/batch
2019-05-04 13:17:10.919595 #train# step 1300, loss = 0.9752, cross_entropy loss = 0.9752, 1.3 sec/batch
2019-05-04 13:17:12.249675 #train# step 1301, loss = 1.0178, cross_entropy loss = 1.0178, 1.3 sec/batch
2019-05-04 13:17:13.597921 #train# step 1302, loss = 1.0092, cross_entropy loss = 1.0092, 1.3 sec/batch
2019-05-04 13:17:14.933513 #train# step 1303, loss = 1.0043, cross_entropy loss = 1.0043, 1.3 sec/batch
2019-05-04 13:17:16.263167 #train# step 1304, loss = 1.0679, cross_entropy loss = 1.0679, 1.3 sec/batch
2019-05-04 13:17:17.595902 #train# step 1305, loss = 1.0209, cross_entropy loss = 1.0209, 1.3 sec/batch
2019-05-04 13:17:18.923273 #train# step 1306, loss = 1.0650, cross_entropy loss = 1.0650, 1.3 sec/batch
2019-05-04 13:17:20.271424 #train# step 1307, loss = 1.0342, cross_entropy loss = 1.0342, 1.3 sec/batch
2019-05-04 13:17:21.602111 #train# step 1308, loss = 1.0451, cross_entropy loss = 1.0451, 1.3 sec/batch
2019-05-04 13:17:22.939552 #train# step 1309, loss = 1.0981, cross_entropy loss = 1.0981, 1.3 sec/batch
2019-05-04 13:17:24.286250 #train# step 1310, loss = 1.1008, cross_entropy loss = 1.1008, 1.3 sec/batch
2019-05-04 13:17:25.624686 #train# step 1311, loss = 0.9905, cross_entropy loss = 0.9905, 1.3 sec/batch
2019-05-04 13:17:26.973493 #train# step 1312, loss = 1.0107, cross_entropy loss = 1.0107, 1.3 sec/batch
2019-05-04 13:17:28.310038 #train# step 1313, loss = 0.9648, cross_entropy loss = 0.9648, 1.3 sec/batch
2019-05-04 13:17:29.636692 #train# step 1314, loss = 0.9953, cross_entropy loss = 0.9953, 1.3 sec/batch
2019-05-04 13:17:30.976234 #train# step 1315, loss = 1.0561, cross_entropy loss = 1.0561, 1.3 sec/batch
2019-05-04 13:17:32.309617 #train# step 1316, loss = 1.0446, cross_entropy loss = 1.0446, 1.3 sec/batch
2019-05-04 13:17:33.653625 #train# step 1317, loss = 0.9894, cross_entropy loss = 0.9894, 1.3 sec/batch
2019-05-04 13:17:35.007382 #train# step 1318, loss = 1.0376, cross_entropy loss = 1.0376, 1.3 sec/batch
2019-05-04 13:17:36.331002 #train# step 1319, loss = 1.0220, cross_entropy loss = 1.0220, 1.3 sec/batch
2019-05-04 13:17:37.655556 #train# step 1320, loss = 1.0807, cross_entropy loss = 1.0807, 1.3 sec/batch
2019-05-04 13:17:39.001758 #train# step 1321, loss = 1.0539, cross_entropy loss = 1.0539, 1.3 sec/batch
2019-05-04 13:17:40.373656 #train# step 1322, loss = 1.0114, cross_entropy loss = 1.0114, 1.3 sec/batch
2019-05-04 13:17:41.720014 #train# step 1323, loss = 0.9597, cross_entropy loss = 0.9597, 1.3 sec/batch
2019-05-04 13:17:43.054801 #train# step 1324, loss = 1.0307, cross_entropy loss = 1.0307, 1.3 sec/batch
2019-05-04 13:17:44.415550 #train# step 1325, loss = 1.0231, cross_entropy loss = 1.0231, 1.3 sec/batch
2019-05-04 13:17:45.758971 #train# step 1326, loss = 1.0893, cross_entropy loss = 1.0893, 1.3 sec/batch
2019-05-04 13:17:47.153583 #train# step 1327, loss = 1.0193, cross_entropy loss = 1.0193, 1.3 sec/batch
2019-05-04 13:17:48.501477 #train# step 1328, loss = 0.9986, cross_entropy loss = 0.9986, 1.3 sec/batch
2019-05-04 13:17:49.855050 #train# step 1329, loss = 0.9965, cross_entropy loss = 0.9965, 1.3 sec/batch
2019-05-04 13:17:51.228952 #train# step 1330, loss = 1.0069, cross_entropy loss = 1.0069, 1.3 sec/batch
2019-05-04 13:17:52.569286 #train# step 1331, loss = 1.0111, cross_entropy loss = 1.0111, 1.3 sec/batch
2019-05-04 13:17:53.978708 #train# step 1332, loss = 1.1171, cross_entropy loss = 1.1171, 1.4 sec/batch
2019-05-04 13:17:55.331738 #train# step 1333, loss = 1.0795, cross_entropy loss = 1.0795, 1.3 sec/batch
2019-05-04 13:17:56.653473 #train# step 1334, loss = 1.0424, cross_entropy loss = 1.0424, 1.3 sec/batch
2019-05-04 13:17:57.996526 #train# step 1335, loss = 0.9965, cross_entropy loss = 0.9965, 1.3 sec/batch
2019-05-04 13:17:59.326048 #train# step 1336, loss = 0.9673, cross_entropy loss = 0.9673, 1.3 sec/batch
2019-05-04 13:18:00.671854 #train# step 1337, loss = 0.9722, cross_entropy loss = 0.9722, 1.3 sec/batch
2019-05-04 13:18:02.008507 #train# step 1338, loss = 1.0934, cross_entropy loss = 1.0934, 1.3 sec/batch
2019-05-04 13:18:03.347199 #train# step 1339, loss = 1.0232, cross_entropy loss = 1.0232, 1.3 sec/batch
2019-05-04 13:18:04.685304 #train# step 1340, loss = 1.0504, cross_entropy loss = 1.0504, 1.3 sec/batch
2019-05-04 13:18:06.032264 #train# step 1341, loss = 1.0451, cross_entropy loss = 1.0451, 1.3 sec/batch
2019-05-04 13:18:07.376115 #train# step 1342, loss = 0.9586, cross_entropy loss = 0.9586, 1.3 sec/batch
2019-05-04 13:18:08.722369 #train# step 1343, loss = 1.0101, cross_entropy loss = 1.0101, 1.3 sec/batch
2019-05-04 13:18:10.035466 #train# step 1344, loss = 1.0083, cross_entropy loss = 1.0083, 1.3 sec/batch
2019-05-04 13:18:11.369369 #train# step 1345, loss = 1.0102, cross_entropy loss = 1.0102, 1.3 sec/batch
2019-05-04 13:18:12.714856 #train# step 1346, loss = 1.0055, cross_entropy loss = 1.0055, 1.3 sec/batch
2019-05-04 13:18:14.022587 #train# step 1347, loss = 1.0084, cross_entropy loss = 1.0084, 1.3 sec/batch
2019-05-04 13:18:15.358411 #train# step 1348, loss = 1.0459, cross_entropy loss = 1.0459, 1.3 sec/batch
2019-05-04 13:18:16.694187 #train# step 1349, loss = 1.0100, cross_entropy loss = 1.0100, 1.3 sec/batch
2019-05-04 13:18:18.037317 #train# step 1350, loss = 0.9818, cross_entropy loss = 0.9818, 1.3 sec/batch
2019-05-04 13:18:19.377424 #train# step 1351, loss = 0.9901, cross_entropy loss = 0.9901, 1.3 sec/batch
2019-05-04 13:18:20.711430 #train# step 1352, loss = 1.0277, cross_entropy loss = 1.0277, 1.3 sec/batch
2019-05-04 13:18:22.046458 #train# step 1353, loss = 1.0975, cross_entropy loss = 1.0975, 1.3 sec/batch
2019-05-04 13:18:23.376901 #train# step 1354, loss = 0.9859, cross_entropy loss = 0.9859, 1.3 sec/batch
2019-05-04 13:18:24.715305 #train# step 1355, loss = 1.0630, cross_entropy loss = 1.0630, 1.3 sec/batch
2019-05-04 13:18:26.052424 #train# step 1356, loss = 1.0675, cross_entropy loss = 1.0675, 1.3 sec/batch
2019-05-04 13:18:27.383618 #train# step 1357, loss = 0.9730, cross_entropy loss = 0.9730, 1.3 sec/batch
2019-05-04 13:18:28.721246 #train# step 1358, loss = 1.0088, cross_entropy loss = 1.0088, 1.3 sec/batch
2019-05-04 13:18:30.050215 #train# step 1359, loss = 1.0354, cross_entropy loss = 1.0354, 1.3 sec/batch
2019-05-04 13:18:31.387238 #train# step 1360, loss = 1.0140, cross_entropy loss = 1.0140, 1.3 sec/batch
2019-05-04 13:18:32.733875 #train# step 1361, loss = 0.9851, cross_entropy loss = 0.9851, 1.3 sec/batch
2019-05-04 13:18:34.055207 #train# step 1362, loss = 0.9708, cross_entropy loss = 0.9708, 1.3 sec/batch
2019-05-04 13:18:35.387871 #train# step 1363, loss = 1.0479, cross_entropy loss = 1.0479, 1.3 sec/batch
2019-05-04 13:18:36.721957 #train# step 1364, loss = 1.0442, cross_entropy loss = 1.0442, 1.3 sec/batch
2019-05-04 13:18:38.064187 #train# step 1365, loss = 0.9477, cross_entropy loss = 0.9477, 1.3 sec/batch
2019-05-04 13:18:39.386371 #train# step 1366, loss = 0.9794, cross_entropy loss = 0.9794, 1.3 sec/batch
2019-05-04 13:18:40.726178 #train# step 1367, loss = 0.9989, cross_entropy loss = 0.9989, 1.3 sec/batch
2019-05-04 13:18:42.060263 #train# step 1368, loss = 1.0215, cross_entropy loss = 1.0215, 1.3 sec/batch
2019-05-04 13:18:43.400664 #train# step 1369, loss = 1.0213, cross_entropy loss = 1.0213, 1.3 sec/batch
2019-05-04 13:18:44.733849 #train# step 1370, loss = 0.9722, cross_entropy loss = 0.9722, 1.3 sec/batch
2019-05-04 13:18:46.058121 #train# step 1371, loss = 0.9853, cross_entropy loss = 0.9853, 1.3 sec/batch
2019-05-04 13:18:47.380218 #train# step 1372, loss = 1.0359, cross_entropy loss = 1.0359, 1.3 sec/batch
2019-05-04 13:18:48.697704 #train# step 1373, loss = 1.0242, cross_entropy loss = 1.0242, 1.3 sec/batch
2019-05-04 13:18:50.045664 #train# step 1374, loss = 1.0218, cross_entropy loss = 1.0218, 1.3 sec/batch
2019-05-04 13:18:51.380104 #train# step 1375, loss = 1.0057, cross_entropy loss = 1.0057, 1.3 sec/batch
2019-05-04 13:18:52.686633 #train# step 1376, loss = 1.0647, cross_entropy loss = 1.0647, 1.3 sec/batch
2019-05-04 13:18:54.022953 #train# step 1377, loss = 1.0487, cross_entropy loss = 1.0487, 1.3 sec/batch
2019-05-04 13:18:55.356402 #train# step 1378, loss = 1.0145, cross_entropy loss = 1.0145, 1.3 sec/batch
2019-05-04 13:18:56.698220 #train# step 1379, loss = 1.0046, cross_entropy loss = 1.0046, 1.3 sec/batch
2019-05-04 13:18:58.031623 #train# step 1380, loss = 1.0663, cross_entropy loss = 1.0663, 1.3 sec/batch
2019-05-04 13:18:59.372724 #train# step 1381, loss = 1.0148, cross_entropy loss = 1.0148, 1.3 sec/batch
2019-05-04 13:19:00.710846 #train# step 1382, loss = 0.9395, cross_entropy loss = 0.9395, 1.3 sec/batch
2019-05-04 13:19:02.047703 #train# step 1383, loss = 1.0448, cross_entropy loss = 1.0448, 1.3 sec/batch
2019-05-04 13:19:03.393435 #train# step 1384, loss = 0.9981, cross_entropy loss = 0.9981, 1.3 sec/batch
2019-05-04 13:19:04.732855 #train# step 1385, loss = 0.9941, cross_entropy loss = 0.9941, 1.3 sec/batch
2019-05-04 13:19:06.066186 #train# step 1386, loss = 0.9748, cross_entropy loss = 0.9748, 1.3 sec/batch
2019-05-04 13:19:07.418548 #train# step 1387, loss = 1.0211, cross_entropy loss = 1.0211, 1.3 sec/batch
2019-05-04 13:19:08.749295 #train# step 1388, loss = 1.0274, cross_entropy loss = 1.0274, 1.3 sec/batch
2019-05-04 13:19:10.083049 #train# step 1389, loss = 1.0441, cross_entropy loss = 1.0441, 1.3 sec/batch
2019-05-04 13:19:11.416299 #train# step 1390, loss = 1.0387, cross_entropy loss = 1.0387, 1.3 sec/batch
2019-05-04 13:19:12.749031 #train# step 1391, loss = 0.9778, cross_entropy loss = 0.9778, 1.3 sec/batch
2019-05-04 13:19:14.111544 #train# step 1392, loss = 1.0084, cross_entropy loss = 1.0084, 1.3 sec/batch
2019-05-04 13:19:15.455574 #train# step 1393, loss = 1.0541, cross_entropy loss = 1.0541, 1.3 sec/batch
2019-05-04 13:19:16.801058 #train# step 1394, loss = 1.0124, cross_entropy loss = 1.0124, 1.3 sec/batch
2019-05-04 13:19:18.135868 #train# step 1395, loss = 1.0138, cross_entropy loss = 1.0138, 1.3 sec/batch
2019-05-04 13:19:19.470473 #train# step 1396, loss = 0.9576, cross_entropy loss = 0.9576, 1.3 sec/batch
2019-05-04 13:19:20.801089 #train# step 1397, loss = 0.9889, cross_entropy loss = 0.9889, 1.3 sec/batch
2019-05-04 13:19:22.133228 #train# step 1398, loss = 1.0093, cross_entropy loss = 1.0093, 1.3 sec/batch
2019-05-04 13:19:23.480486 #train# step 1399, loss = 1.0115, cross_entropy loss = 1.0115, 1.3 sec/batch
2019-05-04 13:19:24.819988 #train# step 1400, loss = 1.0104, cross_entropy loss = 1.0104, 1.3 sec/batch
2019-05-04 13:19:26.173319 #train# step 1401, loss = 1.0464, cross_entropy loss = 1.0464, 1.3 sec/batch
2019-05-04 13:19:27.521903 #train# step 1402, loss = 1.0339, cross_entropy loss = 1.0339, 1.3 sec/batch
2019-05-04 13:19:28.870196 #train# step 1403, loss = 1.0709, cross_entropy loss = 1.0709, 1.3 sec/batch
2019-05-04 13:19:30.223582 #train# step 1404, loss = 1.0055, cross_entropy loss = 1.0055, 1.3 sec/batch
2019-05-04 13:19:31.579294 #train# step 1405, loss = 0.9816, cross_entropy loss = 0.9816, 1.3 sec/batch
2019-05-04 13:19:32.928865 #train# step 1406, loss = 1.0117, cross_entropy loss = 1.0117, 1.3 sec/batch
2019-05-04 13:19:34.276382 #train# step 1407, loss = 0.9923, cross_entropy loss = 0.9923, 1.3 sec/batch
2019-05-04 13:19:35.621665 #train# step 1408, loss = 1.0804, cross_entropy loss = 1.0804, 1.3 sec/batch
2019-05-04 13:19:36.953953 #train# step 1409, loss = 0.9989, cross_entropy loss = 0.9989, 1.3 sec/batch
2019-05-04 13:19:38.291980 #train# step 1410, loss = 1.0104, cross_entropy loss = 1.0104, 1.3 sec/batch
2019-05-04 13:19:39.633811 #train# step 1411, loss = 0.9525, cross_entropy loss = 0.9525, 1.3 sec/batch
2019-05-04 13:19:40.978384 #train# step 1412, loss = 1.0072, cross_entropy loss = 1.0072, 1.3 sec/batch
2019-05-04 13:19:42.334732 #train# step 1413, loss = 0.9424, cross_entropy loss = 0.9424, 1.3 sec/batch
2019-05-04 13:19:43.674858 #train# step 1414, loss = 1.0226, cross_entropy loss = 1.0226, 1.3 sec/batch
2019-05-04 13:19:45.017428 #train# step 1415, loss = 1.0278, cross_entropy loss = 1.0278, 1.3 sec/batch
2019-05-04 13:19:46.349747 #train# step 1416, loss = 1.0515, cross_entropy loss = 1.0515, 1.3 sec/batch
2019-05-04 13:19:47.726359 #train# step 1417, loss = 1.0279, cross_entropy loss = 1.0279, 1.3 sec/batch
2019-05-04 13:19:49.080318 #train# step 1418, loss = 0.9996, cross_entropy loss = 0.9996, 1.3 sec/batch
2019-05-04 13:19:50.445957 #train# step 1419, loss = 1.0137, cross_entropy loss = 1.0137, 1.3 sec/batch
2019-05-04 13:19:51.797861 #train# step 1420, loss = 1.0126, cross_entropy loss = 1.0126, 1.3 sec/batch
2019-05-04 13:19:53.157752 #train# step 1421, loss = 1.0004, cross_entropy loss = 1.0004, 1.3 sec/batch
2019-05-04 13:19:54.511876 #train# step 1422, loss = 1.0042, cross_entropy loss = 1.0042, 1.3 sec/batch
2019-05-04 13:19:55.864302 #train# step 1423, loss = 1.0324, cross_entropy loss = 1.0324, 1.3 sec/batch
2019-05-04 13:19:57.212980 #train# step 1424, loss = 1.1042, cross_entropy loss = 1.1042, 1.3 sec/batch
2019-05-04 13:19:58.559259 #train# step 1425, loss = 1.0107, cross_entropy loss = 1.0107, 1.3 sec/batch
2019-05-04 13:19:59.898171 #train# step 1426, loss = 1.0259, cross_entropy loss = 1.0259, 1.3 sec/batch
2019-05-04 13:20:01.243641 #train# step 1427, loss = 1.0486, cross_entropy loss = 1.0486, 1.3 sec/batch
2019-05-04 13:20:02.577716 #train# step 1428, loss = 0.9866, cross_entropy loss = 0.9866, 1.3 sec/batch
2019-05-04 13:20:03.923147 #train# step 1429, loss = 1.0093, cross_entropy loss = 1.0093, 1.3 sec/batch
2019-05-04 13:20:05.260773 #train# step 1430, loss = 0.9874, cross_entropy loss = 0.9874, 1.3 sec/batch
2019-05-04 13:20:06.607445 #train# step 1431, loss = 1.0922, cross_entropy loss = 1.0922, 1.3 sec/batch
2019-05-04 13:20:07.949977 #train# step 1432, loss = 0.9984, cross_entropy loss = 0.9984, 1.3 sec/batch
2019-05-04 13:20:09.254256 #train# step 1433, loss = 1.0503, cross_entropy loss = 1.0503, 1.3 sec/batch
2019-05-04 13:20:10.587307 #train# step 1434, loss = 1.0175, cross_entropy loss = 1.0175, 1.3 sec/batch
2019-05-04 13:20:11.911356 #train# step 1435, loss = 0.9871, cross_entropy loss = 0.9871, 1.3 sec/batch
2019-05-04 13:20:13.250184 #train# step 1436, loss = 0.9844, cross_entropy loss = 0.9844, 1.3 sec/batch
2019-05-04 13:20:14.587999 #train# step 1437, loss = 1.0297, cross_entropy loss = 1.0297, 1.3 sec/batch
2019-05-04 13:20:15.920025 #train# step 1438, loss = 0.9848, cross_entropy loss = 0.9848, 1.3 sec/batch
2019-05-04 13:20:17.261408 #train# step 1439, loss = 1.0799, cross_entropy loss = 1.0799, 1.3 sec/batch
2019-05-04 13:20:18.588694 #train# step 1440, loss = 1.0025, cross_entropy loss = 1.0025, 1.3 sec/batch
2019-05-04 13:20:19.926718 #train# step 1441, loss = 1.0108, cross_entropy loss = 1.0108, 1.3 sec/batch
2019-05-04 13:20:21.263645 #train# step 1442, loss = 0.9662, cross_entropy loss = 0.9662, 1.3 sec/batch
2019-05-04 13:20:22.605324 #train# step 1443, loss = 1.0035, cross_entropy loss = 1.0035, 1.3 sec/batch
2019-05-04 13:20:23.938688 #train# step 1444, loss = 1.0407, cross_entropy loss = 1.0407, 1.3 sec/batch
2019-05-04 13:20:25.272638 #train# step 1445, loss = 1.0065, cross_entropy loss = 1.0065, 1.3 sec/batch
2019-05-04 13:20:26.606104 #train# step 1446, loss = 0.9803, cross_entropy loss = 0.9803, 1.3 sec/batch
2019-05-04 13:20:27.944306 #train# step 1447, loss = 1.0385, cross_entropy loss = 1.0385, 1.3 sec/batch
2019-05-04 13:20:29.263535 #train# step 1448, loss = 0.9689, cross_entropy loss = 0.9689, 1.3 sec/batch
2019-05-04 13:20:30.606225 #train# step 1449, loss = 1.0555, cross_entropy loss = 1.0555, 1.3 sec/batch
2019-05-04 13:20:31.932906 #train# step 1450, loss = 0.9565, cross_entropy loss = 0.9565, 1.3 sec/batch
2019-05-04 13:20:33.272170 #train# step 1451, loss = 0.9954, cross_entropy loss = 0.9954, 1.3 sec/batch
2019-05-04 13:20:34.613980 #train# step 1452, loss = 0.9769, cross_entropy loss = 0.9769, 1.3 sec/batch
2019-05-04 13:20:35.935587 #train# step 1453, loss = 0.9969, cross_entropy loss = 0.9969, 1.3 sec/batch
2019-05-04 13:20:37.275172 #train# step 1454, loss = 0.9936, cross_entropy loss = 0.9936, 1.3 sec/batch
2019-05-04 13:20:38.597042 #train# step 1455, loss = 1.0232, cross_entropy loss = 1.0232, 1.3 sec/batch
2019-05-04 13:20:39.915394 #train# step 1456, loss = 0.9972, cross_entropy loss = 0.9972, 1.3 sec/batch
2019-05-04 13:20:41.247366 #train# step 1457, loss = 1.0715, cross_entropy loss = 1.0715, 1.3 sec/batch
2019-05-04 13:20:42.581370 #train# step 1458, loss = 0.9901, cross_entropy loss = 0.9901, 1.3 sec/batch
2019-05-04 13:20:43.907281 #train# step 1459, loss = 0.9793, cross_entropy loss = 0.9793, 1.3 sec/batch
2019-05-04 13:20:45.241648 #train# step 1460, loss = 1.0735, cross_entropy loss = 1.0735, 1.3 sec/batch
2019-05-04 13:20:46.592529 #train# step 1461, loss = 0.9741, cross_entropy loss = 0.9741, 1.3 sec/batch
2019-05-04 13:20:47.939374 #train# step 1462, loss = 1.0306, cross_entropy loss = 1.0306, 1.3 sec/batch
2019-05-04 13:20:49.280973 #train# step 1463, loss = 0.9792, cross_entropy loss = 0.9792, 1.3 sec/batch
2019-05-04 13:20:50.622651 #train# step 1464, loss = 0.9978, cross_entropy loss = 0.9978, 1.3 sec/batch
2019-05-04 13:20:51.960593 #train# step 1465, loss = 1.0788, cross_entropy loss = 1.0788, 1.3 sec/batch
2019-05-04 13:20:53.294528 #train# step 1466, loss = 0.9855, cross_entropy loss = 0.9855, 1.3 sec/batch
2019-05-04 13:20:54.640645 #train# step 1467, loss = 0.9849, cross_entropy loss = 0.9849, 1.3 sec/batch
2019-05-04 13:20:55.982475 #train# step 1468, loss = 1.0345, cross_entropy loss = 1.0345, 1.3 sec/batch
2019-05-04 13:20:57.314900 #train# step 1469, loss = 1.0266, cross_entropy loss = 1.0266, 1.3 sec/batch
2019-05-04 13:20:58.655425 #train# step 1470, loss = 0.9756, cross_entropy loss = 0.9756, 1.3 sec/batch
2019-05-04 13:21:00.007333 #train# step 1471, loss = 1.0158, cross_entropy loss = 1.0158, 1.3 sec/batch
2019-05-04 13:21:01.356317 #train# step 1472, loss = 0.9956, cross_entropy loss = 0.9956, 1.3 sec/batch
2019-05-04 13:21:02.681837 #train# step 1473, loss = 0.9950, cross_entropy loss = 0.9950, 1.3 sec/batch
2019-05-04 13:21:04.042218 #train# step 1474, loss = 1.0607, cross_entropy loss = 1.0607, 1.3 sec/batch
2019-05-04 13:21:05.383541 #train# step 1475, loss = 1.0004, cross_entropy loss = 1.0004, 1.3 sec/batch
2019-05-04 13:21:06.743216 #train# step 1476, loss = 0.9996, cross_entropy loss = 0.9996, 1.3 sec/batch
2019-05-04 13:21:08.092338 #train# step 1477, loss = 0.9495, cross_entropy loss = 0.9495, 1.3 sec/batch
2019-05-04 13:21:09.435431 #train# step 1478, loss = 0.9565, cross_entropy loss = 0.9565, 1.3 sec/batch
2019-05-04 13:21:10.783387 #train# step 1479, loss = 1.0583, cross_entropy loss = 1.0583, 1.3 sec/batch
2019-05-04 13:21:12.120193 #train# step 1480, loss = 1.0046, cross_entropy loss = 1.0046, 1.3 sec/batch
2019-05-04 13:21:13.482507 #train# step 1481, loss = 0.9427, cross_entropy loss = 0.9427, 1.3 sec/batch
2019-05-04 13:21:14.799973 #train# step 1482, loss = 0.9580, cross_entropy loss = 0.9580, 1.3 sec/batch
2019-05-04 13:21:16.147133 #train# step 1483, loss = 0.9403, cross_entropy loss = 0.9403, 1.3 sec/batch
2019-05-04 13:21:17.499677 #train# step 1484, loss = 1.0190, cross_entropy loss = 1.0190, 1.3 sec/batch
2019-05-04 13:21:18.846922 #train# step 1485, loss = 0.9466, cross_entropy loss = 0.9466, 1.3 sec/batch
2019-05-04 13:21:20.186842 #train# step 1486, loss = 1.0235, cross_entropy loss = 1.0235, 1.3 sec/batch
2019-05-04 13:21:21.524627 #train# step 1487, loss = 0.9808, cross_entropy loss = 0.9808, 1.3 sec/batch
2019-05-04 13:21:22.868470 #train# step 1488, loss = 0.9899, cross_entropy loss = 0.9899, 1.3 sec/batch
2019-05-04 13:21:24.221918 #train# step 1489, loss = 0.9465, cross_entropy loss = 0.9465, 1.3 sec/batch
2019-05-04 13:21:25.555089 #train# step 1490, loss = 1.0512, cross_entropy loss = 1.0512, 1.3 sec/batch
2019-05-04 13:21:26.878119 #train# step 1491, loss = 0.9628, cross_entropy loss = 0.9628, 1.3 sec/batch
2019-05-04 13:21:28.224517 #train# step 1492, loss = 0.9943, cross_entropy loss = 0.9943, 1.3 sec/batch
2019-05-04 13:21:29.576445 #train# step 1493, loss = 1.0172, cross_entropy loss = 1.0172, 1.3 sec/batch
2019-05-04 13:21:30.939247 #train# step 1494, loss = 1.0025, cross_entropy loss = 1.0025, 1.3 sec/batch
2019-05-04 13:21:32.293378 #train# step 1495, loss = 0.9816, cross_entropy loss = 0.9816, 1.3 sec/batch
2019-05-04 13:21:33.655191 #train# step 1496, loss = 1.0952, cross_entropy loss = 1.0952, 1.3 sec/batch
2019-05-04 13:21:35.012109 #train# step 1497, loss = 1.0663, cross_entropy loss = 1.0663, 1.3 sec/batch
2019-05-04 13:21:36.381571 #train# step 1498, loss = 0.9950, cross_entropy loss = 0.9950, 1.3 sec/batch
2019-05-04 13:21:37.730557 #train# step 1499, loss = 1.0139, cross_entropy loss = 1.0139, 1.3 sec/batch
2019-05-04 13:21:39.091446 #train# step 1500, loss = 0.9770, cross_entropy loss = 0.9770, 1.3 sec/batch
2019-05-04 13:21:40.488485 #train# step 1501, loss = 1.0632, cross_entropy loss = 1.0632, 1.3 sec/batch
2019-05-04 13:21:41.807895 #train# step 1502, loss = 1.0157, cross_entropy loss = 1.0157, 1.3 sec/batch
2019-05-04 13:21:43.168192 #train# step 1503, loss = 1.0121, cross_entropy loss = 1.0121, 1.3 sec/batch
2019-05-04 13:21:44.515481 #train# step 1504, loss = 1.0471, cross_entropy loss = 1.0471, 1.3 sec/batch
2019-05-04 13:21:45.856133 #train# step 1505, loss = 0.9710, cross_entropy loss = 0.9710, 1.3 sec/batch
2019-05-04 13:21:47.205779 #train# step 1506, loss = 1.0366, cross_entropy loss = 1.0366, 1.3 sec/batch
2019-05-04 13:21:48.555666 #train# step 1507, loss = 0.9345, cross_entropy loss = 0.9345, 1.3 sec/batch
2019-05-04 13:21:49.905983 #train# step 1508, loss = 1.0869, cross_entropy loss = 1.0869, 1.3 sec/batch
2019-05-04 13:21:51.223208 #train# step 1509, loss = 0.9810, cross_entropy loss = 0.9810, 1.3 sec/batch
2019-05-04 13:21:52.575437 #train# step 1510, loss = 1.0374, cross_entropy loss = 1.0374, 1.3 sec/batch
2019-05-04 13:21:53.926026 #train# step 1511, loss = 1.0329, cross_entropy loss = 1.0329, 1.3 sec/batch
2019-05-04 13:21:55.289334 #train# step 1512, loss = 1.0586, cross_entropy loss = 1.0586, 1.3 sec/batch
2019-05-04 13:21:56.647246 #train# step 1513, loss = 0.9886, cross_entropy loss = 0.9886, 1.3 sec/batch
2019-05-04 13:21:57.997932 #train# step 1514, loss = 0.9360, cross_entropy loss = 0.9360, 1.3 sec/batch
2019-05-04 13:21:59.361043 #train# step 1515, loss = 0.9807, cross_entropy loss = 0.9807, 1.3 sec/batch
2019-05-04 13:22:00.722146 #train# step 1516, loss = 0.9596, cross_entropy loss = 0.9596, 1.3 sec/batch
2019-05-04 13:22:02.043501 #train# step 1517, loss = 0.9908, cross_entropy loss = 0.9908, 1.3 sec/batch
2019-05-04 13:22:03.393553 #train# step 1518, loss = 1.0207, cross_entropy loss = 1.0207, 1.3 sec/batch
2019-05-04 13:22:04.743213 #train# step 1519, loss = 0.9764, cross_entropy loss = 0.9764, 1.3 sec/batch
2019-05-04 13:22:06.095428 #train# step 1520, loss = 0.9654, cross_entropy loss = 0.9654, 1.3 sec/batch
2019-05-04 13:22:07.447716 #train# step 1521, loss = 1.0035, cross_entropy loss = 1.0035, 1.3 sec/batch
2019-05-04 13:22:08.787903 #train# step 1522, loss = 1.0657, cross_entropy loss = 1.0657, 1.3 sec/batch
2019-05-04 13:22:10.132011 #train# step 1523, loss = 1.0235, cross_entropy loss = 1.0235, 1.3 sec/batch
2019-05-04 13:22:11.480708 #train# step 1524, loss = 0.9324, cross_entropy loss = 0.9324, 1.3 sec/batch
2019-05-04 13:22:12.810172 #train# step 1525, loss = 1.0027, cross_entropy loss = 1.0027, 1.3 sec/batch
2019-05-04 13:22:14.161649 #train# step 1526, loss = 1.0441, cross_entropy loss = 1.0441, 1.3 sec/batch
2019-05-04 13:22:15.491686 #train# step 1527, loss = 0.9316, cross_entropy loss = 0.9316, 1.3 sec/batch
2019-05-04 13:22:16.839168 #train# step 1528, loss = 1.0071, cross_entropy loss = 1.0071, 1.3 sec/batch
2019-05-04 13:22:18.170129 #train# step 1529, loss = 0.9813, cross_entropy loss = 0.9813, 1.3 sec/batch
2019-05-04 13:22:19.497000 #train# step 1530, loss = 1.0718, cross_entropy loss = 1.0718, 1.3 sec/batch
2019-05-04 13:22:20.839699 #train# step 1531, loss = 1.0617, cross_entropy loss = 1.0617, 1.3 sec/batch
2019-05-04 13:22:22.177736 #train# step 1532, loss = 1.0267, cross_entropy loss = 1.0267, 1.3 sec/batch
2019-05-04 13:22:23.526707 #train# step 1533, loss = 0.9694, cross_entropy loss = 0.9694, 1.3 sec/batch
2019-05-04 13:22:24.829783 #train# step 1534, loss = 0.9519, cross_entropy loss = 0.9519, 1.3 sec/batch
2019-05-04 13:22:26.172034 #train# step 1535, loss = 1.0511, cross_entropy loss = 1.0511, 1.3 sec/batch
2019-05-04 13:22:27.529242 #train# step 1536, loss = 1.0070, cross_entropy loss = 1.0070, 1.3 sec/batch
2019-05-04 13:22:28.864088 #train# step 1537, loss = 0.9638, cross_entropy loss = 0.9638, 1.3 sec/batch
2019-05-04 13:22:30.229059 #train# step 1538, loss = 0.9824, cross_entropy loss = 0.9824, 1.3 sec/batch
2019-05-04 13:22:31.571151 #train# step 1539, loss = 0.9762, cross_entropy loss = 0.9762, 1.3 sec/batch
2019-05-04 13:22:32.920428 #train# step 1540, loss = 0.9396, cross_entropy loss = 0.9396, 1.3 sec/batch
2019-05-04 13:22:34.287459 #train# step 1541, loss = 1.0149, cross_entropy loss = 1.0149, 1.3 sec/batch
2019-05-04 13:22:35.637377 #train# step 1542, loss = 0.9411, cross_entropy loss = 0.9411, 1.3 sec/batch
2019-05-04 13:22:36.999897 #train# step 1543, loss = 0.9711, cross_entropy loss = 0.9711, 1.3 sec/batch
2019-05-04 13:22:38.365787 #train# step 1544, loss = 0.9931, cross_entropy loss = 0.9931, 1.3 sec/batch
2019-05-04 13:22:39.716389 #train# step 1545, loss = 1.0141, cross_entropy loss = 1.0141, 1.3 sec/batch
2019-05-04 13:22:41.084642 #train# step 1546, loss = 1.0052, cross_entropy loss = 1.0052, 1.3 sec/batch
2019-05-04 13:22:42.448563 #train# step 1547, loss = 0.9964, cross_entropy loss = 0.9964, 1.3 sec/batch
2019-05-04 13:22:43.818809 #train# step 1548, loss = 1.0234, cross_entropy loss = 1.0234, 1.3 sec/batch
2019-05-04 13:22:45.240177 #train# step 1549, loss = 1.0539, cross_entropy loss = 1.0539, 1.4 sec/batch
2019-05-04 13:22:46.606294 #train# step 1550, loss = 0.9794, cross_entropy loss = 0.9794, 1.3 sec/batch
2019-05-04 13:22:48.059368 #train# step 1551, loss = 0.9829, cross_entropy loss = 0.9829, 1.4 sec/batch
2019-05-04 13:22:49.496783 #train# step 1552, loss = 1.0024, cross_entropy loss = 1.0024, 1.4 sec/batch
2019-05-04 13:22:50.859661 #train# step 1553, loss = 1.0385, cross_entropy loss = 1.0385, 1.3 sec/batch
2019-05-04 13:22:52.279092 #train# step 1554, loss = 0.9795, cross_entropy loss = 0.9795, 1.4 sec/batch
2019-05-04 13:22:53.743952 #train# step 1555, loss = 1.0177, cross_entropy loss = 1.0177, 1.4 sec/batch
2019-05-04 13:22:55.202179 #train# step 1556, loss = 0.9608, cross_entropy loss = 0.9608, 1.4 sec/batch
2019-05-04 13:22:56.568176 #train# step 1557, loss = 0.9839, cross_entropy loss = 0.9839, 1.3 sec/batch
2019-05-04 13:22:57.931615 #train# step 1558, loss = 1.0189, cross_entropy loss = 1.0189, 1.3 sec/batch
2019-05-04 13:22:59.293501 #train# step 1559, loss = 0.9634, cross_entropy loss = 0.9634, 1.3 sec/batch
2019-05-04 13:23:00.653762 #train# step 1560, loss = 0.9695, cross_entropy loss = 0.9695, 1.3 sec/batch
2019-05-04 13:23:02.014395 #train# step 1561, loss = 1.0007, cross_entropy loss = 1.0007, 1.3 sec/batch
2019-05-04 13:23:03.374485 #train# step 1562, loss = 0.9864, cross_entropy loss = 0.9864, 1.3 sec/batch
2019-05-04 13:23:04.738298 #train# step 1563, loss = 1.0349, cross_entropy loss = 1.0349, 1.3 sec/batch
2019-05-04 13:23:06.106913 #train# step 1564, loss = 0.9271, cross_entropy loss = 0.9271, 1.3 sec/batch
2019-05-04 13:23:07.466772 #train# step 1565, loss = 1.0018, cross_entropy loss = 1.0018, 1.3 sec/batch
2019-05-04 13:23:08.820967 #train# step 1566, loss = 1.0193, cross_entropy loss = 1.0193, 1.3 sec/batch
2019-05-04 13:23:10.178104 #train# step 1567, loss = 0.9583, cross_entropy loss = 0.9583, 1.3 sec/batch
2019-05-04 13:23:11.542582 #train# step 1568, loss = 1.0257, cross_entropy loss = 1.0257, 1.3 sec/batch
2019-05-04 13:23:12.907810 #train# step 1569, loss = 0.9713, cross_entropy loss = 0.9713, 1.3 sec/batch
2019-05-04 13:23:14.262250 #train# step 1570, loss = 0.9930, cross_entropy loss = 0.9930, 1.3 sec/batch
2019-05-04 13:23:15.627753 #train# step 1571, loss = 0.9467, cross_entropy loss = 0.9467, 1.3 sec/batch
2019-05-04 13:23:17.000393 #train# step 1572, loss = 0.9932, cross_entropy loss = 0.9932, 1.3 sec/batch
2019-05-04 13:23:18.350359 #train# step 1573, loss = 0.9952, cross_entropy loss = 0.9952, 1.3 sec/batch
2019-05-04 13:23:19.704706 #train# step 1574, loss = 1.0427, cross_entropy loss = 1.0427, 1.3 sec/batch
2019-05-04 13:23:21.060318 #train# step 1575, loss = 1.0369, cross_entropy loss = 1.0369, 1.3 sec/batch
2019-05-04 13:23:22.435215 #train# step 1576, loss = 1.0090, cross_entropy loss = 1.0090, 1.3 sec/batch
2019-05-04 13:23:23.801714 #train# step 1577, loss = 1.0228, cross_entropy loss = 1.0228, 1.3 sec/batch
2019-05-04 13:23:25.158248 #train# step 1578, loss = 0.9734, cross_entropy loss = 0.9734, 1.3 sec/batch
2019-05-04 13:23:26.507737 #train# step 1579, loss = 0.9462, cross_entropy loss = 0.9462, 1.3 sec/batch
2019-05-04 13:23:27.861626 #train# step 1580, loss = 1.0515, cross_entropy loss = 1.0515, 1.3 sec/batch
2019-05-04 13:23:29.210086 #train# step 1581, loss = 0.9851, cross_entropy loss = 0.9851, 1.3 sec/batch
2019-05-04 13:23:30.553496 #train# step 1582, loss = 1.0392, cross_entropy loss = 1.0392, 1.3 sec/batch
2019-05-04 13:23:31.899278 #train# step 1583, loss = 0.9728, cross_entropy loss = 0.9728, 1.3 sec/batch
2019-05-04 13:23:33.236743 #train# step 1584, loss = 1.0342, cross_entropy loss = 1.0342, 1.3 sec/batch
2019-05-04 13:23:34.576335 #train# step 1585, loss = 0.9434, cross_entropy loss = 0.9434, 1.3 sec/batch
2019-05-04 13:23:35.935700 #train# step 1586, loss = 1.0708, cross_entropy loss = 1.0708, 1.3 sec/batch
2019-05-04 13:23:37.297542 #train# step 1587, loss = 0.9725, cross_entropy loss = 0.9725, 1.3 sec/batch
2019-05-04 13:23:38.645467 #train# step 1588, loss = 0.9892, cross_entropy loss = 0.9892, 1.3 sec/batch
2019-05-04 13:23:39.997487 #train# step 1589, loss = 0.9680, cross_entropy loss = 0.9680, 1.3 sec/batch
2019-05-04 13:23:41.326521 #train# step 1590, loss = 0.9701, cross_entropy loss = 0.9701, 1.3 sec/batch
2019-05-04 13:23:42.676276 #train# step 1591, loss = 0.9537, cross_entropy loss = 0.9537, 1.3 sec/batch
2019-05-04 13:23:44.022722 #train# step 1592, loss = 0.9609, cross_entropy loss = 0.9609, 1.3 sec/batch
2019-05-04 13:23:45.383940 #train# step 1593, loss = 0.9783, cross_entropy loss = 0.9783, 1.3 sec/batch
2019-05-04 13:23:46.718660 #train# step 1594, loss = 1.0498, cross_entropy loss = 1.0498, 1.3 sec/batch
2019-05-04 13:23:48.083045 #train# step 1595, loss = 1.0145, cross_entropy loss = 1.0145, 1.3 sec/batch
2019-05-04 13:23:49.419958 #train# step 1596, loss = 1.0292, cross_entropy loss = 1.0292, 1.3 sec/batch
2019-05-04 13:23:50.766253 #train# step 1597, loss = 0.9856, cross_entropy loss = 0.9856, 1.3 sec/batch
2019-05-04 13:23:52.125043 #train# step 1598, loss = 0.9881, cross_entropy loss = 0.9881, 1.3 sec/batch
2019-05-04 13:23:53.549947 #train# step 1599, loss = 0.9922, cross_entropy loss = 0.9922, 1.4 sec/batch
2019-05-04 13:23:54.980711 #train# step 1600, loss = 0.9981, cross_entropy loss = 0.9981, 1.4 sec/batch
2019-05-04 13:23:56.348333 #train# step 1601, loss = 1.0589, cross_entropy loss = 1.0589, 1.3 sec/batch
2019-05-04 13:23:57.707722 #train# step 1602, loss = 0.9666, cross_entropy loss = 0.9666, 1.3 sec/batch
2019-05-04 13:23:59.081706 #train# step 1603, loss = 1.0092, cross_entropy loss = 1.0092, 1.3 sec/batch
2019-05-04 13:24:00.451765 #train# step 1604, loss = 0.9784, cross_entropy loss = 0.9784, 1.3 sec/batch
2019-05-04 13:24:02.039533 #train# step 1605, loss = 0.9830, cross_entropy loss = 0.9830, 1.5 sec/batch
2019-05-04 13:24:03.403448 #train# step 1606, loss = 0.9341, cross_entropy loss = 0.9341, 1.3 sec/batch
2019-05-04 13:24:04.773864 #train# step 1607, loss = 1.0031, cross_entropy loss = 1.0031, 1.3 sec/batch
2019-05-04 13:24:06.143935 #train# step 1608, loss = 1.0221, cross_entropy loss = 1.0221, 1.3 sec/batch
2019-05-04 13:24:07.490190 #train# step 1609, loss = 1.0214, cross_entropy loss = 1.0214, 1.3 sec/batch
2019-05-04 13:24:08.861061 #train# step 1610, loss = 0.9702, cross_entropy loss = 0.9702, 1.3 sec/batch
2019-05-04 13:24:10.204146 #train# step 1611, loss = 1.0154, cross_entropy loss = 1.0154, 1.3 sec/batch
2019-05-04 13:24:11.569905 #train# step 1612, loss = 1.0175, cross_entropy loss = 1.0175, 1.3 sec/batch
2019-05-04 13:24:12.945433 #train# step 1613, loss = 1.0180, cross_entropy loss = 1.0180, 1.3 sec/batch
2019-05-04 13:24:14.316626 #train# step 1614, loss = 0.9561, cross_entropy loss = 0.9561, 1.3 sec/batch
2019-05-04 13:24:15.687113 #train# step 1615, loss = 1.0129, cross_entropy loss = 1.0129, 1.3 sec/batch
2019-05-04 13:24:17.032255 #train# step 1616, loss = 0.9773, cross_entropy loss = 0.9773, 1.3 sec/batch
2019-05-04 13:24:18.389822 #train# step 1617, loss = 0.9777, cross_entropy loss = 0.9777, 1.3 sec/batch
2019-05-04 13:24:19.746692 #train# step 1618, loss = 0.9900, cross_entropy loss = 0.9900, 1.3 sec/batch
2019-05-04 13:24:21.104222 #train# step 1619, loss = 0.9579, cross_entropy loss = 0.9579, 1.3 sec/batch
2019-05-04 13:24:22.457991 #train# step 1620, loss = 1.0523, cross_entropy loss = 1.0523, 1.3 sec/batch
2019-05-04 13:24:23.810742 #train# step 1621, loss = 1.0837, cross_entropy loss = 1.0837, 1.3 sec/batch
2019-05-04 13:24:25.165385 #train# step 1622, loss = 0.9782, cross_entropy loss = 0.9782, 1.3 sec/batch
2019-05-04 13:24:26.522695 #train# step 1623, loss = 1.0497, cross_entropy loss = 1.0497, 1.3 sec/batch
2019-05-04 13:24:27.883153 #train# step 1624, loss = 0.9403, cross_entropy loss = 0.9403, 1.3 sec/batch
2019-05-04 13:24:29.239638 #train# step 1625, loss = 0.9982, cross_entropy loss = 0.9982, 1.3 sec/batch
2019-05-04 13:24:30.597946 #train# step 1626, loss = 0.9920, cross_entropy loss = 0.9920, 1.3 sec/batch
2019-05-04 13:24:31.964014 #train# step 1627, loss = 0.9804, cross_entropy loss = 0.9804, 1.3 sec/batch
2019-05-04 13:24:33.327098 #train# step 1628, loss = 0.9532, cross_entropy loss = 0.9532, 1.3 sec/batch
2019-05-04 13:24:34.685370 #train# step 1629, loss = 1.0345, cross_entropy loss = 1.0345, 1.3 sec/batch
2019-05-04 13:24:36.042257 #train# step 1630, loss = 0.9924, cross_entropy loss = 0.9924, 1.3 sec/batch
2019-05-04 13:24:37.402791 #train# step 1631, loss = 0.9464, cross_entropy loss = 0.9464, 1.3 sec/batch
2019-05-04 13:24:38.766262 #train# step 1632, loss = 0.9341, cross_entropy loss = 0.9341, 1.3 sec/batch
2019-05-04 13:24:40.123910 #train# step 1633, loss = 1.0040, cross_entropy loss = 1.0040, 1.3 sec/batch
2019-05-04 13:24:41.477096 #train# step 1634, loss = 1.0359, cross_entropy loss = 1.0359, 1.3 sec/batch
2019-05-04 13:24:42.811862 #train# step 1635, loss = 1.0212, cross_entropy loss = 1.0212, 1.3 sec/batch
2019-05-04 13:24:44.164811 #train# step 1636, loss = 1.0198, cross_entropy loss = 1.0198, 1.3 sec/batch
2019-05-04 13:24:45.504250 #train# step 1637, loss = 0.9203, cross_entropy loss = 0.9203, 1.3 sec/batch
2019-05-04 13:24:46.858177 #train# step 1638, loss = 0.9512, cross_entropy loss = 0.9512, 1.3 sec/batch
2019-05-04 13:24:48.185809 #train# step 1639, loss = 1.0065, cross_entropy loss = 1.0065, 1.3 sec/batch
2019-05-04 13:24:49.547617 #train# step 1640, loss = 1.0500, cross_entropy loss = 1.0500, 1.3 sec/batch
2019-05-04 13:24:50.904267 #train# step 1641, loss = 0.9660, cross_entropy loss = 0.9660, 1.3 sec/batch
2019-05-04 13:24:52.305503 #train# step 1642, loss = 0.9495, cross_entropy loss = 0.9495, 1.4 sec/batch
2019-05-04 13:24:53.778181 #train# step 1643, loss = 0.9949, cross_entropy loss = 0.9949, 1.4 sec/batch
2019-05-04 13:24:55.237471 #train# step 1644, loss = 0.9399, cross_entropy loss = 0.9399, 1.4 sec/batch
2019-05-04 13:24:56.610139 #train# step 1645, loss = 1.0368, cross_entropy loss = 1.0368, 1.3 sec/batch
2019-05-04 13:24:57.989773 #train# step 1646, loss = 0.9932, cross_entropy loss = 0.9932, 1.3 sec/batch
2019-05-04 13:24:59.371285 #train# step 1647, loss = 0.8721, cross_entropy loss = 0.8721, 1.3 sec/batch
2019-05-04 13:25:00.743036 #train# step 1648, loss = 1.0057, cross_entropy loss = 1.0057, 1.3 sec/batch
2019-05-04 13:25:02.102858 #train# step 1649, loss = 0.9435, cross_entropy loss = 0.9435, 1.3 sec/batch
2019-05-04 13:25:03.470791 #train# step 1650, loss = 0.9880, cross_entropy loss = 0.9880, 1.3 sec/batch
2019-05-04 13:25:04.839315 #train# step 1651, loss = 1.0458, cross_entropy loss = 1.0458, 1.3 sec/batch
2019-05-04 13:25:06.212665 #train# step 1652, loss = 1.0084, cross_entropy loss = 1.0084, 1.3 sec/batch
2019-05-04 13:25:07.584044 #train# step 1653, loss = 1.0603, cross_entropy loss = 1.0603, 1.3 sec/batch
2019-05-04 13:25:08.949823 #train# step 1654, loss = 0.9513, cross_entropy loss = 0.9513, 1.3 sec/batch
2019-05-04 13:25:10.314199 #train# step 1655, loss = 1.0214, cross_entropy loss = 1.0214, 1.3 sec/batch
2019-05-04 13:25:11.674351 #train# step 1656, loss = 1.0207, cross_entropy loss = 1.0207, 1.3 sec/batch
2019-05-04 13:25:13.012005 #train# step 1657, loss = 0.9302, cross_entropy loss = 0.9302, 1.3 sec/batch
2019-05-04 13:25:14.381761 #train# step 1658, loss = 1.0464, cross_entropy loss = 1.0464, 1.3 sec/batch
2019-05-04 13:25:15.738422 #train# step 1659, loss = 0.9786, cross_entropy loss = 0.9786, 1.3 sec/batch
2019-05-04 13:25:17.093599 #train# step 1660, loss = 0.9740, cross_entropy loss = 0.9740, 1.3 sec/batch
2019-05-04 13:25:18.452968 #train# step 1661, loss = 1.0134, cross_entropy loss = 1.0134, 1.3 sec/batch
2019-05-04 13:25:19.804714 #train# step 1662, loss = 0.9785, cross_entropy loss = 0.9785, 1.3 sec/batch
2019-05-04 13:25:21.158876 #train# step 1663, loss = 0.9775, cross_entropy loss = 0.9775, 1.3 sec/batch
2019-05-04 13:25:22.493381 #train# step 1664, loss = 0.9351, cross_entropy loss = 0.9351, 1.3 sec/batch
2019-05-04 13:25:23.871672 #train# step 1665, loss = 1.0378, cross_entropy loss = 1.0378, 1.3 sec/batch
2019-05-04 13:25:25.295071 #train# step 1666, loss = 0.9787, cross_entropy loss = 0.9787, 1.4 sec/batch
2019-05-04 13:25:26.740440 #train# step 1667, loss = 0.9585, cross_entropy loss = 0.9585, 1.4 sec/batch
2019-05-04 13:25:28.181056 #train# step 1668, loss = 0.9854, cross_entropy loss = 0.9854, 1.4 sec/batch
2019-05-04 13:25:29.569713 #train# step 1669, loss = 0.9597, cross_entropy loss = 0.9597, 1.3 sec/batch
2019-05-04 13:25:30.932258 #train# step 1670, loss = 0.9974, cross_entropy loss = 0.9974, 1.3 sec/batch
2019-05-04 13:25:32.308924 #train# step 1671, loss = 0.9407, cross_entropy loss = 0.9407, 1.3 sec/batch
2019-05-04 13:25:33.685873 #train# step 1672, loss = 0.9286, cross_entropy loss = 0.9286, 1.3 sec/batch
2019-05-04 13:25:35.056725 #train# step 1673, loss = 0.9987, cross_entropy loss = 0.9987, 1.3 sec/batch
2019-05-04 13:25:36.394350 #train# step 1674, loss = 0.9069, cross_entropy loss = 0.9069, 1.3 sec/batch
2019-05-04 13:25:37.763239 #train# step 1675, loss = 1.0017, cross_entropy loss = 1.0017, 1.3 sec/batch
2019-05-04 13:25:39.105147 #train# step 1676, loss = 0.9449, cross_entropy loss = 0.9449, 1.3 sec/batch
2019-05-04 13:25:40.472877 #train# step 1677, loss = 0.9226, cross_entropy loss = 0.9226, 1.3 sec/batch
2019-05-04 13:25:41.839909 #train# step 1678, loss = 1.0049, cross_entropy loss = 1.0049, 1.3 sec/batch
2019-05-04 13:25:43.203364 #train# step 1679, loss = 0.9778, cross_entropy loss = 0.9778, 1.3 sec/batch
2019-05-04 13:25:44.567128 #train# step 1680, loss = 0.9620, cross_entropy loss = 0.9620, 1.3 sec/batch
2019-05-04 13:25:45.933192 #train# step 1681, loss = 0.9698, cross_entropy loss = 0.9698, 1.3 sec/batch
2019-05-04 13:25:47.303042 #train# step 1682, loss = 0.9227, cross_entropy loss = 0.9227, 1.3 sec/batch
2019-05-04 13:25:48.675246 #train# step 1683, loss = 0.9576, cross_entropy loss = 0.9576, 1.3 sec/batch
2019-05-04 13:25:50.047914 #train# step 1684, loss = 0.9554, cross_entropy loss = 0.9554, 1.3 sec/batch
2019-05-04 13:25:51.416257 #train# step 1685, loss = 0.9773, cross_entropy loss = 0.9773, 1.3 sec/batch
2019-05-04 13:25:52.787023 #train# step 1686, loss = 1.0166, cross_entropy loss = 1.0166, 1.3 sec/batch
2019-05-04 13:25:54.156377 #train# step 1687, loss = 0.9673, cross_entropy loss = 0.9673, 1.3 sec/batch
2019-05-04 13:25:55.531045 #train# step 1688, loss = 1.0592, cross_entropy loss = 1.0592, 1.3 sec/batch
2019-05-04 13:25:56.899733 #train# step 1689, loss = 1.0202, cross_entropy loss = 1.0202, 1.3 sec/batch
2019-05-04 13:25:58.267264 #train# step 1690, loss = 0.9475, cross_entropy loss = 0.9475, 1.3 sec/batch
2019-05-04 13:25:59.641881 #train# step 1691, loss = 1.0106, cross_entropy loss = 1.0106, 1.3 sec/batch
2019-05-04 13:26:01.012410 #train# step 1692, loss = 0.9702, cross_entropy loss = 0.9702, 1.3 sec/batch
2019-05-04 13:26:02.373700 #train# step 1693, loss = 0.9970, cross_entropy loss = 0.9970, 1.3 sec/batch
2019-05-04 13:26:03.731358 #train# step 1694, loss = 0.9649, cross_entropy loss = 0.9649, 1.3 sec/batch
2019-05-04 13:26:05.090655 #train# step 1695, loss = 1.0127, cross_entropy loss = 1.0127, 1.3 sec/batch
2019-05-04 13:26:06.454813 #train# step 1696, loss = 0.9679, cross_entropy loss = 0.9679, 1.3 sec/batch
2019-05-04 13:26:07.814770 #train# step 1697, loss = 0.9666, cross_entropy loss = 0.9666, 1.3 sec/batch
2019-05-04 13:26:09.161483 #train# step 1698, loss = 0.9897, cross_entropy loss = 0.9897, 1.3 sec/batch
2019-05-04 13:26:10.526872 #train# step 1699, loss = 0.9917, cross_entropy loss = 0.9917, 1.3 sec/batch
2019-05-04 13:26:11.880390 #train# step 1700, loss = 0.9972, cross_entropy loss = 0.9972, 1.3 sec/batch
2019-05-04 13:26:13.238297 #train# step 1701, loss = 0.9448, cross_entropy loss = 0.9448, 1.3 sec/batch
2019-05-04 13:26:14.585330 #train# step 1702, loss = 0.9847, cross_entropy loss = 0.9847, 1.3 sec/batch
2019-05-04 13:26:15.951264 #train# step 1703, loss = 1.0297, cross_entropy loss = 1.0297, 1.3 sec/batch
2019-05-04 13:26:17.310543 #train# step 1704, loss = 0.9427, cross_entropy loss = 0.9427, 1.3 sec/batch
2019-05-04 13:26:18.671370 #train# step 1705, loss = 0.9857, cross_entropy loss = 0.9857, 1.3 sec/batch
2019-05-04 13:26:20.019402 #train# step 1706, loss = 0.9668, cross_entropy loss = 0.9668, 1.3 sec/batch
2019-05-04 13:26:21.390638 #train# step 1707, loss = 0.9937, cross_entropy loss = 0.9937, 1.3 sec/batch
2019-05-04 13:26:22.795809 #train# step 1708, loss = 0.9471, cross_entropy loss = 0.9471, 1.4 sec/batch
2019-05-04 13:26:24.191721 #train# step 1709, loss = 1.0195, cross_entropy loss = 1.0195, 1.4 sec/batch
2019-05-04 13:26:25.574913 #train# step 1710, loss = 0.9765, cross_entropy loss = 0.9765, 1.3 sec/batch
2019-05-04 13:26:26.994350 #train# step 1711, loss = 0.9800, cross_entropy loss = 0.9800, 1.4 sec/batch
2019-05-04 13:26:28.376546 #train# step 1712, loss = 0.9671, cross_entropy loss = 0.9671, 1.3 sec/batch
2019-05-04 13:26:29.746759 #train# step 1713, loss = 0.9765, cross_entropy loss = 0.9765, 1.3 sec/batch
2019-05-04 13:26:31.126601 #train# step 1714, loss = 0.9329, cross_entropy loss = 0.9329, 1.3 sec/batch
2019-05-04 13:26:32.497425 #train# step 1715, loss = 0.9962, cross_entropy loss = 0.9962, 1.3 sec/batch
2019-05-04 13:26:33.872378 #train# step 1716, loss = 0.9941, cross_entropy loss = 0.9941, 1.3 sec/batch
2019-05-04 13:26:35.220327 #train# step 1717, loss = 0.9896, cross_entropy loss = 0.9896, 1.3 sec/batch
2019-05-04 13:26:36.580791 #train# step 1718, loss = 0.9823, cross_entropy loss = 0.9823, 1.3 sec/batch
2019-05-04 13:26:37.951751 #train# step 1719, loss = 0.9160, cross_entropy loss = 0.9160, 1.3 sec/batch
2019-05-04 13:26:39.296359 #train# step 1720, loss = 0.9629, cross_entropy loss = 0.9629, 1.3 sec/batch
2019-05-04 13:26:40.673147 #train# step 1721, loss = 1.0044, cross_entropy loss = 1.0044, 1.3 sec/batch
2019-05-04 13:26:42.037429 #train# step 1722, loss = 0.9927, cross_entropy loss = 0.9927, 1.3 sec/batch
2019-05-04 13:26:43.419208 #train# step 1723, loss = 0.9438, cross_entropy loss = 0.9438, 1.3 sec/batch
2019-05-04 13:26:44.797347 #train# step 1724, loss = 0.9709, cross_entropy loss = 0.9709, 1.3 sec/batch
2019-05-04 13:26:46.166303 #train# step 1725, loss = 0.9677, cross_entropy loss = 0.9677, 1.3 sec/batch
2019-05-04 13:26:47.546699 #train# step 1726, loss = 0.9843, cross_entropy loss = 0.9843, 1.3 sec/batch
2019-05-04 13:26:48.928552 #train# step 1727, loss = 1.0369, cross_entropy loss = 1.0369, 1.3 sec/batch
2019-05-04 13:26:50.303215 #train# step 1728, loss = 0.9661, cross_entropy loss = 0.9661, 1.3 sec/batch
2019-05-04 13:26:51.683974 #train# step 1729, loss = 0.9764, cross_entropy loss = 0.9764, 1.3 sec/batch
2019-05-04 13:26:53.049344 #train# step 1730, loss = 0.9795, cross_entropy loss = 0.9795, 1.3 sec/batch
2019-05-04 13:26:54.413352 #train# step 1731, loss = 0.9332, cross_entropy loss = 0.9332, 1.3 sec/batch
2019-05-04 13:26:55.777537 #train# step 1732, loss = 0.8959, cross_entropy loss = 0.8959, 1.3 sec/batch
2019-05-04 13:26:57.139534 #train# step 1733, loss = 0.9644, cross_entropy loss = 0.9644, 1.3 sec/batch
2019-05-04 13:26:58.504965 #train# step 1734, loss = 1.0067, cross_entropy loss = 1.0067, 1.3 sec/batch
2019-05-04 13:26:59.864582 #train# step 1735, loss = 0.9888, cross_entropy loss = 0.9888, 1.3 sec/batch
2019-05-04 13:27:01.222902 #train# step 1736, loss = 0.9799, cross_entropy loss = 0.9799, 1.3 sec/batch
2019-05-04 13:27:02.590185 #train# step 1737, loss = 0.9822, cross_entropy loss = 0.9822, 1.3 sec/batch
2019-05-04 13:27:03.958360 #train# step 1738, loss = 1.0004, cross_entropy loss = 1.0004, 1.3 sec/batch
2019-05-04 13:27:05.325815 #train# step 1739, loss = 0.9650, cross_entropy loss = 0.9650, 1.3 sec/batch
2019-05-04 13:27:06.687001 #train# step 1740, loss = 0.9580, cross_entropy loss = 0.9580, 1.3 sec/batch
2019-05-04 13:27:08.049312 #train# step 1741, loss = 1.0152, cross_entropy loss = 1.0152, 1.3 sec/batch
2019-05-04 13:27:09.409110 #train# step 1742, loss = 0.9467, cross_entropy loss = 0.9467, 1.3 sec/batch
2019-05-04 13:27:10.772480 #train# step 1743, loss = 1.0040, cross_entropy loss = 1.0040, 1.3 sec/batch
2019-05-04 13:27:12.142868 #train# step 1744, loss = 0.9806, cross_entropy loss = 0.9806, 1.3 sec/batch
2019-05-04 13:27:13.503356 #train# step 1745, loss = 0.9752, cross_entropy loss = 0.9752, 1.3 sec/batch
2019-05-04 13:27:14.855315 #train# step 1746, loss = 1.0132, cross_entropy loss = 1.0132, 1.3 sec/batch
2019-05-04 13:27:16.218634 #train# step 1747, loss = 0.9917, cross_entropy loss = 0.9917, 1.3 sec/batch
2019-05-04 13:27:17.575171 #train# step 1748, loss = 1.0100, cross_entropy loss = 1.0100, 1.3 sec/batch
2019-05-04 13:27:18.935796 #train# step 1749, loss = 0.9432, cross_entropy loss = 0.9432, 1.3 sec/batch
2019-05-04 13:27:20.343674 #train# step 1750, loss = 0.9365, cross_entropy loss = 0.9365, 1.4 sec/batch
2019-05-04 13:27:21.751953 #train# step 1751, loss = 0.9837, cross_entropy loss = 0.9837, 1.4 sec/batch
2019-05-04 13:27:23.200692 #train# step 1752, loss = 0.9469, cross_entropy loss = 0.9469, 1.4 sec/batch
2019-05-04 13:27:24.627322 #train# step 1753, loss = 0.9262, cross_entropy loss = 0.9262, 1.4 sec/batch
2019-05-04 13:27:26.015775 #train# step 1754, loss = 0.9765, cross_entropy loss = 0.9765, 1.3 sec/batch
2019-05-04 13:27:27.404484 #train# step 1755, loss = 0.9125, cross_entropy loss = 0.9125, 1.3 sec/batch
2019-05-04 13:27:28.780743 #train# step 1756, loss = 0.9270, cross_entropy loss = 0.9270, 1.3 sec/batch
2019-05-04 13:27:30.161342 #train# step 1757, loss = 0.9441, cross_entropy loss = 0.9441, 1.3 sec/batch
2019-05-04 13:27:31.523547 #train# step 1758, loss = 0.9826, cross_entropy loss = 0.9826, 1.3 sec/batch
2019-05-04 13:27:32.905024 #train# step 1759, loss = 0.9949, cross_entropy loss = 0.9949, 1.3 sec/batch
2019-05-04 13:27:34.281696 #train# step 1760, loss = 0.9481, cross_entropy loss = 0.9481, 1.3 sec/batch
2019-05-04 13:27:35.663088 #train# step 1761, loss = 0.9865, cross_entropy loss = 0.9865, 1.3 sec/batch
2019-05-04 13:27:37.005597 #train# step 1762, loss = 1.0173, cross_entropy loss = 1.0173, 1.3 sec/batch
2019-05-04 13:27:38.384868 #train# step 1763, loss = 0.9647, cross_entropy loss = 0.9647, 1.3 sec/batch
2019-05-04 13:27:39.762721 #train# step 1764, loss = 0.9566, cross_entropy loss = 0.9566, 1.3 sec/batch
2019-05-04 13:27:41.147321 #train# step 1765, loss = 0.9880, cross_entropy loss = 0.9880, 1.3 sec/batch
2019-05-04 13:27:42.525694 #train# step 1766, loss = 1.0255, cross_entropy loss = 1.0255, 1.3 sec/batch
2019-05-04 13:27:43.897468 #train# step 1767, loss = 0.9298, cross_entropy loss = 0.9298, 1.3 sec/batch
2019-05-04 13:27:45.253121 #train# step 1768, loss = 0.9252, cross_entropy loss = 0.9252, 1.3 sec/batch
2019-05-04 13:27:46.621294 #train# step 1769, loss = 0.9277, cross_entropy loss = 0.9277, 1.3 sec/batch
2019-05-04 13:27:47.988011 #train# step 1770, loss = 0.9845, cross_entropy loss = 0.9845, 1.3 sec/batch
2019-05-04 13:27:49.357799 #train# step 1771, loss = 1.0356, cross_entropy loss = 1.0356, 1.3 sec/batch
2019-05-04 13:27:50.719213 #train# step 1772, loss = 0.9673, cross_entropy loss = 0.9673, 1.3 sec/batch
2019-05-04 13:27:52.096923 #train# step 1773, loss = 0.9657, cross_entropy loss = 0.9657, 1.3 sec/batch
2019-05-04 13:27:53.459299 #train# step 1774, loss = 0.9646, cross_entropy loss = 0.9646, 1.3 sec/batch
2019-05-04 13:27:54.824899 #train# step 1775, loss = 0.9682, cross_entropy loss = 0.9682, 1.3 sec/batch
2019-05-04 13:27:56.194899 #train# step 1776, loss = 0.9811, cross_entropy loss = 0.9811, 1.3 sec/batch
2019-05-04 13:27:57.566688 #train# step 1777, loss = 0.9681, cross_entropy loss = 0.9681, 1.3 sec/batch
2019-05-04 13:27:58.929141 #train# step 1778, loss = 0.9595, cross_entropy loss = 0.9595, 1.3 sec/batch
2019-05-04 13:28:00.303541 #train# step 1779, loss = 0.9291, cross_entropy loss = 0.9291, 1.3 sec/batch
2019-05-04 13:28:01.675561 #train# step 1780, loss = 1.0098, cross_entropy loss = 1.0098, 1.3 sec/batch
2019-05-04 13:28:03.049472 #train# step 1781, loss = 0.9046, cross_entropy loss = 0.9046, 1.3 sec/batch
2019-05-04 13:28:04.417250 #train# step 1782, loss = 0.9519, cross_entropy loss = 0.9519, 1.3 sec/batch
2019-05-04 13:28:05.777367 #train# step 1783, loss = 0.9665, cross_entropy loss = 0.9665, 1.3 sec/batch
2019-05-04 13:28:07.142290 #train# step 1784, loss = 1.0230, cross_entropy loss = 1.0230, 1.3 sec/batch
2019-05-04 13:28:08.493163 #train# step 1785, loss = 1.0000, cross_entropy loss = 1.0000, 1.3 sec/batch
2019-05-04 13:28:09.852310 #train# step 1786, loss = 0.9605, cross_entropy loss = 0.9605, 1.3 sec/batch
2019-05-04 13:28:11.194063 #train# step 1787, loss = 0.9654, cross_entropy loss = 0.9654, 1.3 sec/batch
2019-05-04 13:28:12.556058 #train# step 1788, loss = 0.9772, cross_entropy loss = 0.9772, 1.3 sec/batch
2019-05-04 13:28:13.919095 #train# step 1789, loss = 0.9688, cross_entropy loss = 0.9688, 1.3 sec/batch
2019-05-04 13:28:15.278555 #train# step 1790, loss = 0.9370, cross_entropy loss = 0.9370, 1.3 sec/batch
2019-05-04 13:28:16.626575 #train# step 1791, loss = 0.9681, cross_entropy loss = 0.9681, 1.3 sec/batch
2019-05-04 13:28:18.006806 #train# step 1792, loss = 0.9324, cross_entropy loss = 0.9324, 1.3 sec/batch
2019-05-04 13:28:19.411496 #train# step 1793, loss = 0.9795, cross_entropy loss = 0.9795, 1.4 sec/batch
2019-05-04 13:28:20.807963 #train# step 1794, loss = 0.9815, cross_entropy loss = 0.9815, 1.4 sec/batch
2019-05-04 13:28:22.185477 #train# step 1795, loss = 0.9534, cross_entropy loss = 0.9534, 1.3 sec/batch
2019-05-04 13:28:23.559170 #train# step 1796, loss = 0.9961, cross_entropy loss = 0.9961, 1.3 sec/batch
2019-05-04 13:28:25.030202 #train# step 1797, loss = 0.9429, cross_entropy loss = 0.9429, 1.4 sec/batch
2019-05-04 13:28:26.397183 #train# step 1798, loss = 0.9151, cross_entropy loss = 0.9151, 1.3 sec/batch
2019-05-04 13:28:27.747958 #train# step 1799, loss = 0.9564, cross_entropy loss = 0.9564, 1.3 sec/batch
2019-05-04 13:28:29.095136 #train# step 1800, loss = 0.9761, cross_entropy loss = 0.9761, 1.3 sec/batch
2019-05-04 13:28:30.479741 #train# step 1801, loss = 1.0220, cross_entropy loss = 1.0220, 1.3 sec/batch
2019-05-04 13:28:31.865494 #train# step 1802, loss = 0.9753, cross_entropy loss = 0.9753, 1.3 sec/batch
2019-05-04 13:28:33.250627 #train# step 1803, loss = 0.9444, cross_entropy loss = 0.9444, 1.3 sec/batch
2019-05-04 13:28:34.633388 #train# step 1804, loss = 0.9788, cross_entropy loss = 0.9788, 1.3 sec/batch
2019-05-04 13:28:36.002733 #train# step 1805, loss = 0.9505, cross_entropy loss = 0.9505, 1.3 sec/batch
2019-05-04 13:28:37.382837 #train# step 1806, loss = 0.9777, cross_entropy loss = 0.9777, 1.3 sec/batch
2019-05-04 13:28:38.756155 #train# step 1807, loss = 0.9341, cross_entropy loss = 0.9341, 1.3 sec/batch
2019-05-04 13:28:40.123421 #train# step 1808, loss = 0.9803, cross_entropy loss = 0.9803, 1.3 sec/batch
2019-05-04 13:28:41.487283 #train# step 1809, loss = 0.9373, cross_entropy loss = 0.9373, 1.3 sec/batch
2019-05-04 13:28:42.851453 #train# step 1810, loss = 0.9902, cross_entropy loss = 0.9902, 1.3 sec/batch
2019-05-04 13:28:44.214402 #train# step 1811, loss = 0.9725, cross_entropy loss = 0.9725, 1.3 sec/batch
2019-05-04 13:28:45.566738 #train# step 1812, loss = 0.9179, cross_entropy loss = 0.9179, 1.3 sec/batch
2019-05-04 13:28:46.920945 #train# step 1813, loss = 1.0273, cross_entropy loss = 1.0273, 1.3 sec/batch
2019-05-04 13:28:48.286315 #train# step 1814, loss = 1.0297, cross_entropy loss = 1.0297, 1.3 sec/batch
2019-05-04 13:28:49.663802 #train# step 1815, loss = 0.9722, cross_entropy loss = 0.9722, 1.3 sec/batch
2019-05-04 13:28:51.030978 #train# step 1816, loss = 1.0116, cross_entropy loss = 1.0116, 1.3 sec/batch
2019-05-04 13:28:52.408603 #train# step 1817, loss = 0.9410, cross_entropy loss = 0.9410, 1.3 sec/batch
2019-05-04 13:28:53.778610 #train# step 1818, loss = 0.9704, cross_entropy loss = 0.9704, 1.3 sec/batch
2019-05-04 13:28:55.151451 #train# step 1819, loss = 0.9835, cross_entropy loss = 0.9835, 1.3 sec/batch
2019-05-04 13:28:56.517068 #train# step 1820, loss = 0.9429, cross_entropy loss = 0.9429, 1.3 sec/batch
2019-05-04 13:28:57.891109 #train# step 1821, loss = 0.9720, cross_entropy loss = 0.9720, 1.3 sec/batch
2019-05-04 13:28:59.264988 #train# step 1822, loss = 0.9912, cross_entropy loss = 0.9912, 1.3 sec/batch
2019-05-04 13:29:00.630099 #train# step 1823, loss = 1.0327, cross_entropy loss = 1.0327, 1.3 sec/batch
2019-05-04 13:29:01.997580 #train# step 1824, loss = 0.9907, cross_entropy loss = 0.9907, 1.3 sec/batch
2019-05-04 13:29:03.365195 #train# step 1825, loss = 1.0062, cross_entropy loss = 1.0062, 1.3 sec/batch
2019-05-04 13:29:04.728999 #train# step 1826, loss = 1.0074, cross_entropy loss = 1.0074, 1.3 sec/batch
2019-05-04 13:29:06.102356 #train# step 1827, loss = 1.0100, cross_entropy loss = 1.0100, 1.3 sec/batch
2019-05-04 13:29:07.438824 #train# step 1828, loss = 0.9808, cross_entropy loss = 0.9808, 1.3 sec/batch
2019-05-04 13:29:08.801242 #train# step 1829, loss = 0.9300, cross_entropy loss = 0.9300, 1.3 sec/batch
2019-05-04 13:29:10.203345 #train# step 1830, loss = 0.9784, cross_entropy loss = 0.9784, 1.4 sec/batch
2019-05-04 13:29:11.607509 #train# step 1831, loss = 1.0060, cross_entropy loss = 1.0060, 1.4 sec/batch
2019-05-04 13:29:12.994038 #train# step 1832, loss = 0.8791, cross_entropy loss = 0.8791, 1.3 sec/batch
2019-05-04 13:29:14.389974 #train# step 1833, loss = 0.9875, cross_entropy loss = 0.9875, 1.3 sec/batch
2019-05-04 13:29:15.743607 #train# step 1834, loss = 0.9448, cross_entropy loss = 0.9448, 1.3 sec/batch
2019-05-04 13:29:17.184072 #train# step 1835, loss = 0.9159, cross_entropy loss = 0.9159, 1.4 sec/batch
2019-05-04 13:29:18.580581 #train# step 1836, loss = 0.9333, cross_entropy loss = 0.9333, 1.3 sec/batch
2019-05-04 13:29:19.933290 #train# step 1837, loss = 0.9684, cross_entropy loss = 0.9684, 1.3 sec/batch
2019-05-04 13:29:21.318870 #train# step 1838, loss = 0.9870, cross_entropy loss = 0.9870, 1.3 sec/batch
2019-05-04 13:29:22.701343 #train# step 1839, loss = 0.9865, cross_entropy loss = 0.9865, 1.3 sec/batch
2019-05-04 13:29:24.078518 #train# step 1840, loss = 1.0390, cross_entropy loss = 1.0390, 1.3 sec/batch
2019-05-04 13:29:25.461771 #train# step 1841, loss = 0.9828, cross_entropy loss = 0.9828, 1.3 sec/batch
2019-05-04 13:29:26.830583 #train# step 1842, loss = 0.9145, cross_entropy loss = 0.9145, 1.3 sec/batch
2019-05-04 13:29:28.194946 #train# step 1843, loss = 0.9555, cross_entropy loss = 0.9555, 1.3 sec/batch
2019-05-04 13:29:29.564472 #train# step 1844, loss = 0.8805, cross_entropy loss = 0.8805, 1.3 sec/batch
2019-05-04 13:29:30.910488 #train# step 1845, loss = 0.9851, cross_entropy loss = 0.9851, 1.3 sec/batch
2019-05-04 13:29:32.292932 #train# step 1846, loss = 1.0020, cross_entropy loss = 1.0020, 1.3 sec/batch
2019-05-04 13:29:33.671160 #train# step 1847, loss = 0.8963, cross_entropy loss = 0.8963, 1.3 sec/batch
2019-05-04 13:29:35.041209 #train# step 1848, loss = 0.9863, cross_entropy loss = 0.9863, 1.3 sec/batch
2019-05-04 13:29:36.412654 #train# step 1849, loss = 1.0118, cross_entropy loss = 1.0118, 1.3 sec/batch
2019-05-04 13:29:37.781718 #train# step 1850, loss = 1.0304, cross_entropy loss = 1.0304, 1.3 sec/batch
2019-05-04 13:29:39.160438 #train# step 1851, loss = 0.9126, cross_entropy loss = 0.9126, 1.3 sec/batch
2019-05-04 13:29:40.541734 #train# step 1852, loss = 0.9137, cross_entropy loss = 0.9137, 1.3 sec/batch
2019-05-04 13:29:41.868222 #train# step 1853, loss = 0.9877, cross_entropy loss = 0.9877, 1.3 sec/batch
2019-05-04 13:29:43.227736 #train# step 1854, loss = 0.9173, cross_entropy loss = 0.9173, 1.3 sec/batch
2019-05-04 13:29:44.605890 #train# step 1855, loss = 0.9753, cross_entropy loss = 0.9753, 1.3 sec/batch
2019-05-04 13:29:45.974585 #train# step 1856, loss = 0.8990, cross_entropy loss = 0.8990, 1.3 sec/batch
2019-05-04 13:29:47.355775 #train# step 1857, loss = 0.9610, cross_entropy loss = 0.9610, 1.3 sec/batch
2019-05-04 13:29:48.738050 #train# step 1858, loss = 0.9685, cross_entropy loss = 0.9685, 1.3 sec/batch
2019-05-04 13:29:50.112722 #train# step 1859, loss = 0.9953, cross_entropy loss = 0.9953, 1.3 sec/batch
2019-05-04 13:29:51.474765 #train# step 1860, loss = 0.9622, cross_entropy loss = 0.9622, 1.3 sec/batch
2019-05-04 13:29:52.807036 #train# step 1861, loss = 0.9583, cross_entropy loss = 0.9583, 1.3 sec/batch
2019-05-04 13:29:54.156391 #train# step 1862, loss = 0.9754, cross_entropy loss = 0.9754, 1.3 sec/batch
2019-05-04 13:29:55.476605 #train# step 1863, loss = 0.9677, cross_entropy loss = 0.9677, 1.3 sec/batch
2019-05-04 13:29:56.822113 #train# step 1864, loss = 0.9368, cross_entropy loss = 0.9368, 1.3 sec/batch
2019-05-04 13:29:58.186005 #train# step 1865, loss = 0.9884, cross_entropy loss = 0.9884, 1.3 sec/batch
2019-05-04 13:29:59.543756 #train# step 1866, loss = 0.9522, cross_entropy loss = 0.9522, 1.3 sec/batch
2019-05-04 13:30:00.923451 #train# step 1867, loss = 0.9414, cross_entropy loss = 0.9414, 1.3 sec/batch
2019-05-04 13:30:02.291781 #train# step 1868, loss = 0.9287, cross_entropy loss = 0.9287, 1.3 sec/batch
2019-05-04 13:30:03.639761 #train# step 1869, loss = 1.0271, cross_entropy loss = 1.0271, 1.3 sec/batch
2019-05-04 13:30:05.003582 #train# step 1870, loss = 0.9901, cross_entropy loss = 0.9901, 1.3 sec/batch
2019-05-04 13:30:06.380622 #train# step 1871, loss = 0.8910, cross_entropy loss = 0.8910, 1.3 sec/batch
2019-05-04 13:30:07.778427 #train# step 1872, loss = 0.9733, cross_entropy loss = 0.9733, 1.4 sec/batch
2019-05-04 13:30:09.180713 #train# step 1873, loss = 0.9370, cross_entropy loss = 0.9370, 1.4 sec/batch
2019-05-04 13:30:10.568524 #train# step 1874, loss = 0.9538, cross_entropy loss = 0.9538, 1.4 sec/batch
2019-05-04 13:30:11.955263 #train# step 1875, loss = 0.9990, cross_entropy loss = 0.9990, 1.3 sec/batch
2019-05-04 13:30:13.348470 #train# step 1876, loss = 0.9523, cross_entropy loss = 0.9523, 1.3 sec/batch
2019-05-04 13:30:14.758171 #train# step 1877, loss = 0.9525, cross_entropy loss = 0.9525, 1.4 sec/batch
2019-05-04 13:30:16.152554 #train# step 1878, loss = 1.0066, cross_entropy loss = 1.0066, 1.3 sec/batch
2019-05-04 13:30:17.543357 #train# step 1879, loss = 0.8786, cross_entropy loss = 0.8786, 1.3 sec/batch
2019-05-04 13:30:18.900279 #train# step 1880, loss = 0.9249, cross_entropy loss = 0.9249, 1.3 sec/batch
2019-05-04 13:30:20.271565 #train# step 1881, loss = 0.9477, cross_entropy loss = 0.9477, 1.3 sec/batch
2019-05-04 13:30:21.630661 #train# step 1882, loss = 0.8822, cross_entropy loss = 0.8822, 1.3 sec/batch
2019-05-04 13:30:23.002209 #train# step 1883, loss = 0.9831, cross_entropy loss = 0.9831, 1.3 sec/batch
2019-05-04 13:30:24.391853 #train# step 1884, loss = 0.9959, cross_entropy loss = 0.9959, 1.3 sec/batch
2019-05-04 13:30:25.772997 #train# step 1885, loss = 0.9336, cross_entropy loss = 0.9336, 1.3 sec/batch
2019-05-04 13:30:27.136292 #train# step 1886, loss = 0.9670, cross_entropy loss = 0.9670, 1.3 sec/batch
2019-05-04 13:30:28.513036 #train# step 1887, loss = 0.9318, cross_entropy loss = 0.9318, 1.3 sec/batch
2019-05-04 13:30:29.874729 #train# step 1888, loss = 0.9186, cross_entropy loss = 0.9186, 1.3 sec/batch
2019-05-04 13:30:31.251445 #train# step 1889, loss = 0.9361, cross_entropy loss = 0.9361, 1.3 sec/batch
2019-05-04 13:30:32.631064 #train# step 1890, loss = 0.9910, cross_entropy loss = 0.9910, 1.3 sec/batch
2019-05-04 13:30:34.008727 #train# step 1891, loss = 0.9573, cross_entropy loss = 0.9573, 1.3 sec/batch
2019-05-04 13:30:35.386597 #train# step 1892, loss = 0.9900, cross_entropy loss = 0.9900, 1.3 sec/batch
2019-05-04 13:30:36.768722 #train# step 1893, loss = 1.0137, cross_entropy loss = 1.0137, 1.3 sec/batch
2019-05-04 13:30:38.138574 #train# step 1894, loss = 0.9749, cross_entropy loss = 0.9749, 1.3 sec/batch
2019-05-04 13:30:39.497336 #train# step 1895, loss = 0.9510, cross_entropy loss = 0.9510, 1.3 sec/batch
2019-05-04 13:30:40.881254 #train# step 1896, loss = 0.9438, cross_entropy loss = 0.9438, 1.3 sec/batch
2019-05-04 13:30:42.262973 #train# step 1897, loss = 0.9677, cross_entropy loss = 0.9677, 1.3 sec/batch
2019-05-04 13:30:43.638043 #train# step 1898, loss = 0.9619, cross_entropy loss = 0.9619, 1.3 sec/batch
2019-05-04 13:30:44.995188 #train# step 1899, loss = 0.9599, cross_entropy loss = 0.9599, 1.3 sec/batch
2019-05-04 13:30:46.366773 #train# step 1900, loss = 0.9643, cross_entropy loss = 0.9643, 1.3 sec/batch
2019-05-04 13:30:47.722787 #train# step 1901, loss = 0.9806, cross_entropy loss = 0.9806, 1.3 sec/batch
2019-05-04 13:30:49.058981 #train# step 1902, loss = 0.9439, cross_entropy loss = 0.9439, 1.3 sec/batch
2019-05-04 13:30:50.399096 #train# step 1903, loss = 0.9660, cross_entropy loss = 0.9660, 1.3 sec/batch
2019-05-04 13:30:51.777719 #train# step 1904, loss = 0.9598, cross_entropy loss = 0.9598, 1.3 sec/batch
2019-05-04 13:30:53.155922 #train# step 1905, loss = 0.9606, cross_entropy loss = 0.9606, 1.3 sec/batch
2019-05-04 13:30:54.521622 #train# step 1906, loss = 0.9668, cross_entropy loss = 0.9668, 1.3 sec/batch
2019-05-04 13:30:55.895705 #train# step 1907, loss = 0.9484, cross_entropy loss = 0.9484, 1.3 sec/batch
2019-05-04 13:30:57.257622 #train# step 1908, loss = 0.9627, cross_entropy loss = 0.9627, 1.3 sec/batch
2019-05-04 13:30:58.628764 #train# step 1909, loss = 0.9192, cross_entropy loss = 0.9192, 1.3 sec/batch
2019-05-04 13:30:59.993028 #train# step 1910, loss = 0.9766, cross_entropy loss = 0.9766, 1.3 sec/batch
2019-05-04 13:31:01.390927 #train# step 1911, loss = 1.0531, cross_entropy loss = 1.0531, 1.4 sec/batch
2019-05-04 13:31:02.797716 #train# step 1912, loss = 0.9388, cross_entropy loss = 0.9388, 1.4 sec/batch
2019-05-04 13:31:04.196883 #train# step 1913, loss = 0.9761, cross_entropy loss = 0.9761, 1.4 sec/batch
2019-05-04 13:31:05.589457 #train# step 1914, loss = 0.9311, cross_entropy loss = 0.9311, 1.3 sec/batch
2019-05-04 13:31:06.970131 #train# step 1915, loss = 0.9246, cross_entropy loss = 0.9246, 1.3 sec/batch
2019-05-04 13:31:08.362327 #train# step 1916, loss = 0.9536, cross_entropy loss = 0.9536, 1.3 sec/batch
2019-05-04 13:31:09.753718 #train# step 1917, loss = 0.9476, cross_entropy loss = 0.9476, 1.3 sec/batch
2019-05-04 13:31:11.146865 #train# step 1918, loss = 0.9224, cross_entropy loss = 0.9224, 1.3 sec/batch
2019-05-04 13:31:12.539756 #train# step 1919, loss = 0.9820, cross_entropy loss = 0.9820, 1.3 sec/batch
2019-05-04 13:31:13.929857 #train# step 1920, loss = 0.9442, cross_entropy loss = 0.9442, 1.3 sec/batch
2019-05-04 13:31:15.306521 #train# step 1921, loss = 0.9472, cross_entropy loss = 0.9472, 1.3 sec/batch
2019-05-04 13:31:16.699682 #train# step 1922, loss = 0.9534, cross_entropy loss = 0.9534, 1.3 sec/batch
2019-05-04 13:31:18.081998 #train# step 1923, loss = 1.0209, cross_entropy loss = 1.0209, 1.3 sec/batch
2019-05-04 13:31:19.462956 #train# step 1924, loss = 0.8952, cross_entropy loss = 0.8952, 1.3 sec/batch
2019-05-04 13:31:20.849163 #train# step 1925, loss = 0.9517, cross_entropy loss = 0.9517, 1.3 sec/batch
2019-05-04 13:31:22.233137 #train# step 1926, loss = 0.9705, cross_entropy loss = 0.9705, 1.3 sec/batch
2019-05-04 13:31:23.620014 #train# step 1927, loss = 0.9916, cross_entropy loss = 0.9916, 1.3 sec/batch
2019-05-04 13:31:25.003247 #train# step 1928, loss = 1.0166, cross_entropy loss = 1.0166, 1.3 sec/batch
2019-05-04 13:31:26.381664 #train# step 1929, loss = 0.9313, cross_entropy loss = 0.9313, 1.3 sec/batch
2019-05-04 13:31:27.762741 #train# step 1930, loss = 0.9336, cross_entropy loss = 0.9336, 1.3 sec/batch
2019-05-04 13:31:29.151618 #train# step 1931, loss = 0.9681, cross_entropy loss = 0.9681, 1.3 sec/batch
2019-05-04 13:31:30.530523 #train# step 1932, loss = 0.9900, cross_entropy loss = 0.9900, 1.3 sec/batch
2019-05-04 13:31:31.900900 #train# step 1933, loss = 0.8733, cross_entropy loss = 0.8733, 1.3 sec/batch
2019-05-04 13:31:33.289845 #train# step 1934, loss = 0.9530, cross_entropy loss = 0.9530, 1.3 sec/batch
2019-05-04 13:31:34.661934 #train# step 1935, loss = 0.8862, cross_entropy loss = 0.8862, 1.3 sec/batch
2019-05-04 13:31:36.052482 #train# step 1936, loss = 0.9802, cross_entropy loss = 0.9802, 1.3 sec/batch
2019-05-04 13:31:37.438351 #train# step 1937, loss = 0.8731, cross_entropy loss = 0.8731, 1.3 sec/batch
2019-05-04 13:31:38.811151 #train# step 1938, loss = 0.9408, cross_entropy loss = 0.9408, 1.3 sec/batch
2019-05-04 13:31:40.186527 #train# step 1939, loss = 1.0068, cross_entropy loss = 1.0068, 1.3 sec/batch
2019-05-04 13:31:41.543286 #train# step 1940, loss = 0.9060, cross_entropy loss = 0.9060, 1.3 sec/batch
2019-05-04 13:31:42.911393 #train# step 1941, loss = 0.9212, cross_entropy loss = 0.9212, 1.3 sec/batch
2019-05-04 13:31:44.278386 #train# step 1942, loss = 0.9674, cross_entropy loss = 0.9674, 1.3 sec/batch
2019-05-04 13:31:45.646935 #train# step 1943, loss = 0.9946, cross_entropy loss = 0.9946, 1.3 sec/batch
2019-05-04 13:31:46.991454 #train# step 1944, loss = 0.9897, cross_entropy loss = 0.9897, 1.3 sec/batch
2019-05-04 13:31:48.369168 #train# step 1945, loss = 0.9855, cross_entropy loss = 0.9855, 1.3 sec/batch
2019-05-04 13:31:49.745381 #train# step 1946, loss = 0.9520, cross_entropy loss = 0.9520, 1.3 sec/batch
2019-05-04 13:31:51.121924 #train# step 1947, loss = 0.9060, cross_entropy loss = 0.9060, 1.3 sec/batch
2019-05-04 13:31:52.494059 #train# step 1948, loss = 0.8915, cross_entropy loss = 0.8915, 1.3 sec/batch
2019-05-04 13:31:53.860005 #train# step 1949, loss = 0.8912, cross_entropy loss = 0.8912, 1.3 sec/batch
2019-05-04 13:31:55.233212 #train# step 1950, loss = 0.9929, cross_entropy loss = 0.9929, 1.3 sec/batch
2019-05-04 13:31:56.610721 #train# step 1951, loss = 0.9625, cross_entropy loss = 0.9625, 1.3 sec/batch
2019-05-04 13:31:57.979999 #train# step 1952, loss = 0.9536, cross_entropy loss = 0.9536, 1.3 sec/batch
2019-05-04 13:31:59.343936 #train# step 1953, loss = 0.9682, cross_entropy loss = 0.9682, 1.3 sec/batch
2019-05-04 13:32:00.720157 #train# step 1954, loss = 0.9381, cross_entropy loss = 0.9381, 1.3 sec/batch
2019-05-04 13:32:02.118269 #train# step 1955, loss = 0.9891, cross_entropy loss = 0.9891, 1.4 sec/batch
2019-05-04 13:32:03.507102 #train# step 1956, loss = 0.9504, cross_entropy loss = 0.9504, 1.4 sec/batch
2019-05-04 13:32:04.888056 #train# step 1957, loss = 0.9729, cross_entropy loss = 0.9729, 1.3 sec/batch
2019-05-04 13:32:06.278924 #train# step 1958, loss = 0.9472, cross_entropy loss = 0.9472, 1.4 sec/batch
2019-05-04 13:32:07.650646 #train# step 1959, loss = 0.8886, cross_entropy loss = 0.8886, 1.3 sec/batch
2019-05-04 13:32:09.010587 #train# step 1960, loss = 0.9714, cross_entropy loss = 0.9714, 1.3 sec/batch
2019-05-04 13:32:10.372426 #train# step 1961, loss = 1.0184, cross_entropy loss = 1.0184, 1.3 sec/batch
2019-05-04 13:32:11.751433 #train# step 1962, loss = 0.9378, cross_entropy loss = 0.9378, 1.3 sec/batch
2019-05-04 13:32:13.145054 #train# step 1963, loss = 0.9335, cross_entropy loss = 0.9335, 1.3 sec/batch
2019-05-04 13:32:14.535804 #train# step 1964, loss = 0.9996, cross_entropy loss = 0.9996, 1.3 sec/batch
2019-05-04 13:32:15.933718 #train# step 1965, loss = 0.9782, cross_entropy loss = 0.9782, 1.3 sec/batch
2019-05-04 13:32:17.320042 #train# step 1966, loss = 0.9744, cross_entropy loss = 0.9744, 1.3 sec/batch
2019-05-04 13:32:18.707735 #train# step 1967, loss = 0.9940, cross_entropy loss = 0.9940, 1.3 sec/batch
2019-05-04 13:32:20.100206 #train# step 1968, loss = 0.9372, cross_entropy loss = 0.9372, 1.3 sec/batch
2019-05-04 13:32:21.489866 #train# step 1969, loss = 1.0000, cross_entropy loss = 1.0000, 1.3 sec/batch
2019-05-04 13:32:22.874453 #train# step 1970, loss = 0.9541, cross_entropy loss = 0.9541, 1.3 sec/batch
2019-05-04 13:32:24.267301 #train# step 1971, loss = 0.9340, cross_entropy loss = 0.9340, 1.3 sec/batch
2019-05-04 13:32:25.661806 #train# step 1972, loss = 0.9073, cross_entropy loss = 0.9073, 1.3 sec/batch
2019-05-04 13:32:27.056131 #train# step 1973, loss = 0.8674, cross_entropy loss = 0.8674, 1.3 sec/batch
2019-05-04 13:32:28.450481 #train# step 1974, loss = 0.9220, cross_entropy loss = 0.9220, 1.3 sec/batch
2019-05-04 13:32:29.847417 #train# step 1975, loss = 0.9659, cross_entropy loss = 0.9659, 1.3 sec/batch
2019-05-04 13:32:31.225238 #train# step 1976, loss = 0.8893, cross_entropy loss = 0.8893, 1.3 sec/batch
2019-05-04 13:32:32.561916 #train# step 1977, loss = 0.9094, cross_entropy loss = 0.9094, 1.3 sec/batch
2019-05-04 13:32:33.914811 #train# step 1978, loss = 1.0011, cross_entropy loss = 1.0011, 1.3 sec/batch
2019-05-04 13:32:35.298463 #train# step 1979, loss = 0.9607, cross_entropy loss = 0.9607, 1.3 sec/batch
2019-05-04 13:32:36.677886 #train# step 1980, loss = 0.9871, cross_entropy loss = 0.9871, 1.3 sec/batch
2019-05-04 13:32:38.045874 #train# step 1981, loss = 0.9080, cross_entropy loss = 0.9080, 1.3 sec/batch
2019-05-04 13:32:39.394232 #train# step 1982, loss = 0.9259, cross_entropy loss = 0.9259, 1.3 sec/batch
2019-05-04 13:32:40.781923 #train# step 1983, loss = 0.8790, cross_entropy loss = 0.8790, 1.4 sec/batch
2019-05-04 13:32:42.157964 #train# step 1984, loss = 0.9419, cross_entropy loss = 0.9419, 1.3 sec/batch
2019-05-04 13:32:43.540677 #train# step 1985, loss = 0.9624, cross_entropy loss = 0.9624, 1.3 sec/batch
2019-05-04 13:32:44.920763 #train# step 1986, loss = 0.9621, cross_entropy loss = 0.9621, 1.3 sec/batch
2019-05-04 13:32:46.296066 #train# step 1987, loss = 0.9095, cross_entropy loss = 0.9095, 1.3 sec/batch
2019-05-04 13:32:47.679967 #train# step 1988, loss = 0.9729, cross_entropy loss = 0.9729, 1.3 sec/batch
2019-05-04 13:32:49.058286 #train# step 1989, loss = 0.9904, cross_entropy loss = 0.9904, 1.3 sec/batch
2019-05-04 13:32:50.408131 #train# step 1990, loss = 0.9579, cross_entropy loss = 0.9579, 1.3 sec/batch
2019-05-04 13:32:51.787040 #train# step 1991, loss = 0.9947, cross_entropy loss = 0.9947, 1.3 sec/batch
2019-05-04 13:32:53.167043 #train# step 1992, loss = 0.9320, cross_entropy loss = 0.9320, 1.3 sec/batch
2019-05-04 13:32:54.548148 #train# step 1993, loss = 0.8804, cross_entropy loss = 0.8804, 1.3 sec/batch
2019-05-04 13:32:55.929926 #train# step 1994, loss = 1.0075, cross_entropy loss = 1.0075, 1.3 sec/batch
2019-05-04 13:32:57.315330 #train# step 1995, loss = 0.9637, cross_entropy loss = 0.9637, 1.3 sec/batch
2019-05-04 13:32:58.693906 #train# step 1996, loss = 1.0174, cross_entropy loss = 1.0174, 1.3 sec/batch
2019-05-04 13:33:00.051772 #train# step 1997, loss = 0.9316, cross_entropy loss = 0.9316, 1.3 sec/batch
2019-05-04 13:33:01.434954 #train# step 1998, loss = 0.9052, cross_entropy loss = 0.9052, 1.3 sec/batch
2019-05-04 13:33:02.815560 #train# step 1999, loss = 0.9462, cross_entropy loss = 0.9462, 1.3 sec/batch
2019-05-04 13:33:04.187873 #train# step 2000, loss = 0.9185, cross_entropy loss = 0.9185, 1.3 sec/batch
2019-05-04 13:33:04.187954 #traing# finish training
saving model to ./models/lr_5e-05_cqlambda_0.0_alpha_10.0_dataset_cifar10.npy
model saved
initializing
launching session
loading img model from ./models/lr_5e-05_cqlambda_0.0_alpha_10.0_dataset_cifar10.npy
['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8']
img model loading finished
Initializing Dataset
Dataset already
Initializing Dataset
Dataset already
2019-05-04 13:33:41.718888 #validation# start validation
2019-05-04 13:33:41.718944 #validation# totally 1000 query in 10 batches
Cosine Loss: 0.98170596
Cosine Loss: 0.94488555
Cosine Loss: 0.9956258
Cosine Loss: 0.8314972
Cosine Loss: 1.0049586
Cosine Loss: 0.9983287
Cosine Loss: 1.1430655
Cosine Loss: 0.884341
Cosine Loss: 0.939397
Cosine Loss: 1.0411527
2019-05-04 13:33:51.402333 #validation# totally 54000 database in 540 batches
Cosine Loss[0/540]: 0.94142693
Cosine Loss[100/540]: 0.939797
Cosine Loss[200/540]: 0.9166287
Cosine Loss[300/540]: 0.9975825
Cosine Loss[400/540]: 0.87284297
Cosine Loss[500/540]: 0.9694418
i2i_by_feature: 0.6456153461451837
i2i_after_sign: 0.5893167253112275
i2i_prec_radius_2: 0.8444438330372773
i2i_recall_radius_2: 0.05521814814814815
i2i_map_radius_2: 0.9046139370918271
{'R': 54000,
 'alpha': 10.0,
 'batch_size': 256,
 'cq_lambda': 0.0,
 'dataset': 'cifar10',
 'decay_step': 5000,
 'device': '/gpu:0',
 'finetune_all': True,
 'img_db': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/database.txt',
 'img_model': 'alexnet',
 'img_te': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/test.txt',
 'img_tr': '/home/chenshen/Projects/Hash/DeepHash/CY-DeepHash/data/cifar10/train.txt',
 'label_dim': 10,
 'learning_rate': 5e-05,
 'learning_rate_decay_factor': 0.5,
 'log_dir': 'tflog',
 'loss_type': 'normed_cross_entropy',
 'max_iter': 2000,
 'model_weights': './models/lr_5e-05_cqlambda_0.0_alpha_10.0_dataset_cifar10.npy',
 'output_dim': 32,
 'save_dir': './models/',
 'val_batch_size': 100}
